# Subsection_K_Natesan

Ramamurthy, B. Rieck, S. Scardapane, M. T. Schaub, P. Veliˇckovi´c, B. Wang, Y. Wang, G. Wei, and G. Zamzmi. Position: Topological deep learning is the new frontier for relational learning. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 39529–39555. PMLR, 21–27 Jul 2024. [36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 10 [37] T. G. Project. GUDHI User and Reference Manual. GUDHI Editorial Board, 3.10.1 edition, 2024. [38] N. Saul and C. Tralie. Scikit-tda: Topological data analysis for python, 2019. [39] A. Thomas, K. Bates, A. Elchesen, I. Hartsock, H. Lu, and P. Bubenik. Topological data analysis of c. elegans locomotion and behavior. Frontiers in Artificial Intelligence, 4:668395, 2021. [40] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. [41] J. H. Yip, M. Biagetti, A. Cole, K. Viswanathan, and G. Shiu. Cosmology with persistent homology: a fisher forecast. Journal of Cosmology and Astroparticle Physics, 2024(09):034, Sept. 2024. [42] S. Zhang, M. Xiao, and H. Wang. GPU-Accelerated Computation of Vietoris-Rips Persistence Barcodes. In S. Cabello and D. Z. Chen, editors, 36th International Symposium on Computa- tional Geometry (SoCG 2020), volume 164 of Leibniz International Proceedings in Informatics (LIPIcs), pages 70:1–70:17, Dagstuhl, Germany, 2020. Schloss Dagstuhl – Leibniz-Zentrum f¨ur Informatik. Appendix - Additional technical details In most experiments, multiple persistence diagrams were computed for each data entry. Persistence diagrams for homology of different degrees were counted as separate for this purpose. We used a standard method to vectorize, computing a single vector for each of a data entry’s PDs, then concatenating the resulting vectors to obtain a final output. This procedure introduces a set of vectorization hyperparameters for each persistence diagram associated to a data entry. Our experiments allow each PD’s vectorization hyperparameters to vary independently. For example, each synthetic shape point cloud had associated H0, H1, and H2 PDs. Correspondingly, vectorization hyperparameters for all 3 homology degrees were allowed to vary independently. Some hyperparameter choices for both persistence image and Adcock-Carlsson vectorization entail raising birth, death, or persistence coordinates of PD points to a positive power as part of computations. We observed that this can cause overflows of the 32-bit floats preferred by the Python library sklearn when moderately large coordinates occur in the input PDs. This arose primarily for L1 diagrams computed from alpha complexes. Other filtration types cannot have especially large filtration values relative to inputs by construction. To mitigate the issue, we preprocessed outputs after vectorizing by replacing overflows with the maximum possible non-infinite representable 32-bit float in Python for the synthetic shape and f-MNIST experiments. 11 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Rips PI No Noise FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.6 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Alpha PI No Noise FR AP NNB L1 0.6 0.8 1.0 Alpha PI Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Alpha PI Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Alpha PI Noise = 0.6 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Rips ACC No Noise FR AP NNB L1 0.6 0.8 1.0 Rips ACC Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Rips ACC Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Rips ACC Noise = 0.6 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Alpha ACC No Noise FR AP NNB L1 0.6 0.8 1.0 Alpha ACC Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Alpha ACC Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Alpha ACC Noise = 0.6 (a) (b) (c) (d) Figure 7: Average precision scores of random forest classifiers that were trained on (a) PIs derived from Rips filtrations, (b) PIs derived from Alpha filtrations, (c) AC coordinates derived from Rips filtrations, and (d) AC coordinates derived from Alpha filtrations. Filtrations were built on point cloud samples (from 5 shape classes), that were perturbed with varying amounts of noise. Unreduced diagrams were computed from either apparent pairs (AP), low 1s in non-negative-beta (NNB) columns, or low 1s across all columns (L1). 12 Figure 8: Permutation feature importance per component for the FR (top) and L1 (bottom) mod- els. The importance score of a component increases proportionately to the average amount of performance degradation after randomly permuting only the corresponding component among all samples. 13 Figure 9: The differences between the average class 7 PIs and average class 5 PIs of corresponding sweep numbers and homology dimensions. Averages were taken over all PIs in the testing set after normalization via mean centering and standard deviation scaling. 14 Figure 10: Reconstructed feature weight vectors corresponding to most important principal compo- nents with the largest positive permutation feature importance score per model type. The second principal component (PC2) for the FR PD and the third principal component (PC3) for the L1 PD were most important to the held-out testing data prediction score. The color map scale is nor- malized by the minimum and maximum weight values across all feature weights and model types to allow comparison of the scales of the weights of FR PC2 and L1 PC3. 15