# Body

1 A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision Shuying Huang, Junpeng Li, Member, IEEE, Changchun Hua, Fellow, IEEE and Yana Yang Member, IEEE Abstract—To alleviate the annotation burden in supervised learning, N-tuples learning has recently emerged as a powerful weakly-supervised method. While existing N-tuples learning ap- proaches extend pairwise learning to higher-order comparisons and accommodate various real-world scenarios, they often rely on task-specific designs and lack a unified theoretical foundation. In this paper, we propose a general N-tuples learning framework based on empirical risk minimization, which systematically inte- grates pointwise unlabeled data to enhance learning performance. This paper first unifies the data generation processes of N- tuples and pointwise unlabeled data under a shared probabilistic formulation. Based on this unified view, we derive an unbiased empirical risk estimator that generalizes a broad class of existing N-tuples models. We further establish a generalization error bound for theoretical support. To demonstrate the flexibility of the framework, we instantiate it in four representative weakly supervised scenarios, each recoverable as a special case of our general model. Additionally, to address overfitting issues arising from negative risk terms, we adopt correction functions to adjust the empirical risk. Extensive experiments on benchmark datasets validate the effectiveness of the proposed framework and demonstrate that leveraging pointwise unlabeled data consistently improves generalization across various N-tuples learning tasks. Index Terms—Weakly-supervised learning, N-tuples learning, pointwise unlabeled data , unbiased risk estimator. I. INTRODUCTION Weakly-supervised learning (WSL) [1] has emerged as a pivotal paradigm for reducing the cost of manual labeling by exploiting supervision signals that are inaccurate [2], in- complete [3], or inexact [4]. Classical WSL scenarios include positive-unlabeled learning (PU learning) [5]–[7], where only positive and unlabeled instances are available; partial-label learning [8]–[10], where each instance is associated with a set of candidate labels, only one of which is correct; and complementary-label learning [11]–[13], where each label specifies a class that the instance does not belong to. Other variants include positive-confidence learning [14], which uti- lizes unlabeled data accompanied by confidence scores reflect- ing their likelihood of being positive. The most challenging framework, however, is the unlabeled-unlabeled (UU) learning [15]–[17], which constructs classifiers using only unlabeled datasets that differ in class-prior distributions. Collectively, these approaches broaden the applicability of machine learning to complex tasks without exhaustive annotation. Pairwise weak supervision has also attracted attention for capturing relationships between instance pairs. For example, S. Huang, J. Li, C. Hua, and Y. Yang are with the Engineering Research Center of the Ministry of Education for Intelligent Control System and Intelligent Equipment, Yanshan University, Qinhuangdao, China(hsy0403@foxmail.com;jpl@ysu.edu.cn;cch@ysu.edu.cn;yyn@ysu.edu.cn). The "Comparison" column illustrates Pcomp learning [18] and NT-Comp learning [22], while the "Similarity" column corresponds to SU learning [23] and NSU learning [24] Fig. 1: Illustration of weak supervision structures from pointwise to pairwise and N-tuple settings. TABLE I: Representative weak supervision settings and typical tasks. Setting Supervision Assumption Representative Tasks Pointwise Labels on individual instances (possibly partial or noisy) PU learning [5], partial label learning [9], complementary-label learning [12], positive-confidence learning [14], UU learning [15] Pairwise Constraints on instances pairs Pairwise comparisons learning [18], similarity/dissimilarity learning [23], and not-all-negative pairwise learning [21] N-tuple Constraints on instances groups N-tuples comparisons learning [22], N-tuples similarities and unlabeled learning [24] pairwise comparisons (Pcomp) learning [18], [19] captures relative preferences by enforcing that one instance is more likely to be positive than the other; similarity and unlabeled (SU) learning [20] determines whether two instances belong to the same class, and not-all-negative pairwise(PposU) [21] assumes that at least one instance in each pair is positive. While these approaches have proven effective in applications such as recommendation and ranking, their binary nature limits their ability to capture higher-order relationships within larger groups of instances. To address this challenge, recent works have introduced N-tuple weak supervision to handle group-wise relations. For example, N-tuple comparison learning (NT-Comp) [22] assumes a ranking over the N instances in each tuple based on their probabilities of being positive, providing richer ordinal constraints among the group. In the N-tuple similarity and unlabeled (NSU) [24] setting, all N instances in a tuple are arXiv:2507.07771v1 [stat.ML] 10 Jul 2025 2 TABLE II: Mathematical definitions and real-world applications of different N-tuples learning scenarios Task types N-tuples scenarios Mathematical definition Pointwise unlabeled data NT-Comp [22] The N instances are ranked in descending order of confidence for being positive ∀i ∈{1, . . . , N−1}, P(yi = +1) > P(yi+1 = +1) ✘ NSU [24] All N instances are from the same class ∀i, j ∈{1, . . . , N}, yi = yj ✔ MNU Mixed-class N-tuples: Not all N instances are from the same class ∃i, j ∈{1, . . . , N}, yi ̸= yj ✔ NposU Not-all-negative N-tuples: At least one instance among the N is positive ∃i ∈{1, . . . , N}, yi = +1 ✔ Note: NT-Comp [22] and NSU [24] are representative existing methods, while MNU and NposU are novel task settings derived and discussed in this paper under the unified N-tuples learning framework. known to share the same (unknown) label, capturing group- level similarity. Figure 1 visually illustrates the structural pro- gression from pointwise to pairwise and N-tuple supervision. While Table I summarizes representative weak supervision paradigms. Compared to pairwise signals, N-tuple supervision provides a more expressive framework for modeling high- order dependencies among multiple instances. However, ex- isting N-tuple methods are typically tailored to specific tasks and lack generalizability across broader settings. In this work, we propose a unified N-tuple weakly- supervised learning framework that subsumes existing N-tuple methods and extends them with greater flexibility. We begin by considering ¯Y as the full label space consisting of all 2N possible label configurations for an N-tuple of binary in- stances, where each instance is labeled as either positive (+1) or negative (−1). For any given weak supervision scenario, we then define a subset Ysub ⊆¯Y of these assignments that satisfy the task’s constraints. For example, the NT-Comp scenario corresponds to those assignments where the probabilities of being positive are strictly decreasing across the tuple, i.e., P(y1 = +1) > P(y2 = +1) > · · · > P(yN = +1), while NSU corresponds to assignments where all labels in the tuple are identical, i.e., ∀i, j ∈{1, 2, . . . , N}, yi = yj. We further introduce two more general scenarios that naturally combine N-tuple weak supervision with pointwise unlabeled data. Specifically, the mixed-class N-tuples and pointwise un- labeled learning (MNU) allows tuples where not all instances share the same label, i.e., ∃i, j ∈{1, 2, . . . , N}, yi ̸= yj; and the not-all-negative N-tuples and pointwise unlabeled learning (NposU) ensures at least one positive instance, i.e., ∃i ∈ {1, 2, . . . , N}, yi = +1. These settings correspond to practical tasks such as academic performance ranking (NT-Comp), batch quality inspection (NSU), general image classification (MNU), and fraud detection (NposU). By selecting different subsets of the label space, our framework unifies these diverse supervision forms and eliminates the need for task-specific model designs. The specific label constraints corresponding to each weakly supervised scenario are summarized in Table II. This study builds on the empirical risk minimization (ERM) framework [6] and presents a systematic approach to address challenges in weakly-supervised learning. We begin by an- alyzing the statistical properties of weakly supervision and modeling their underlying distribution. This distribution is then incorporated into the loss function to reconstruct the empirical risk, guiding the model to better leverage weak supervision. In addition, we provide rigorous theoretical analysis of the proposed framework. By leveraging rademacher complexity, we derive estimation error bounds for both the general formu- lation and its special cases. The results show that empirical risk minimization under our framework is statistically consis- tent: as the number of training instances grows, the learned classifier converges to the best possible classifier under the given constraints. In summary, our method advances both theory and practice by offering a conceptually simple yet powerful framework for N-tuple weak supervision with strong learning guarantees. The main contributions of this work can be summarized as follows: • Unified N-tuple Framework: We propose a unified framework that models diverse weak supervision sce- narios by specifying task-dependent label constraints over the full 2N N-tuple label space. This formulation subsumes existing methods (e.g., NT-Comp, NSU) and naturally generalizes to new settins ( MNU, NposU). The resulting algorithm constructs unbiased risk estimators and jointly optimizes with unlabeled data, offering an efficient and principled learning solution. • Theoretical Guarantees: We establish generalization bounds for both the unified model and its special cases using rademacher complexity. These results confirm the statistical consistency of our approach and provide the- oretical insights into learning under weak supervision constraints. • Empirical Validation: We conduct extensive experi- ments on benchmark datasets across diverse weakly su- pervised tasks. Our unified method consistently outper- forms baseline and specialized models, demonstrating its effectiveness and superior generalizability. 3 II. PRELIMINARIES To provide a solid foundation for our research, this section introduces the relevant background of supervised classification method and fundamental concepts. Supervised classification is a traditional learning paradigm that trains classifiers using precisely labeled examples. Given a dataset with precisely labeled instances, let X ⊂Rd denote the feature space consisting of both positive and negative examples. Y = {−1, 1} indicates the label space of X, with y = 1 denoting a positive instance and y = −1 a negative one. The positive dataset Xp is independently drawn from the marginal distribution p+(x) = p(x | y = +1). Similarly, the negative sample set Xn is independently sampled from the marginal distribution p−(x) = p(x | y = −1). Thus, each training instance (x, y) ∈(X, Y) is drawn from an unknown joint probability distribution with density p(x, y). The goal of supervised classification is to learn a classifier g : X →R by minimizing the expected risk defined as: R(g) = E p(x,y)[ℓ(g(x), y)] =τ+ E p+(x)[ℓ(g(x), +1)] + τ− E p−(x)[ℓ(g(x), −1)]. (1) where ℓ(g(x), y) represents the loss function measuring the discrepancy between the classifier’s prediction and the true label. Here, τ+ = p(y = +1) represent the class-prior of positive examples and τ−= p(y = −1) denote the class-prior of negative examples. These class priors satisfy the constraint τ+ + τ−= 1. Thus, the optimal classifier in supervised classification is obtained by solving: g∗= argmin g∈G R(g), (2) where G denotes the hypothesis space of possible classifiers. III. GENERALIZED FRAMEWORK To accommodate diverse weak supervision settings, this section proposes a unified framework based on common data generation process. We develop a risk minimization framework aligned with the distributional properties of weakly-supervised data. Estimation error bounds are subsequently established to ensure theoretical guarantees. A. Generation Process of Training Data This section describes the generation process of the training data used in our weakly-supervised learning framework. N-tuples data : To standardize the data generation pro- cess, we define the sample space as ¯D = {¯xi}¯n i=1 = {(x1,i, . . . , xN,i)}¯n i=1, where each ¯xi is an N-tuple of in- stances arbitrarily drawn from the feature space, and ¯n denotes the total number of such tuples. The associated label space is specified as ¯Y = {−1, 1}N. (3) The possible N-tuple configurations are summarized in Ta- ble III. TABLE III: Categorization of N-tuple configurations based on weak supervision types Problem Cases Description Containing n positive ® Ⓢ (+1,+1,+1...+1,+1,+1)※ One case Containing n−1 positive ® ⋆ (+1,+1,+1...+1, +1,-1)※ n 1  (+1,+1,+1...+1,-1,+1) ... (+1,-1,+1...+1,+1,+1) (-1,+1,+1...+1,+1,+1) Containing n−2 positive ® ⋆ (+1,+1,+1...+1,-1,-1)※ n 2  (+1,+1,+1...-1,-1,+1) ... (+1,-1,-1...+1,+1,+1) (-1,-1,+1...+1,+1,+1) ... ... ... Containing one positive ® ⋆ (+1,-1,-1...-1,-1,-1)※ n n−1  (-1,+1,-1...-1,-1,-1) ... (-1,-1,-1...-1,+1,-1) (-1,-1,-1...-1,-1,+1) Containing n negative Ⓢ (-1,-1,-1,...-1,-1,-1)※ One case ※: N-tuple comparison data (combinations ordered by decreasing confidence of being positive); ®: Not-all-negative N-tuples (at least one positive instance); Ⓢ: Similar N-tuples (all instances from the same class); ⋆: Mixed-class N-tuples (instances not all from the same class). A subset Ysub ⊆¯Y is further defined by imposing specific constraints on the label vectors y = (y1, . . . , yN), where each yj ∈{−1, 1} denotes the (latent) label of the j-th instance in the tuple: Ysub =  y ∈¯Y | additional constraints , (4) where Ysub ̸= ∅and Ysub ̸= ¯Y. Accordingly, we define a dataset Dn = {¯xi}nb i=1 , consisting of N-tuples whose latent label vectors belong to Ysub, where nb is the number of such valid tuples. Lemma 1: The dataset Dn is independently drawn from the distribution pn(¯x), given by pn(¯x) = P y∈Ysub  NQ k=1 pyk(xk)τyk  P y∈Ysub NQ k=1 τyk . (5) where pyk(xk) denotes the class-conditional density, τyk = τ+ and pyk = p+ if yk = +1; otherwise, τyk = τ−and pyk = p− if yk = −1. The proof is provided in Appendix A. Let Dj = {xj,i}nb i=1 denote the dataset of the j-th elements extracted from all tuples in Dn, treating each instance inde- pendently and disregarding the original tuple structure. Based on this, we present Theorem 1. 4 Theorem 1: The dataset Dj is independently sampled from an underlying distribution ˜pj(x). ˜pj(x) = P y∈Ysub yj=+1 NQ k=1 τyk Z | {z } αj p+(x) + P y∈Ysub yj=−1 NQ k=1 τyk Z | {z } βj p−(x), (6) where Z = P y∈Ysub NQ k=1 τyk. αj and βj represent the weighting coefficients associated with the positive and negative class distributions, respectively. The proof is provided in Appendix B. In the symmetric case, all positions within the N-tuple are statistically equivalent, and thus ˜p1(x) = · · · = ˜pN(x). This implies that αj = α and βj = β for all j ∈{1, . . . , N}. Furthermore, we consider a ponitwise unlabeled dataset, which plays a crucial role in leveraging the underlying data distribution to improve model generalization under weak su- pervision. Pointwise unlabeled data : Pointwise unlabeled data refer to instances that are not associated with any label information, and the corresponding dataset is denoted as Du = {xu,i}nu i=1. Each instance xu,i ∈Du is assumed to be independently drawn from a marginal distribution p(x), which is expressed as a convex combination of the class-conditional distributions: p(x) = τ+p+(x) + τ−p−(x). (7) To incorporate both N-tuples and pointwise unlabeled data, the following datasets are considered: Dn = {¯xi}nb i=1 ∼pn(¯x), (8) Du = {xu,i}nu i=1 ∼p(x). (9) This formulation integrates structural information from N- tuples and distributional insights from unlabeled instances, thereby laying the foundation for constructing a unified risk function. B. Unbiased Risk Estimator for the proposed method This section begins by revisiting the generalized linear system that relates the observed mixture distributions to the underlying class-conditional distributions: ˜p(x) p(x)  = A Γ  p+(x) p−(x)  , (10) where ˜p(x) = [˜p1(x), ˜p2(x), . . . , ˜pN(x)]⊤ is an N- dimensional column vector, and the matrix A Γ  ∈R(N+1)×2 aggregates the mixture coefficients. The matrix A ∈RN×2 is defined as A =   α1 β1 α2 β2 ... ... αN βN  , and Γ = [τ+ τ−], In the asymmetric setting, as presented in Section IV-A, A is an N × 2 matrix whose coefficients may vary across rows, reflecting heterogeneous instance-level mixtures. In contrast, under the symmetric assumption (Sections IV-B-IV-D), all rows in A = [α β] are identical, implying that all mixtures share the same marginal distribution. By applying the operations in Eq. (10), Lemma 2 can be derived. Lemma 2: The class-conditional densities p+(x) and p−(x) can be recovered by the following closed-form solution: p+(x) p−(x)  = M⊤M −1 M⊤ ˜p(x) p(x)  . (11) where M = A Γ  . This result holds under the condition that M ⊤M is invertible, i.e., M has full column rank. The detailed proof of Lemma 2 is provided in Appendix C. Supposing (MT M)−1MT = C11 C12 · · · C1N D1 C21 C22 · · · C2N D2  , (12) Thus, we have, p+(x) = N X j=1 C1j ˜pj(x) + D1p(x), p−(x) = N X j=1 C2j ˜pj(x) + D2p(x). (13) Substituting the expressions for p+(x) and p−(x) into the risk function in Eq. (1), we obtain: Theorem 2: The risk function in Eq. (1) can thus be rewritten as: Rn(g) = N X j=1 E ˜pj(x)[τ+C1jℓ(g(x), +1) + τ−C2jℓ(g(x), −1)] + E p(x) [τ+D1ℓ(g(x), +1) + τ−D2ℓ(g(x), −1)] , (14) The detailed proof of Theorem 2 is provided in Appendix D. Accordingly, the empirical version of the risk function based on sample means is given by: bRn(g) = 1 nb N X j=1 nb X i=1 [τ+C1jℓ(g(xj,i), +1) + τ−C2jℓ(g(xj,i), −1)] + 1 nu nu X i=1 [τ+D1ℓ(g(xu,i), +1) + τ−D2ℓ(g(xu,i), −1)] . (15) The classifier is thus trained by minimizing the empirical risk bRn(g). ˆgn = argmin g∈G bRn(g), (16) This setting provides a unified framework to exploit N-tuples structured information and pointwise unlabeled data. 5 Theorem 3: If the pointwise data distribution is symmetric, then the risk function simplifies to: Rn(g) = τ+τ− ατ−−βτ+ E x∼˜pj(x) [Lℓ(g(x))] + E x∼p(x) [Lu,ℓ(g(x))] , (17) where: Lℓ(z) = ℓ(z, +1) −ℓ(z, −1), Lu,ℓ(z) = ατ−ℓ(z, −1) −βτ+ℓ(z, +1) ατ−−βτ+ . The detailed proof of Theorem is provided in Appendix E for completeness. C. Estimation Error Bound This section presents a generalization error bound for the classifier bgn, learned from N-tuples structured data combined with pointwise unlabeled instances. The derivation relies on the following assumptions regarding the hypothesis class and the loss function. Assumptions: • Let G ⊂RX be the function class under consideration. Each g ∈G is uniformly bounded, i.e., ∥g∥∞≤Cg for some constant Cg > 0. • The loss function ℓis ρ-Lipschitz continuous with respect to its first argument, with ρ ∈(0, ∞). In addition, let Cℓ= supt∈{±1} ℓ(Cg, t) denote the maximum loss value under this bound. To assess the generalization performance of the classifier bgn trained from N-tuples and pointwise unlabeled data , we derive an estimation error bound based on Rademacher complexity [25]. Theorem 4: Let bgn = argming∈G bRn(g) be the general empirical risk minimizer. For any δ > 0, with probability at least 1 −δ: R(ˆgn) −R(g∗) ≤Kn 1 √nb + Ku 1 √nu (18) where Kn = τ+ N P j=1 C1j + τ− N P k=1 C2j !  4ρCG + Cℓ q 2 ln 4N δ  and Ku = (τ+D1 + τ−D2)  4ρCG + Cℓ q 2 ln 4 δ  Appendix F provides the detailed derivation of Theorem 4 and presents Lemma 4, whose proof relies on standard results and can be found, for example, in Theorem 3.1 of [26]. Theorem 4 implies that the learned classifier ˆgn converges to the optimal classifier g∗at the optimal rate of O( 1 √nb + 1 √nu ) as nb →∞and nu →∞. This result confirms the consistency of the proposed method and highlights its sample efficiency when leveraging both N-tuples structured data and unlabeled instances. The error bound under the symmetric condition is formally provided in Theorem 5. Theorem 5: For any δ > 0, with probability at least 1 − δ, the risk under the symmetric data distribution satisfies the following error bound: R(ˆgn) −R(g∗) ≤Sn 1 √Nnb + Su 1 √nu (19) Algorithm 1 Generalized algorithm Input: Model g, N-tuple Dn = {(¯xi)}nb i=1 (sampled from pn(¯x)), Pointwise unlabeled data Du = {xu,i}nu i=1 (sam- pled from p(x)); 1: for i = 1, 2, ... number of epochs do 2: Shuffle Dc = Dn ∪Du 3: for j = 1, 2, ... number of batch_size do 4: Fetch mini-batch ˜ Dc from Dc 5: Update model g by minimize risk loss bRn(g) in Eq.(23) 6: end for 7: end for Ensure: g. where Sn = 4τ+τ− ατ−−βτ+ (2ρCG +Cℓ q 1 2 ln 4 δ ) and Su = 4ρCG + 2Cℓ q ln 4 δ 2 . A supplementary proof of Theorem 5 can be found in Ap- pendix G. IV. BRIDGING GENERALIZED N-TUPLES LEARNING WITH SPECIFIC APPLICATIONS This section analyzes the structure of the general model as it specializes to various weak supervision settings. The corresponding supervision scenarios and their label structures are summarized in Table III, with distinct symbols indicating different weak supervision types. A. Case 1: N-tuples comparisons and unlabeled learning (NCompU) To align with the general modeling framework, this section integrates the NT-Comp learning [22] by incorporating point- wise unlabeled data to enhance classification. The N-tuples comparisons data [22] considers a scenario in which the instances within each tuples are ordered according to their confidence of belonging to the positive class. Specifically, for an N-tuples (x1, x2, . . . , xN), the confidence satisfy a descending order: conf(x1) ≥conf(x2) ≥· · · ≥conf(xN), where conf(xj) denotes the confidence of xj being positive for j = 1, . . . , N. Accordingly, the label space of N-tuples comparisons data is defined as: Ysub = Ycomp = {y ∈{−1, 1}N | P(y1 = +1) > · · · > P(yN = +1)}. (20) This label space encodes all label assignments that are con- sistent with the descending confidence assumption. The full N-tuples comparisons dataset is defined as Dc = {(xc1,i, . . . , xcN,i)}nc i=1, where each tuple contains N in- stances ordered by confidence. For pointwise analysis, we extract position-specific instance sets eDcj =  xcj,i nc i=1, 6 where each eDcj contains all instances at the j-th position across tuples. eDcj is independently sampled from: ˜pj(x) = αjp+(x) + βjp−(x), (21) where αj = N P k=j τ k +τ N−k − N P k=0 τ k +τ N−k − , βj = j−1 P k=0 τ k +τ N−k − N P k=0 τ k +τ N−k − The training data consist of pointwise unlabeled instances and N-tuples comparisons data, as summarized below: • N-tuples Comparisons: Dj = eDcj = {xcj,i}nc i=1 ∼ ˜pj(x) • Pointwise unlabeled data : Du = {xu,i}nu i=1 ∼p(x) The empirical risk function can be reformulated to incor- porate N-tuple comparison data and pointwise unlabeled data . Corollary 1: The empirical risk for NCompU can be rewrit- ten. bRn(g) = 1 nc N X j=1 nc X i=1 [τ+C1jℓ(g(xj,i), +1) + τ−C2jℓ(g(xj,i), −1)] + 1 nu nu X i=1 [τ+D1ℓ(g(xu,i), +1) + τ−D2ℓ(g(xu,i), −1)] = bRcu(g). (22) with coefficients: C1j = αjγ3 −βjγ2 γ1γ3 −γ2 2 , C2j = −αjγ2 + βjγ1 γ1γ3 −γ2 2 , D1 = τ+γ3 −τ−γ2 γ1γ3 −γ2 2 , D2 = −τ+γ2 + τ−γ1 γ1γ3 −γ2 2 . (23) where: γ1 = N P j=1 α2 j + τ 2 +, γ2 = N P j=1 αjβj + τ+τ−, γ3 = N P j=1 β2 j + τ 2 −. Then, the estimation error bound for NCompU learning is given. Corollary 2: Let bgcu = argming∈G bRcu(g) be the NcompU empirical classifier, for any δ > 0, with probability at least 1 −δ: R(ˆgcu) −R(g∗) ≤Kn √nc + Ku √nu , (24) where Kn and Ku are derived by plugging Eq. (23) into the general bound form of Eq. (18). This demonstrates that the risk R(ˆgcu) converges to the op- timal risk R(g∗) at the rate O  1 √nc + 1 √nu  , as nc, nu →∞. Proof Sketch. Corollaries 1 and 2 are immediate results of Theorems 2 and 4, with coefficients αj and βj specified in Eq. (21). B. Case 2: N-tuples similarities and unlabeled learning N-tuples similarities [24] refer to a collection of N instances that all belong to the same class. The associated label space is constrained to: Ysub = Ysim = {(+1, +1, . . . , +1), (−1, −1, . . . , −1)} , (25) which implies that all instances within a tuple are either positive or negative. We define the N-tuples similarity dataset as Ds = {(xs1,i, . . . , xsN,i)}ns i=1, where each of the ns tuples consists of N instances from the same class, ensuring label consis- tency. We further flatten the tuples into a pointwise dataset eDs = {xs,i}nsN i=1 , where each instance is treated independently for downstream learning tasks. eDs are sampled from: ˜ps(x) = τ N + p+(x) + τ N −p−(x) τ N + + τ N − . (26) Combined with pointwise unlabeled data , • N-tuples Similarities : Dj = eDs = {xs,i}nsN i=1 ∼˜ps(x) • Pointwise unlabeled data : Du = {xu,i}nu i=1 ∼p(x) The corresponding risk estimator and generalization bound under this setting are presented in corollaries 3 and 4. Corollary 3 (Theorem 5 in [24]): The rewritten risk function is: bRs(g) = τ N + + τ N − (τ N−1 + −τ N−1 − )nns N nn s N X i=1 [ℓ(g(x), +1) −ℓ(g(x), −1)] + 1 nu nu X i=1 [− τ 2 − 2τ+ −1ℓ(g(x), +1) + τ 2 + 2τ+ −1ℓ(g(x), −1)], (27) Corollary 4 (Theorem 6 in [24]): Let bgs = argming∈G bRs(g) be the empirical classifier, for any δ > 0, with probability at least 1 −δ: R(ˆgs) −R(g∗) ≤Kn 1 p nns N Ku 1 √nu (28) where Kn = 4(τ N + +τ N −) τ N−1 + −τ N−1 − (2ρCG + Cℓ q 1 2 ln 4 δ ) and Ku = 4ρCG + Cℓ q 2 ln 4 δ . C. Case 3: Mixed-class N-tuples and unlabeled learning In this setting, each N-tuple is known to contain a mixture of positive and negative class instances, but the exact number or positions of the positive samples are not specified. Formally, the constraint is: Ysub = Ymix = {y ∈{−1, 1}N | y ̸= −1, y ̸= 1}. (29) Let eDm = {xm,i}nmN i=1 be the pointwise dataset induced from nm mixed-class N-tuples, where each instance is inde- pendently drawn from ˜pj(x). Theorem 6: Under the mixed-class learning framework, the marginal distribution ˜pj(x) in Eq. (6) admits the following coefficients: α = N−1 P k=1 N−1 k  τ N−k + τ k − N−1 P k=1 N N−k  τ N−k + τ k − , β = N−1 P k=1 N−1 N−k  τ N−k + τ k − N−1 P k=1 N N−k  τ N−k + τ k − . (30) Under symmetry, αj = α, βj = β for all j, and matrix A = [α, β]. The detailed proof of the Theorem 6 is provided in the Appendix H. 7 The training data consist of: • Mixed-class N-tuples: Dj = eDm ∼˜pj(x) • Pointwise unlabeled data : Du = {xu,i}nu i=1 ∼p(x) Based on this, the empirical risk for mixed-class and point- wise unlabeled data is derived as follows. Corollary 5: Substituting the coefficients from Eq. (30) into Eq. (17), the empirical risk becomes: bRn(g) = τ+τ− nmN(ατ−−βτ+) nmN X i=1 [Lℓ(g(xm,i))] + 1 nu nu X i=1 [Lu,ℓg(xu,i)] = bRm(g) (31) The estimation error bound under this risk is given below. Corollary 6: Let bgm = argming∈G bRm(g) be the empirical classifier, for any δ > 0, with probability at least 1 −δ: R(ˆgm) −R(g∗) ≤ Kn √nmN + Ku √nu , (32) where Kn and Ku are obtained by plugging the coefficients from Eq. (30) into the general bound form of Eq. (19). This result shows that the excess risk R(ˆgm) −→R(g∗) at the optimal rate of O  1 √nc + 1 √nu  , as nm, nu →∞. Proof Sketch. Corollaries 5 and 6 are immediate results of Theorems 3 and 5, with coefficients α and β specified in Eq. (30). The same applies to corollaries 7 and 8. Thus, the proofs are omitted. D. Case 4: Not-All-Negative N-Tuples and unlabeled learning In this setting, each N-tuple is weakly labeled with the information that at least one instance is from the positive class, while the exact label configuration is unknown. Formally, the weak label constraint is defined as: Ysub = Ynan = {y ∈{−1, 1}N | y ̸= −1}. (33) Let eDe = {xe,i}Nne i=1 denote the pointwise dataset induced from ne not-all-negative N-tuples, where each instance is independently drawn from the marginal distribution ˜pj(x). Theorem 7: Under the mixed-class weak supervision set- ting, the marginal distribution ˜pj(x) defined in Eq. (6) satis- fies: α = τ N + + N−1 P k=1 N−1 k  τ N−k + τ k − 1 −τ N − , β = N−1 P k=1 n−1 N−k  τ N−k + τ k − 1 −τ N − . (34) Under symmetry, αj = α, βj = β for all j, and the coefficient matrix simplifies to A = [α, β]. A detailed proof of the theorem is presented in the Ap- pendix I. The training data consist of: • Not-all-negative N-tuples: Dj = eDe ∼˜pj(x) • Pointwise unlabeled data : Du = {xu,i}nu i=1 ∼p(x) Based on these coefficients, the empirical risk can be constructed as follows. TABLE IV: Comparison of Key Parameters Task Type α β N-tuples comparisons N P k=j τk +τN−k − N P k=0 τk +τN−k − j−1 P k=0 τk +τN−k − N P k=0 τk +τN−k − Similar N-tuples τN + τN + +τN − τN − τN + +τN − Mixed-class N-tuples N−1 P k=1 N−1 k  τN−k + τk − N−1 P k=1  N N−k  τN−k + τk − N−1 P k=1 N−1 N−k  τN−k + τk − N−1 P k=1  N N−k  τN−k + τk − Not-all-negative N-Tuples τN + + N−1 P k=1 N−1 k  τN−k + τk − 1−τN − N−1 P k=1  n−1 N−k  τN−k + τk − 1−τN − Corollary 7: Incorporating the coefficients from Eq. (34) into Eq. (17), the risk function can be rewritten as: bRn(g) = τ+τ− nnmN(ατ−−βτ+) nn mN X i=1 [Lℓ(g(xm,i))] + 1 nu nu X i=1 [Lu,ℓg(xu,i)] = bRe(g) (35) The corresponding estimation error bound is as follows. Corollary 8: Let bge = argming∈G bRe(g) be the empirical classifier, for any δ > 0, with probability at least 1 −δ: R(ˆge) −R(g∗) ≤Kn √ne + Ku √nu , (36) where Kn and Ku are specified by substituting the coefficients from Eq. (34) into the general bound form of Eq. (19). This demonstrates that the learned risk R(ˆge) converges to the optimal risk R(g∗) at the rate O  1 √ne + 1 √nu  , as ne, nu →∞. V. RISK CORRECTION A. General risk formulation Eq. (15) defines an empirical risk estimator that may in- volve negative coefficients, potentially leading to negative risk values. Such negative risk is generally regarded as a sign of overfitting. To mitigate overfitting caused by this issue, prior works [16], [27] have proposed correction functions. Specifically, [16] enforced the non-negativity of empirical risk by applying a linear correction unit that clips negative values, whereas [27] argued that negative terms may still contain useful information for training and thus introduced a consistent correction function. The generalized form of the correction function is defined as follows: ¯Rn(g) = f( bRn(g)). (37) where, f(x) = ( x, x ≥0, k|x|, x < 0. and k > 0. In practice, we employ the rectified linear unit (ReLU), f(z) = max(0, z), and the absolute value function, f(z) = |z|, 8 to regularize the negative risk. The effectiveness of applying correction functions to mitigate the impact of negative risk is demonstrated in the experimental results, as illustrated in Fig. 2. B. Consistency guarantee Let ¯gn = argming∈G ¯Rn(g). In this section, we analyze the consistency of the corrected risk function ¯Rn(g) and its associated classifier ¯gn. First, based on the assumptions on the correction function, we have ¯Rn(g) ≥bRn(g). Therefore, we establish the consistency of the corrected risk ¯R(g) in Theorem 8. Theorem 8 (Risk Consistency of ¯R(g)): Let τ = max(τ+, τ−) denote the scaling factor for class-prior weights, Lf = max{1, k} denotes the Lipschitz constant of the cor- rection function, and Cw be an upper bound on the weighting coefficients (Cjk, Di). Assume there exists ϵ > 0 such that Rn(g) ≥ϵ. Under these conditions, the bias of ¯R(g) decays exponentially as n →∞. E[ ¯R(g)] −R(g) ≤O(1) · exp  − 2α2 (Nnb + nc)∆2  (38) where ∆= max n 2NτCwCℓ nb , 2τCwCℓ nu o , and O(1) = (Lf + 1)(N + 1)τCwCℓis a constant factor. Moreover, with probability at least 1 −δ, the following holds: ¯R(g) −R(g) ≤Lℓ∆ r ln(2/δ) 2n + O(1) · exp  − 2α2 (Nnb + nc)∆2  (39) A detailed proof is presented in the Appendix J. Theorem 8 demonstrates the consistency of ¯R(g). Leverag- ing this result, we can further establish the consistency of the associated classifier. Theorem 9 (Classifier Consistency of ¯gn): Based on the consistency of ¯R(g), with probability at least 1 −δ, the following inequality holds: R(¯gn) −R(g∗) ≤2Lℓ∆ r ln(2/δ) 2n + 2O(1) · exp  −2α2 n∆2  + Kn 1 √nb + Ku 1 √nu (40) A detailed proof is presented in the Appendix K. Theorem 9 shows that the learned classifier ¯gn is consistent, as its risk converges to that of the optimal classifier g∗as the sample size increases. VI. EXPERIMENT. This section presents empirical evaluations to demonstrate how the proposed generalized framework performs when in- stantiated for specific learning tasks. A. Datasets In the MNIST dataset, images of even digits are assigned to the positive class, while odd digits are categorized as negative. Each grayscale image has a size of 28 × 28, leading to a flattened input dimension of 784. For Fashion-MNIST, the categories T-shirt/top, Pullover, Dress, Coat, and Shirt are grouped as the positive class, with the remaining items forming the negative class. The input structure mirrors MNIST, with images of size 28 × 28 and an input dimension of 784. In SVHN, we follow a similar binary labeling strategy: even digits are considered positive, and odd digits negative. Each color image has dimensions 32 × 32 × 3, resulting in an input dimension of 3,072. For the CIFAR-10 dataset, images depicting airplanes, au- tomobiles, ships, and trucks form the positive class, while the rest are labeled negative. Like SVHN, each image has dimensions 32×32×3, giving an input vector of length 3,072. B. Baseline methods NT-Comp [22]: As detailed in Related Works, this baseline operates on N-tuples comparisons data where instances are ranked by their confidence of belonging to the positive class. KM [28]: K-means is a widely used unsupervised learning method that divides data into K clusters by minimizing the within-cluster sum of squared distances to the centroids. In our setting, we set K = 2 to perform binary clustering. The algorithm treats all samples as unlabeled and does not leverage any pairwise similarity or dissimilarity information. Triplet comparison learning [29]: Triplet comparison learning is an emerging paradigm that learn froms comparative feedback data. A typical triplet comparison data, denoted as (xa, xb, xc), conveys the relative similarity information that instance xa is more similar to xb than to xc. M-tuple similarity-confidence learning [30]: The pro- posed Msconf framework extends the Sconf learning paradigm to M-tuples of varying sizes. It leverages similarity-confidence information across multiple instances by jointly modeling their relative confidence levels and inter-instance similarity. C. The Proposed Methods and Common Setup This subsection outlines the implementation details and ex- perimental settings used to evaluate the proposed methods. We assess their performance on several benchmark datasets. For MNIST and Fashion-MNIST, we use a multilayer perceptron (MLP), while for SVHN and CIFAR-10, we adopt a ResNet- based architecture. Training data are sampled from the original datasets and partitioned into positive and negative classes. Subsequently, N-tuples data under different scenarios are con- structed according to the class distributions, and combined with unlabeled instances for training. In our experiments, the loss function ℓ(z) is chosen as the sigmoid loss. The performance of the specific tasks is assessed by minimizing the empirical risk in Eq. (22), (31) and (35). During training, the learning rate and weight parameters are selected from the range {10−6, ..., 10−1}. All experiments are implemented in PyTorch and executed on an NVIDIA GeForce RTX 3080 GPU. 9 TABLE V: The average classification accuracy and standard error over 5 trials are reported under varying base datasets and category priors in specific experimental settings. The best performance for each method is highlighted in bold. Tasks of N-tuples learning Baseline methods Prior Dataset NcompU NSU MNU NposU KM triplet comparison NT-Comp triplet sconf τ+=0.8 MNIST 95.19(0.91) 95.31(0.22) 91.87(2.11) 83.09(1.74) 68.57(3.14) 70.67(4.48) 89.03(0.33) 92.01(0.83) FASHION 96.17(0.36) 93.66(2.04) 88.45(1.21) 85.11(2.46) 70.22(1.99) 71.85(1.66) 90.78(1.88) 86.44(1.45) SVHN 74.40(0.99) 78.58(0.26) 73.89(1.77) 56.22(3.41) 53.94(4.44) 63.67(4.48) 67.52(0.87) 73.25(2.63) CIFAR-10 77.37(0.99) 80.07(0.66) 78.66(1.27) 53.72(4.56) 59.05(5.72) 64.74(3.57) 73.28(1.23) 74.22(3.51) τ+=0.6 MNIST 92.27(0.42) 88.08(2.42)) 83.14(3.33) 82.55(4.25) 68.75(2.08) 71.78(2.28) 90.14(0.76) 90.74(0.53) FASHION 94.34(0.33) 90.61(1.44) 88.21(3.11) 86.45(2.99) 68.25(2.54) 73.44(2.48) 91.01(1.56) 91.18(0.68) SVHN 60.08(2.73) 67.94(1.93) 54.75(3.81) 50.55(1.54) 57.42(3.59) 59.33(1.72) 66.45(2.14) 72.31(4.21) CIFAR-10 73.46(1.08) 78.71(0.29) 63.42(3.02) 52.36(1.83) 60.77(6.72) 61.85(2.77) 71.55(1.02) 70.01(3.19) τ+=0.2 MNIST 93.72(0.56) 92.59(0.41) 90.80(0.52) 86.24(1.42) 64.72(4.66) 71.44(1.66) 91.59(0.77) 91.79(0.96) FASHION 94.45(0.27) 93.33(0.79) 87.13(0.87) 94.10(0.20) 83.07(3.51) 72.08(3.59) 92.88(1.54) 93.01(1.41) SVHN 66.06(2.05) 77.56(2.50) 71.37(2.75) 55.36(2.19) 55.98(4.15) 59.41(2.33) 64.27(3.75) 75.31(4.22) CIFAR-10 75.40(2.13) 82.27(0.82) 81.76(1.85) 56.22(1.55) 58.01(4.88) 59.55(3.74) 73.48(3.11) 74.21(2.94) (a) MNIST (b) Fashion-MNIST (c) SVHN (d) Cifar-10 Fig. 2: The NcompU method suffers from overly negative empirical risk on the training set, and the impact of the correction function is accordingly illustrated. (a) MNIST (b) Fashion-MNIST (c) SVHN (d) Cifar-10 Fig. 3: The impact of varying the number of pointwise unlabeled data on the performance of different N-tuple learning settings across benchmark datasets. (a) MNIST (b) Fashion-MNIST (c) SVHN (d) Cifar-10 Fig. 4: Comparison of classification performance across four N-tuple-based supervision scenarios under various loss functions. D. Experiment Results and Analysis We summarize and analyze the performance of our proposed frameworks under different weakly supervised settings using benchmark datasets. The main findings are as follows: As summarized in Table V, iour generalized N-tuple learn- ing framework consistently outperforms baseline methods 10 Fig. 5: Comparison of Loss Functions. across all four weak supervision scenarios, highlighting the effectiveness of the proposed risk function in fully leveraging the weak supervision within N-tuple constraints, as well as the auxiliary signal provided by pointwise unlabeled data. However, Figure 2 reveals that the empirical training risk may become negative, indicating a risk of overfitting. The improved classification performance after applying our correction func- tion confirms its effectiveness in mitigating such overfitting, thereby emphasizing the necessity of incorporating correction mechanisms in weakly supervised settings. Figure 3 demonstrates that increasing the number of point- wise unlabeled samples consistently enhances classification accuracy for all N-tuple-based methods, especially on more challenging datasets. This trend suggests that pointwise unla- beled data play a critical role in improving generalization by facilitating better estimation of class-conditional distributions and decision boundaries. Among the methods evaluated, the NcompU and NSU variants exhibit the strongest performance across all datasets, with accuracy steadily improving as more unlabeled data are introduced. In contrast, the NposU method yields the lowest performance with limited improvement. This observation indicates that more complex data distributions make it more challenging for the model to capture discrimi- native features, thereby leading to reduced performance. Figure 4 reveals distinct performance patterns across dif- ferent loss functions, while Figure 5 further highlights their respective characteristics. Among the evaluated losses, the sigmoid loss consistently outperforms both logistic and double hinge losses across the four N-tuple weak supervision scenar- ios. This superior performance may be attributed to its smooth gradient and probabilistic nature, which enable more stable learning under the uncertainty inherent in N-tuple constraints. These results underscore the importance of selecting loss functions that align well with the structural properties of weak supervision frameworks. Across all methods, we observe a clear performance gap between simpler datasets (e.g., MNIST, Fashion-MNIST) and more complex ones (e.g., SVHN, CIFAR-10), reflecting the in- herent limitations of weakly supervised N-tuple learning when applied to high-dimensional and complex data. Nevertheless, the proposed unified framework maintains stable performance trends, demonstrating its adaptability across various levels of data complexity, while also pointing to future opportunities for enhancement in more challenging scenarios. VII. CONCLUSION. This paper presents a generalized learning framework for N- tuples data aimed at reducing annotation costs in supervised learning. By unifying the generation processes of N-tuples and pointwise unlabeled data under a common distributional representation, we derive an unbiased empirical risk estimator that subsumes a broad range of existing N-tuples methods. We further instantiate the framework in four representative weakly supervised learning scenarios, illustrating its broad applicabil- ity and showing that each can be derived as a specific instance of the proposed general model. The proposed framework not only provides a systematic and theoretically grounded solution for various N-tuples learning scenarios but also demonstrates improved generalization performance through the incorpora- tion of pointwise unlabeled data . This unified perspective offers a practical and versatile approach for handling complex N-tuples structures in real-world applications. Future work will focus on deploying the proposed framework in real-world complex datasets to validate its effectiveness and enhance its scalability in practical applications. REFERENCES [1] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” National science review, vol. 5, no. 1, pp. 44–53, 2018. [2] H. Wei, R. Xie, L. Feng, B. Han, and B. An, “Deep learning from multiple noisy annotators as a union,” IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 12, pp. 10552–10562, 2023. [3] Z.-Y. Zhang, P. Zhao, Y. Jiang, and Z.-H. Zhou, “Learning from incom- plete and inaccurate supervision,” IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 12, pp. 5854–5868, 2022. [4] Y. Cao, L. Feng, Y. Xu, B. An, G. Niu, and M. Sugiyama, “Learning from similarity-confidence data,” in International Conference on Ma- chine Learning, pp. 1272–1282, PMLR, 2021. [5] M. Du Plessis, G. Niu, and M. Sugiyama, “Convex formulation for learning from positive and unlabeled data,” in International conference on machine learning, pp. 1386–1394, PMLR, 2015. [6] M. C. Du Plessis, G. Niu, and M. Sugiyama, “Analysis of learning from positive and unlabeled data,” Advances in neural information processing systems, vol. 27, 2014. [7] T. Sakai, G. Niu, and M. Sugiyama, “Semi-supervised auc optimization based on positive-unlabeled learning,” Machine Learning, vol. 107, no. 4, pp. 767–794, 2018. [8] M.-K. Xie and S.-J. Huang, “Partial multi-label learning with noisy label identification,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp. 3676–3687, 2022. [9] L. Feng and B. An, “Partial label learning with self-guided retraining,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 3542–3549, 2019. [10] X. Gong, D. Yuan, and W. Bao, “Discriminative metric learning for partial label learning,” IEEE Transactions on Neural Networks and Learning Systems, vol. PP. [11] L. Feng, T. Kaneko, B. Han, G. Niu, B. An, and M. Sugiyama, “Learning with multiple complementary labels,” in International Conference on Machine Learning, pp. 3072–3081, PMLR, 2020. [12] T. Ishida, G. Niu, W. Hu, and M. Sugiyama, “Learning from comple- mentary labels,” Advances in neural information processing systems, vol. 30, 2017. 11 [13] T. Ishida, G. Niu, A. Menon, and M. Sugiyama, “Complementary-label learning for arbitrary losses and models,” in International Conference on Machine Learning, pp. 2971–2980, PMLR, 2019. [14] T. Ishida, G. Niu, and M. Sugiyama, “Binary classification from positive- confidence data,” Advances in neural information processing systems, vol. 31, 2018. [15] N. Lu, S. Lei, G. Niu, I. Sato, and M. Sugiyama, “Binary classification from multiple unlabeled datasets via surrogate set classification,” in International Conference on Machine Learning, pp. 7134–7144, PMLR, 2021. [16] N. Lu, T. Zhang, G. Niu, and M. Sugiyama, “Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach,” in International Conference on Artificial Intelligence and Statistics, pp. 1115–1125, PMLR, 2020. [17] N. Lu, G. Niu, A. K. Menon, and M. Sugiyama, “On the minimal supervision for training any binary classifier from only unlabeled data,” arXiv preprint arXiv:1808.10585, 2018. [18] L. Feng, S. Shu, N. Lu, B. Han, M. Xu, G. Niu, B. An, and M. Sugiyama, “Pointwise binary classification with pairwise confidence comparisons,” in International Conference on Machine Learning, pp. 3252–3262, PMLR, 2021. [19] J. Li, S. Huang, C. Hua, and Y. Yang, “Learning from pairwise confidence comparisons and unlabeled data,” IEEE Transactions on Emerging Topics in Computational Intelligence, pp. 1–13, 2024. [20] H. Bao, G. Niu, and M. Sugiyama, “Classification from pairwise similarity and unlabeled data,” in International Conference on Machine Learning, pp. 452–461, PMLR, 2018. [21] S. Huang, J. Li, C. Hua, et al., “Learning from not-all-negative pairwise data and unlabeled data,” Pattern Recognition, p. 111442, 2025. [22] J. Li, S. Huang, C. Hua, and Y. Yang, “Binary classification from n-tuple comparisons data,” Neural Networks, vol. 182, p. 106894, 2025. [23] T. Shimada, H. Bao, I. Sato, and M. Sugiyama, “Classification from pairwise similarities/dissimilarities and unlabeled data via empirical risk minimization,” Neural Computation, vol. 33, no. 5, pp. 1234–1268, 2021. [24] J. Li, S. Huang, C. Hua, and Y. Yang, “Learning from n-tuple similar- ities and unlabeled data,” IEEE Transactions on Artificial Intelligence, vol. PP. [25] P. L. Bartlett and S. Mendelson, “Rademacher and gaussian complexi- ties: Risk bounds and structural results,” Journal of Machine Learning Research, vol. 3, no. Nov, pp. 463–482, 2002. [26] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of machine learning. MIT press, 2018. [27] R. Kiryo, G. Niu, M. C. Du Plessis, and M. Sugiyama, “Positive- unlabeled learning with non-negative risk estimator,” Advances in neural information processing systems, vol. 30, 2017. [28] J. MacQueen et al., “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, pp. 281–297, Oakland, CA, USA, 1967. [29] Z. Cui, N. Charoenphakdee, I. Sato, and M. Sugiyama, “Classification from triplet comparison data,” Neural Computation, vol. 32, pp. 659– 681, 03 2020. [30] J. Li, J. Qin, C. Hua, and Y. Yang, “Binary classification from m-tuple similarity-confidence data,” IEEE Transactions on Emerging Topics in Computational Intelligence, 2025. Shuying Huang obtained her bachelor’s degree in Automation from Yantai University, Yantai, China in 2019 and is currently pursuing Ph.D. degree in Control Engineering from Yanshan University, Qinhuangdao, China. Her research interests are in weakly supervised machine learning. Junpeng Li received the B.Sc. degree in Biomedical Engineering and the Ph.D. degree in Control science and Engineering, both from Yanshan University, China, in 2010 and 2016, respectively. Currently, he is Full Professor in the Department of Automation at Yanshan University, China. His current research interests include system modeling, machine learning and intelligent optimization. Changchun Hua received the Ph.D degree in elec- trical engineering from Yanshan University, Qin- huangdao, China, in 2005. He was a research Fellow in National University of Singapore from 2006 to 2007. From 2007 to 2009, he worked in Carleton University, Canada, funded by Province of Ontario Ministry of Research and Innovation Program. From 2009 to 2010, he worked in University of Duisburg- Essen, Germany, funded by Alexander von Hum- boldt Foundation. Now he is a full Professor in Yanshan University, China. He is the author or coauthor of more than 80 papers in mathematical, technical journals, and conferences. He has been involved in more than 10 projects supported by the National Natural Science Foundation of China, the National Education Committee Foundation of China, and other important foundations. His re- search interests are in nonlinear control systems, control systems design over network, teleoperation systems and intelligent control. Yana Yang received her Ph.D. degree in Electrical Engineering from Yanshan University, Qinhuangdao, China, in 2017. Now she is currently a Associate Professor of Department of Automation, Yanshan University, Qinhuangdao 066004, China. She is the author or coauthor of more than 20 papers in math- ematical, technical journals, and conferences. Her research interests are in nonlinear teleoperation sys- tem control, nonlinear control systems, robot system control and sliding mode control.