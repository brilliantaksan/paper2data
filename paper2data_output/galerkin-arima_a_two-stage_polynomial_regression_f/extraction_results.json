{
  "content": {
    "metadata": {
      "page_count": 13,
      "title": "Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting",
      "author": "Haojie Liu; Zihan Lin",
      "subject": "",
      "creator": "arXiv GenPDF (tex2pdf:)",
      "creation_date": ""
    },
    "full_text": "Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting Haojie Liu Department of Economics University of California, Riverside hliu332@ucr.edu Zihan Lin Department of Economics University of California, Riverside zlin169@ucr.edu Abstract Time-series models like ARIMA remain widely used for forecasting but limited to linear assumptions and high computational cost in large and complex datasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA and replace it with a flexible spline-based function estimated by Galerkin projection. This enables the model to capture nonlinear dependencies in lagged values and retain the MA component and Gaussian noise assumption. We derive a closed-form OLS estimator for the Galerkin coefficients and show the model is asymptotically unbiased and consistent under standard conditions. Our method bridges classical time-series modeling and nonparametric regression, which offering improved forecasting performance and computational efficiency. 1 Introduction Time-series forecasting plays a critical role in economics, finance, energy demand, weather prediction, and more. It turns historical data into proactive insight to understand temporal patterns. The Autoregressive Integrated Moving Average (ARIMA) model remains one of the most widely used time-series models today [Vuong et al., 2024]. However, as the volume and complexity of temporal data continue to grow, ARIMA becomes increasingly slow during forecasting—the more temporal data, the longer it takes [Wang et al., 2023]. Its speed is insufficient for handling the growing complexity of modern time series. Moreover, ARIMA assumes a fixed linear relationship among past values, which can be overly restrictive when the true dynamics are nonlinear or complex [Thupeng et al., 2024, Vuong et al., 2024]. Partial differential equations (PDEs) and stochastic differential equations (SDEs) are widely used in statistics to describe continuous-time systems with uncertainty [Graham et al., 2023]. They appear in many areas, such as asset pricing in finance or diffusion-based sampling in Bayesian inference [Dokuchaev et al., 2022]. Galerkin projection is usually used to solve the problems, which is a numerical method that approximates complex functions using basis expansions [Dokuchaev et al., 2022]. We apply this idea to time series forecasting in a discrete setting. To address this, we propose Galerkin-ARIMA, which uses the Galerkin method to replace the linear autoregressive component of the ARIMA model with a spline-based function learned via Galerkin projection [Sun et al., 2022]. Specifically, we approximate the AR term using a set of basis functions over the lag space, allowing the model to flexibly capture nonlinear dependencies [Kushnir and Tokarieva, 2023]. The MA term and Gaussian noise assumptions of ARIMA are retained. We show that under regular conditions, the Galerkin-ARIMA model is asymptotically consistent and unbiased, and it can be more computationally efficient than traditional maximum likelihood 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2507.07469v1 [stat.ML] 10 Jul 2025 estimation methods commonly used for ARIMA models. Our approach combines the structure of ARIMA with the flexibility of nonparametric regression and offers a new path forward for modeling complex time series dynamics—achieved through a closed form solution based on Ordinary Least Squares (OLS). 2 ARIMA The Autoregressive Integrated Moving Average (ARIMA) model is a fundamental time-series model that combines autoregressive (AR) terms, integration (differencing), and moving-average (MA) terms [Box and Jenkins, 1970, Brockwell and Davis, 1991]. In an ARIMA(p, d, q) model, the series is differenced d times to achieve stationarity, and then modeled as an ARMA(p, q). Using the backshift operator B (defined by Byt = yt−1), we can write the differenced series as: y(d) t = ∆dyt = (1 −B)d yt, where y(d) t denotes the d-th order differenced series. We then define the p-dimensional state (or lag) vector xt = y(d) t−1, y(d) t−2, . . . , y(d) t−p \u0001 ∈Rp, which collects the last p observed values of the differenced series. The AR part of the model is a linear combination of these lagged values: mt = p X i=1 ψi y(d) t−i, where ψi are the autoregressive coefficients. We assume the random shocks (innovations) {ϵt} are independent and identically distributed (i.i.d.) Gaussian with mean 0 and variance σ2, i.e. ϵt ∼N(0, σ2). The full ARIMA(p, d, q) model can then be written as: y(d) t = mt + q X j=1 θj ϵt−j + ϵt, which means the differenced series y(d) t is explained by the AR term mt and a moving-average term (a linear combination of the last q noise terms) plus the current noise. This formulation is standard for ARIMA models [see, e.g., Box and Jenkins, 1970]. We will next discuss how to estimate the coefficients Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) in this model. 2.1 Maximum Likelihood Estimation (MLE) The parameters of an ARIMA model are typically estimated by maximizing the likelihood of the observed data under the model [Brockwell and Davis, 1991]. For a Gaussian ARIMA model, maximizing the likelihood is equivalent to minimizing the sum of squared one-step-ahead forecast errors. Let Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) collect all the unknown parameters. We can write the one-step prediction error at time t (for given parameters Ψ) as: ϵt(Ψ) = y(d) t − p X i=1 ψi y(d) t−i − q X j=1 θj ϵt−j(Ψ) . This ϵt(Ψ) represents the residual at time t, i.e. the difference between the observed differenced value y(d) t and its predicted value based on the model with parameters Ψ. The log-likelihood of the data (up to an additive constant) can be written as a function of Ψ: L(Ψ) = −N 2 ln(σ2) − 1 2σ2 N X t=1 ϵt(Ψ)2 , where N is the number of observations in the time series after differencing. In this expression, we have assumed Gaussian innovations so that ϵt(Ψ) are normally distributed. Maximizing L(Ψ) is equivalent to minimizing the sum of squared residuals P t ϵt(Ψ)2. The maximum likelihood estimator (MLE) ˆΨ is defined as: ˆΨ = arg max Ψ L(Ψ) . 2 In practice, this optimization does not have a closed-form solution for ARMA models and must be carried out by iterative numerical methods (e.g., Newton-Raphson or gradient descent) [Brockwell and Davis, 1991]. The above formulation highlights that the MLE seeks the parameter values that best explain the data by producing the smallest prediction errors. In summary, classical ARIMA modeling involves finding linear coefficients (ψi and θj) that minimize the prediction error. In the next section, we introduce a Galerkin approximation approach, which generalizes the AR component to a flexible function (using basis splines) rather than a fixed linear form. This will lead to the Galerkin-ARIMA method, where we aim to improve forecasting by capturing possibly nonlinear relationships in the lagged values. 3 Galerkin Approximation Before introducing the Galerkin-ARIMA model, we briefly review the Galerkin method in the context of stochastic differential equations (SDEs). Consider a one–dimensional SDE: dXt = b(Xt) dt + σ(Xt) dWt , where b(x) is the drift function and σ(x) is the diffusion coefficient. Suppose the drift function b(x) is expensive to compute or is unknown; we wish to approximate b(x) using a finite–dimensional function space spanned by a set of basis functions. Let {ϕj(x)}K j=1 be a family of basis functions (for example, B-splines of degree p on the domain Ω). We postulate an approximating drift of the form: bK(x) = K X j=1 θj ϕj(x) , with unknown coefficients θ = (θ1, . . . , θK) to be determined by a Galerkin projection. The Galerkin method requires that the residual (error) between the true drift and the approximated drift is orthogonal to the space spanned by the basis. Define the residual function r(x) = b(x) −bK(x). We introduce an inner product on the space of functions (weighted by a probability measure µ(x) on Ω): ⟨f, g⟩= Z Ω f(x) g(x) µ(x) dx . The Galerkin orthogonality condition imposes that the residual is orthogonal to each basis function ϕi: ⟨r, ϕi⟩= 0, i = 1, . . . , K. Substituting r(x) = b(x) −PK j=1 θjϕj(x), this condition becomes: Z Ω \u0010 b(x) − K X j=1 θj ϕj(x) \u0011 ϕi(x) µ(x) dx = 0, for each i = 1, . . . , K. Define the Gram matrix G ∈RK×K and the vector f ∈RK by: Gij = Z Ω ϕi(x) ϕj(x) µ(x) dx, fi = Z Ω b(x) ϕi(x) µ(x) dx. The Galerkin conditions then yield a linear system G θ = f. Once solved for θ, we obtain the Galerkin approximation bK(x) = PK j=1 θjϕj(x). Substituting this back into the SDE gives: dXt = bK(Xt) dt + σ(Xt) dWt , which is a spline-based approximation of the original dynamics. 3.1 AR-term Approximation via Galerkin Method We now return to time-series modeling. The key idea of Galerkin-ARIMA is to replace the fixed linear AR structure with a more flexible function approximated by basis functions, while still retaining the MA component. Essentially, we treat the p-dimensional lag vector as an input to an unknown 3 function f(·) (analogous to a drift function in an SDE), and we approximate this function using a finite basis expansion. Let Φ(xt) = [ϕ1(xt), ϕ2(xt), . . . , ϕK(xt)]T ∈RK be a vector of K basis function evaluations at the state xt = (y(d) t−1, . . . , y(d) t−p). For example, each ϕj could be a multi-variate B-spline or some polynomial basis defined on the lag space. We then posit that the AR component mt can be represented as: mt = Φ(xt)T β , β ∈RK, where β = (β1, . . . , βK)T are the coefficients of the basis expansion. In other words, mt is the Galerkin approximation of the potentially nonlinear relationship between past values and the next differenced value y(d) t . The Galerkin-ARIMA model then becomes: y(d) t = mt + q X j=1 θj ϵt−j + ϵt , similar in form to the classical ARIMA, but with mt now given by a flexible spline-based function rather than a fixed linear combination of lags. Estimating the coefficients in this Galerkin framework can be done in closed form using least squares, thanks to the linearity in the parameters β. First, we fit the basis-function coefficients β to approximate the AR part. This can be done by regressing y(d) t onto Φ(xt). In matrix terms, let Y = [y(d) 1 , y(d) 2 , . . . , y(d) N ]T be the vector of responses (differenced series), and let Φ be the N × K matrix whose t-th row is Φ(xt)T . Then the ordinary least squares (OLS) solution for β is: ˆβ = (ΦT Φ)−1 ΦT Y , provided N > K and the basis functions are linearly independent. This ˆβ minimizes the sum of squared errors P t(y(d) t −Φ(xt)T β)2. Using ˆβ, we can compute the fitted residuals (initially ignoring the MA terms) as: ˆϵ(0) t = y(d) t −Φ(xt)T ˆβ , for each t. These residuals ˆϵ(0) t represent the remaining structure after accounting for the spline-based AR approximation. Next, to estimate the moving-average coefficients θ1, . . . , θq, we include the lagged residuals in a sec- ond regression. For each t, form the vector of the last q residuals ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−q]T . Now define the combined regression design matrix Ψ of dimension N × (K + q) whose t-th row is [Φ(xt)T | ˆϵ(0) T t−1:t−q]. In other words, Ψt = [ϕ1(xt), . . . , ϕK(xt) | ˆϵ(0) t−1, . . . , ˆϵ(0) t−q] includes both the basis functions and the lagged residuals. Let γ = (βT , θ1, . . . , θq)T be the combined coefficient vector of length K + q. Then an OLS estimate for γ is given by: ˆγ = (ΨT Ψ)−1 ΨT Y . This yields refined estimates ˆβ (the first K entries of ˆγ) and ˆθ1, . . . , ˆθq (the last q entries of ˆγ) simultaneously. Once we have these estimates, we can form the final fitted model: ˆmt = Φ(xt)T ˆβ , ˆy(d) t+1 = ˆmt+1 + q X j=1 ˆθj ˆϵt+1−j , which provides one-step-ahead forecasts for the differenced series. (Here ˆϵt+1−j are the in-sample residuals at time t + 1 −j.) In essence, ˆmt is a nonparametric (spline-based) estimate of the AR part, and the ˆθj adjust for the remaining serial correlation (the MA part). Under standard conditions for nonparametric regression (in particular, as K →∞and K/N →0 as N →∞), the spline approximation mt = Φ(xt)T β can approximate any sufficiently smooth function of the lags. This suggests that as we increase the number of basis functions K, the Galerkin-ARIMA model can in principle capture the true underlying relationship among the past values arbitrarily well. 4 4 Error Analysis One major purpose of proposing the Galerkin-ARIMA algorithm is to achieve better accuracy or efficiency than classical ARIMA. We therefore analyze the bias, variance, and computational complexity of the new model, comparing it to the standard ARIMA approach. 4.1 Bias First, consider the data-generating process to be a linear ARMA(p, q) process: yt = p X i=1 ψ∗ i yt−i + q X j=1 θ∗ j ϵt−j + ϵt, ϵt iid ∼N(0, σ2) , and suppose we fit the model with the correct orders p and q. Based on standard time-series theory [see Hannan and Rissanen, 1982, Brockwell and Davis, 1991], under regular conditions the estimators of the true parameters are consistent. In particular, for one-step-ahead prediction we have ˆΨ →Ψ∗ (in probability) as N →∞, where Ψ∗= (ψ∗ 1, . . . , ψ∗ p, θ∗ 1, . . . , θ∗ q, σ2) are the true parameters. Consequently, the 1-step forecast from a correctly specified ARIMA model converges in probability to the true value: ˆyt+1|t = p X i=1 ˆψi,N yt+1−i + q X j=1 ˆθj,N ˆϵt+1−j −→ p yt+1 , where ˆψi,N and ˆθj,N are the estimates based on N observations. In particular, the asymptotic bias of the one-step forecast is zero: p−limN→∞ \u0010 yt+1 −ˆyt+1|t \u0011 = 0 . This confirms that a well-specified ARIMA model is asymptotically unbiased under the usual assumptions. Now consider the Galerkin-ARIMA model. Let the lag-response relationship be some smooth but unknown function f : Rp →R such that y(d) t = f(xt)+ (MA terms) + noise. We approximate f(x) by the spline expansion mt = Φ(xt)T β using K basis functions. Under standard smoothness conditions (e.g. f belongs to a Hölder class with smoothness index r) and for x in a compact domain, the Jackson-Bernstein inequality from approximation theory guarantees that the approximation error can be made arbitrarily small as K grows. In particular, one result (see, e.g., de Boor, 1978) states that: inf β∈RK sup x∈C f(x) −Φ(x)T β = O(K−r/p) , where r characterizes the smoothness of f. This means the bias due to using a finite basis of size K decreases as K increases (assuming f is sufficiently smooth). Thus, if we let K →∞as N →∞ (but slowly enough that K/N →0; see below), the bias of ˆmt as an estimator of the true f(xt) will tend to zero. In summary, the Galerkin-ARIMA model can also be made asymptotically unbiased: as the number of basis functions grows, it can recover the true AR relationship without systematic error. 4.2 Variance and Consistency For the classical ARIMA MLE, the parameter estimates are asymptotically normal. In fact, if ˆΨ is the MLE of Ψ∗, then under regular conditions: √ N (ˆΨ −Ψ∗) d−→N 0, I(Ψ∗)−1\u0001 , where I(Ψ∗) is the Fisher information matrix evaluated at the true parameters [Brockwell and Davis, 1991]. This implies Var(ˆΨ) = O(1/N) for each fixed number of parameters (since the information matrix converges to a constant). In the Galerkin-ARIMA approach, the number of parameters γ = (β1, . . . , βK, θ1, . . . , θq) grows with K. The OLS variance-covariance for ˆγ is: Var(ˆγ) = σ2 (ΨT Ψ)−1 ≈σ2 1 N \u0010 p−limN→∞ ΨT Ψ N \u0011−1 . 5 Roughly speaking, each coefficient in γ has variance on the order of 1/N, but there are (K + q) coefficients. If K grows with N, this introduces a trade-off between bias and variance. In fact, Var(ˆγ) will scale on the order of (K + q)/N. To keep the variance of each coefficient bounded, we require K = o(N) (so that (K + q)/N →0). Typically, in nonparametric regression one balances bias and variance by letting K grow at a suitable rate relative to N. A common choice (optimal in a minimax sense for r-smooth functions in Rp) is K ∼N p 2r+p [Stone, 1985]. With such a choice, one can show that if f has Hölder smoothness r and we let K →∞with K/N →0, then ∥ˆf −f∥2 2 = O \u0010 K−2r/p + K N \u0011 , which is the standard convergence rate for nonparametric regression [Stone, 1985]. By plugging in K ∼N p/(2r+p), the two terms in the error bound become of the same order, yielding ∥ˆf −f∥2 2 = O(N −2r/(2r+p)), which approaches 0 as N →∞. This result implies that, with an appropriate growth of K and under smoothness assumptions, the Galerkin-ARIMA estimator ˆf is consistent for the true function f(x). 4.3 Computational Complexity We briefly compare the computational complexity of fitting a standard ARIMA model versus the Galerkin-ARIMA model. In a linear ARIMA model, evaluating the objective (likelihood or sum of squared errors) for a given parameter guess and computing gradients typically requires O((p + q)N) operations per iteration (because each residual ϵt depends on p + q past terms). Let I be the number of iterations needed for convergence of the numerical optimizer. Then the overall complexity of computing the MLE is on the order of O(I · (p + q) N). In practice, I (for something like Newton’s method) might be larger than p + q, and each iteration can be costly for long series. In the Galerkin-ARIMA approach, the main cost comes from two least-squares fits. Constructing the basis function matrix Φ costs O(p N · K) if each of the K basis functions takes O(p) to evaluate for a given xt (for example, polynomial splines might be evaluated in constant time per basis). Solving the OLS for β by direct methods is O(NK2) (since computing (ΦT Φ) is O(NK2) and inverting the K × K matrix is O(K3), though if K is not too large this is manageable). Solving the second OLS for γ is O(N(K + q)2) by a similar reasoning. Combining these, the total complexity for Galerkin-ARIMA is approximately OGAL ≈O(p N K) + O(N(K + q)2) . For reasonably chosen K, this can be significantly smaller than the ARIMA MLE cost. For instance, if I is on the order of N 1/2 (a rough scenario for some difficult likelihood surfaces) or if p and q are moderately large, the direct OLS approach can be advantageous. The ratio of complexities is roughly: OARIMA OGAL ≈I (p + q) N N (K + q)2 = I (p + q) (K + q)2 . In many situations, we have I > p ≈q and we might choose K much smaller than N. As a concrete example, for one-step forecasting (M = 1), if I ∼100, p+q ∼10, and K ∼10, then the above ratio suggests ARIMA MLE could be around 100·10 (10+10)2 = 1000 400 = 2.5 times slower than Galerkin-ARIMA per forecast. For rolling forecasts over a window of M time points, the computational savings could be multiplied further by a factor on the order of M. Of course, the exact speed gain depends on implementation details and the cost of basis function evaluations, but this analysis indicates that Galerkin-ARIMA has the potential to be computationally more efficient, especially for long series or when a moderate number of basis functions suffices. 5 Numerical Experiments 5.1 Synthetic Data Generation All series have length n = 300 and are generated with a fixed random seed for reproducibility. We consider four toy processes: 1. Noisy ARMA(2,1): yt = 0.6 yt−1 −0.3 yt−2 + 0.5 ϵt−1 + ϵt, ϵt ∼N(0, 1), y1 = y2 = 0. 6 2. Seasonal sine + noise: yt = sin \u0010 2π t 20 \u0011 + 0.5 ηt, ηt ∼N(0, 1). 3. Linear trend + AR(1): yt = 0.01 t + 0.8 yt−1 + νt, νt ∼N(0, 0.52), y0 = 0. 4. Nonlinear recursion + noise: yt = 0.5 yt−1 −0.2 y2 t−1 + ξt, ξt ∼N(0, 0.72), y0 = 0. These four processes span a wide variety of dynamics—linear autoregression with a moving-average component, periodic fluctuations, deterministic trend combined with stochastic feedback, and a purely nonlinear recursion. By testing on data that exhibit both classical ARMA behavior and more challenging seasonal, trending, and nonlinear patterns, we can rigorously evaluate whether Galerkin-ARIMA’s flexible basis expansion captures complex dependencies that a standard ARIMA may miss, while still recovering simple linear structure when appropriate. 5.2 Forecasting Setup We perform rolling one-step-ahead forecasts on each series using: • a fixed training window of length W = 100, • a forecast horizon of H = 150, and • Monte Carlo replications R = 10 (to average out noise). At each time i = W, . . . , W + H −1, we fit two models with the same order pair (p, q): Both ARIMA(p, 0, q) and Galerkin-ARIMA(p, q) are evaluated at the same set of order pairs (p, q) ∈{(1, 0), (5, 0), (0, 1), (0, 5), (1, 5), (5, 1)}. The ARIMA(p, 0, q) model is fit by maximizing the Gaussian likelihood via the statsmodels.tsa.arima.ARIMA routine, where p denotes the number of autoregressive lags and q the number of moving-average lags. The Galerkin-ARIMA(p, q) model employs a two-stage Galerkin approximation: in the first (AR) stage, we regress yt on the polynomial basis {1, yt−1, . . . , yt−p, y2 t−1, . . . , y2 t−p} to obtain coefficients ˆβ and residuals ˆϵ(0) t , and in the second (MA) stage, we regress these residuals on the basis {1, ˆϵ(0) t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ(0) t−q)2} to obtain coefficients ˆα for the moving-average component. For each model and each (p, q) combina- tion, we record the mean absolute error (MAE), root-mean-squared error (RMSE), total CPU time, and average CPU time per fit over the H rolling forecasts, then average these metrics over the R Monte Carlo runs. 5.3 Result Table 1 collects the MAE, RMSE, total and per-fit CPU times for ARIMA and Galerkin-ARIMA across all four synthetic datasets and six choices of (p, q). A few key patterns emerge. First, when (p, q) fail to cover the true dynamics (e.g. pure MA or pure AR fits), both methods exhibit elevated MAE/RMSE, but Galerkin-ARIMA sometimes smooths slightly more aggressively, yielding marginally higher errors. Second, as soon as (p, q) match or exceed the underlying orders (for example (1, 5) or (5, 1) on the Noisy_ARMA data, or (5, 0) on the Trend_AR data), the accuracy of Galerkin-ARIMA converges to that of classical ARIMA—the two lines in the corresponding panels become virtually indistinguishable. Third, across every dataset and order, Galerkin-ARIMA runs orders of magnitude faster: total CPU time drops from seconds to hundredths of a second, and average per-fit time falls from ∼10−2s to ∼10−4s. In short, Galerkin-ARIMA attains almost identical forecasting performance to ARIMA as soon as the basis dimension is sufficient, while dramatically reducing computation time. 7 Dataset p q Algorithm MAE RMSE Total Time Avg Time Noisy_ARMA 0 1 ARIMA 0.8636 1.0457 1.7318 0.0115 Galerkin-ARIMA 0.9521 1.1846 0.0743 0.0005 0 5 ARIMA 0.8324 1.0045 4.1475 0.0276 Galerkin-ARIMA 0.8331 1.0148 0.0937 0.0006 1 0 ARIMA 0.9494 1.1816 1.2780 0.0085 Galerkin-ARIMA 0.9521 1.1846 0.0405 0.0003 1 5 ARIMA 0.8382 1.0110 11.1948 0.0746 Galerkin-ARIMA 0.8402 1.0249 0.1021 0.0007 5 0 ARIMA 0.8320 1.0145 3.8751 0.0258 Galerkin-ARIMA 0.8331 1.0148 0.0605 0.0004 5 1 ARIMA 0.8413 1.0272 6.4902 0.0433 Galerkin-ARIMA 0.8298 1.0133 0.0994 0.0007 Nonlinear 0 1 ARIMA 0.5441 0.6742 1.2896 0.0086 Galerkin-ARIMA 0.5180 0.6546 0.0741 0.0005 0 5 ARIMA 0.5312 0.6614 3.4696 0.0231 Galerkin-ARIMA 0.5405 0.6790 0.0945 0.0006 1 0 ARIMA 0.5296 0.6595 1.1266 0.0075 Galerkin-ARIMA 0.5180 0.6546 0.0409 0.0003 1 5 ARIMA 0.5373 0.6665 9.2855 0.0619 Galerkin-ARIMA 0.5343 0.6732 0.1013 0.0007 5 0 ARIMA 0.5321 0.6642 2.9177 0.0195 Galerkin-ARIMA 0.5405 0.6790 0.0608 0.0004 5 1 ARIMA 0.5305 0.6636 7.0720 0.0471 Galerkin-ARIMA 0.5400 0.6809 0.0998 0.0007 Seasonal 0 1 ARIMA 0.5924 0.7261 1.3999 0.0093 Galerkin-ARIMA 0.5305 0.6784 0.0740 0.0005 0 5 ARIMA 0.5280 0.6734 4.2973 0.0286 Galerkin-ARIMA 0.5331 0.6721 0.0940 0.0006 1 0 ARIMA 0.5338 0.6818 1.3825 0.0092 Galerkin-ARIMA 0.5305 0.6784 0.0404 0.0003 1 5 ARIMA 0.5401 0.6745 11.8796 0.0792 Galerkin-ARIMA 0.5349 0.6693 0.1018 0.0007 5 0 ARIMA 0.5291 0.6642 3.8499 0.0257 Galerkin-ARIMA 0.5331 0.6721 0.0600 0.0004 5 1 ARIMA 0.5017 0.6219 12.4039 0.0827 Galerkin-ARIMA 0.5331 0.6756 0.1015 0.0007 Trend_AR 0 1 ARIMA 2.4277 2.6177 1.7387 0.0116 Galerkin-ARIMA 0.3941 0.5017 0.0741 0.0005 0 5 ARIMA 0.8587 0.9952 8.4002 0.0560 Galerkin-ARIMA 0.4165 0.5186 0.0950 0.0006 1 0 ARIMA 0.3879 0.4989 2.1608 0.0144 Galerkin-ARIMA 0.3941 0.5017 0.0407 0.0003 1 5 ARIMA 0.3932 0.4977 11.6516 0.0777 Galerkin-ARIMA 0.4139 0.5252 0.1032 0.0007 5 0 ARIMA 0.3912 0.5000 7.1304 0.0475 Galerkin-ARIMA 0.4165 0.5186 0.0600 0.0004 5 1 ARIMA 0.3917 0.5054 16.7612 0.1117 Galerkin-ARIMA 0.4163 0.5214 0.1010 0.0007 Table 1: Averaged over 10 runs: MAE, RMSE, total and average CPU time for ARIMA vs. Galerkin- ARIMA. 8 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 1: One-step forecasts on the Noisy_ARMA series for various (p, q). All six panels in Figure 1 draw from the same underlying ARMA(2,1) process, and together they vividly illustrate how the autoregressive order p and moving-average order q shape one-step forecasts. When p = 0 (pure MA fits), both ARIMA and Galerkin-ARIMA must rely entirely on past innova- tions: with only q = 1 they flatten out the rapid AR(2) swings and lag behind the true curve, while increasing to q = 5 gives them enough “memory” of past shocks that they begin to partially mimic the missing AR feedback. Conversely, when q = 0 (pure AR fits), a single lag (p = 1) captures general level shifts but not the innovation noise, whereas bumping to p = 5 lets each method recover the AR(2) oscillations almost perfectly—even without any MA component—since the higher-order AR basis soaks up residual randomness. Once both p and q are positive (for example, (p, q) = (1, 5) or (5, 1)), each model’s specification fully spans the true two-lag AR and one-lag MA dynamics. In these richer settings, ARIMA’s maximum-likelihood estimates and our two-stage Galerkin projections converge on virtually identical forecasts, tracing the black “True” series with only vanishing discrepancies. In short, raising either p or q in isolation compensates partially for the missing term, but only the combination of sufficient AR and MA orders unlocks near-perfect recovery of the data-generator. 9 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 2: One-step forecasts on the Seasonal series for various (p, q). Figure 2 displays one-step forecasts on the seasonal sine series yt = sin(2πt/20) + 0.5 ηt. When p = 0 (pure MA), an MA(1) fit (q = 1) overly smooths the oscillations and fails to track the peaks of each cycle, while increasing to q = 5 spreads past shocks across a full period and yields a visibly improved—but still lagged—sinusoidal trace. When q = 0 (pure AR), AR(1) (p = 1) captures the local level but cannot reproduce the periodic up–down swings, whereas AR(5) (p = 5) begins to recover the 20-step seasonality by regressing on multiple lags, albeit with slight phase shifts. Finally, combining both AR and MA terms (p = 1, q = 5 or p = 5, q = 1) gives the best result: the AR component provides the backbone of the sine wave, and the MA component corrects residual phase and amplitude errors. Throughout, Galerkin-ARIMA (red) closely parallels ARIMA (blue), with minor additional smoothing at low orders due to the polynomial residual basis, but converges to essentially identical forecasts once (p, q) span the true seasonal dynamics. 10 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 3: One-step forecasts on the Trend_AR series for various (p, q). Figure 3 displays one-step forecasts on the Trend_AR series yt = 0.01 t+0.8 yt−1 +νt with various (p, q). When p = 0 (pure MA), an MA(1) fit (q = 1) dramatically underestimates the upward drift and remains far below the true curve, while increasing to q = 5 improves the level somewhat but still fails to capture the deterministic trend. When q = 0 (pure AR), an AR(1) fit (p = 1) follows the slope closely—since it directly models the feedback—but the residual noise causes small phase-offsets; raising to AR(5) (p = 5) further refines the fit by absorbing additional past values, producing an almost exact match to the trend. Finally, combining AR and MA (p = 1, q = 5 or p = 5, q = 1) yields similarly excellent forecasts: the AR term captures the linear drift, and the MA term smooths remaining volatility. Throughout, Galerkin-ARIMA (red) mirrors classical ARIMA (blue) with negligible differences once the basis dimension is sufficient, but it falls back to a lower level under purely MA specifications because no trend regressor is included in the residual basis. 11 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 4: One-step forecasts on the Nonlinear series for various (p, q). Figure 4 shows one-step forecasts on the Nonlinear series (data generated by yt = 0.5 yt−1 − 0.2 y2 t−1 + ξt). When p = 0 (pure MA), both ARIMA and Galerkin-ARIMA rely solely on past residuals: with q = 1 they heavily smooth over the sharp nonlinear jumps, and even q = 5 only partially recovers those excursions because no direct lag-value information is used. Conversely, when q = 0 (pure AR), an AR(1) fit (p = 1) tracks the general drift but misses the innovation-driven peaks and troughs, whereas an AR(5) fit (p = 5) captures more of the short-term nonlinearity by regressing on five past values (and their squares), albeit still underestimating extreme outliers. Finally, combining both AR and MA components (p = 1, q = 5 or p = 5, q = 1) yields the closest alignment with the true curve: the AR term models the smooth feedback, while the MA term corrects for residual spikes. In all cases, Galerkin-ARIMA (red) parallels classical ARIMA (blue) in shape, with small differences when the simple polynomial residual basis cannot fully mimic ARIMA’s optimized MA coefficients. As before, once (p, q) are large enough to span the nonlinear dynamics, the two approaches produce virtually indistinguishable forecasts. 12 References George E. P. Box and Gwilym M. Jenkins. Time Series Analysis: Forecasting and Control. Holden- Day, San Francisco, 1970. Peter J. Brockwell and Richard A. Davis. Time Series: Theory and Methods. Springer, New York, 2nd edition, 1991. Carl de Boor. A Practical Guide to Splines. Springer-Verlag, New York, 1978. Mikhail Dokuchaev, Guanglu Zhou, and Song Wang. A modification of galerkin’s method for option pricing. Journal of Industrial and Management Optimization, 18(4):2483–2504, 2022. doi: 10.3934/jimo.2021077. Matthew M. Graham, Alexandre H. Thiery, and Alexandros Beskos. Manifold markov chain monte carlo methods for bayesian inference in diffusion models. Journal of the Royal Statistical Society: Series B, 85(4):1229–1256, 2023. doi: 10.1111/rssb.12517. E. J. Hannan and J. Rissanen. Recursive estimation of mixed autoregressive-moving average order. Biometrika, 69(1):81–94, 1982. M. Kushnir and K. Tokarieva. A generalization of the arima model to the nonlinear and continuous cases. Cybernetics and Systems Analysis, 59(6):900–909, 2023. doi: 10.1007/s10559-023-00625-8. Charles J. Stone. Additive regression and other nonparametric models. Annals of Statistics, 13(2): 689–705, 1985. Qiang Sun, Wei Chen, and Zhi-Qiang Li. An improved arima stock price forecasting method based on b-spline and model averaging. Academic Journal of Computing & Information Science, 5(10): 14–20, 2022. doi: 10.25236/AJCIS.2022.051003. W. M. Thupeng, R. Sivasamy, and O. A. Daman. Rainfall series forecasting models by ARIMA, NN, and HOMM methods. Advances and Applications in Statistics, 91(1):83–98, 2024. doi: 10.17654/0972361724007. Pham Hoang Vuong, Lam Hung Phu, Tran Hong Van Nguyen, Le Nhat Duy, Pham The Bao, and Tan Dat Trinh. A bibliometric literature review of stock price forecasting: From statistical model to deep learning approach. Science Progress, 107(1):1–31, 2024. doi: 10.1177/00368504241236557. Xiaoqian Wang, Yanfei Kang, Rob J. Hyndman, and Feng Li. Distributed ARIMA models for ultra-long time series. International Journal of Forecasting, 39(3):1163–1184, 2023. doi: 10.1016/ j.ijforecast.2022.05.001. 13",
    "pages": {
      "1": "Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting Haojie Liu Department of Economics University of California, Riverside hliu332@ucr.edu Zihan Lin Department of Economics University of California, Riverside zlin169@ucr.edu Abstract Time-series models like ARIMA remain widely used for forecasting but limited to linear assumptions and high computational cost in large and complex datasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA and replace it with a flexible spline-based function estimated by Galerkin projection. This enables the model to capture nonlinear dependencies in lagged values and retain the MA component and Gaussian noise assumption. We derive a closed-form OLS estimator for the Galerkin coefficients and show the model is asymptotically unbiased and consistent under standard conditions. Our method bridges classical time-series modeling and nonparametric regression, which offering improved forecasting performance and computational efficiency. 1 Introduction Time-series forecasting plays a critical role in economics, finance, energy demand, weather prediction, and more. It turns historical data into proactive insight to understand temporal patterns. The Autoregressive Integrated Moving Average (ARIMA) model remains one of the most widely used time-series models today [Vuong et al., 2024]. However, as the volume and complexity of temporal data continue to grow, ARIMA becomes increasingly slow during forecasting—the more temporal data, the longer it takes [Wang et al., 2023]. Its speed is insufficient for handling the growing complexity of modern time series. Moreover, ARIMA assumes a fixed linear relationship among past values, which can be overly restrictive when the true dynamics are nonlinear or complex [Thupeng et al., 2024, Vuong et al., 2024]. Partial differential equations (PDEs) and stochastic differential equations (SDEs) are widely used in statistics to describe continuous-time systems with uncertainty [Graham et al., 2023]. They appear in many areas, such as asset pricing in finance or diffusion-based sampling in Bayesian inference [Dokuchaev et al., 2022]. Galerkin projection is usually used to solve the problems, which is a numerical method that approximates complex functions using basis expansions [Dokuchaev et al., 2022]. We apply this idea to time series forecasting in a discrete setting. To address this, we propose Galerkin-ARIMA, which uses the Galerkin method to replace the linear autoregressive component of the ARIMA model with a spline-based function learned via Galerkin projection [Sun et al., 2022]. Specifically, we approximate the AR term using a set of basis functions over the lag space, allowing the model to flexibly capture nonlinear dependencies [Kushnir and Tokarieva, 2023]. The MA term and Gaussian noise assumptions of ARIMA are retained. We show that under regular conditions, the Galerkin-ARIMA model is asymptotically consistent and unbiased, and it can be more computationally efficient than traditional maximum likelihood 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2507.07469v1 [stat.ML] 10 Jul 2025",
      "2": "estimation methods commonly used for ARIMA models. Our approach combines the structure of ARIMA with the flexibility of nonparametric regression and offers a new path forward for modeling complex time series dynamics—achieved through a closed form solution based on Ordinary Least Squares (OLS). 2 ARIMA The Autoregressive Integrated Moving Average (ARIMA) model is a fundamental time-series model that combines autoregressive (AR) terms, integration (differencing), and moving-average (MA) terms [Box and Jenkins, 1970, Brockwell and Davis, 1991]. In an ARIMA(p, d, q) model, the series is differenced d times to achieve stationarity, and then modeled as an ARMA(p, q). Using the backshift operator B (defined by Byt = yt−1), we can write the differenced series as: y(d) t = ∆dyt = (1 −B)d yt, where y(d) t denotes the d-th order differenced series. We then define the p-dimensional state (or lag) vector xt = y(d) t−1, y(d) t−2, . . . , y(d) t−p \u0001 ∈Rp, which collects the last p observed values of the differenced series. The AR part of the model is a linear combination of these lagged values: mt = p X i=1 ψi y(d) t−i, where ψi are the autoregressive coefficients. We assume the random shocks (innovations) {ϵt} are independent and identically distributed (i.i.d.) Gaussian with mean 0 and variance σ2, i.e. ϵt ∼N(0, σ2). The full ARIMA(p, d, q) model can then be written as: y(d) t = mt + q X j=1 θj ϵt−j + ϵt, which means the differenced series y(d) t is explained by the AR term mt and a moving-average term (a linear combination of the last q noise terms) plus the current noise. This formulation is standard for ARIMA models [see, e.g., Box and Jenkins, 1970]. We will next discuss how to estimate the coefficients Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) in this model. 2.1 Maximum Likelihood Estimation (MLE) The parameters of an ARIMA model are typically estimated by maximizing the likelihood of the observed data under the model [Brockwell and Davis, 1991]. For a Gaussian ARIMA model, maximizing the likelihood is equivalent to minimizing the sum of squared one-step-ahead forecast errors. Let Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) collect all the unknown parameters. We can write the one-step prediction error at time t (for given parameters Ψ) as: ϵt(Ψ) = y(d) t − p X i=1 ψi y(d) t−i − q X j=1 θj ϵt−j(Ψ) . This ϵt(Ψ) represents the residual at time t, i.e. the difference between the observed differenced value y(d) t and its predicted value based on the model with parameters Ψ. The log-likelihood of the data (up to an additive constant) can be written as a function of Ψ: L(Ψ) = −N 2 ln(σ2) − 1 2σ2 N X t=1 ϵt(Ψ)2 , where N is the number of observations in the time series after differencing. In this expression, we have assumed Gaussian innovations so that ϵt(Ψ) are normally distributed. Maximizing L(Ψ) is equivalent to minimizing the sum of squared residuals P t ϵt(Ψ)2. The maximum likelihood estimator (MLE) ˆΨ is defined as: ˆΨ = arg max Ψ L(Ψ) . 2",
      "3": "In practice, this optimization does not have a closed-form solution for ARMA models and must be carried out by iterative numerical methods (e.g., Newton-Raphson or gradient descent) [Brockwell and Davis, 1991]. The above formulation highlights that the MLE seeks the parameter values that best explain the data by producing the smallest prediction errors. In summary, classical ARIMA modeling involves finding linear coefficients (ψi and θj) that minimize the prediction error. In the next section, we introduce a Galerkin approximation approach, which generalizes the AR component to a flexible function (using basis splines) rather than a fixed linear form. This will lead to the Galerkin-ARIMA method, where we aim to improve forecasting by capturing possibly nonlinear relationships in the lagged values. 3 Galerkin Approximation Before introducing the Galerkin-ARIMA model, we briefly review the Galerkin method in the context of stochastic differential equations (SDEs). Consider a one–dimensional SDE: dXt = b(Xt) dt + σ(Xt) dWt , where b(x) is the drift function and σ(x) is the diffusion coefficient. Suppose the drift function b(x) is expensive to compute or is unknown; we wish to approximate b(x) using a finite–dimensional function space spanned by a set of basis functions. Let {ϕj(x)}K j=1 be a family of basis functions (for example, B-splines of degree p on the domain Ω). We postulate an approximating drift of the form: bK(x) = K X j=1 θj ϕj(x) , with unknown coefficients θ = (θ1, . . . , θK) to be determined by a Galerkin projection. The Galerkin method requires that the residual (error) between the true drift and the approximated drift is orthogonal to the space spanned by the basis. Define the residual function r(x) = b(x) −bK(x). We introduce an inner product on the space of functions (weighted by a probability measure µ(x) on Ω): ⟨f, g⟩= Z Ω f(x) g(x) µ(x) dx . The Galerkin orthogonality condition imposes that the residual is orthogonal to each basis function ϕi: ⟨r, ϕi⟩= 0, i = 1, . . . , K. Substituting r(x) = b(x) −PK j=1 θjϕj(x), this condition becomes: Z Ω \u0010 b(x) − K X j=1 θj ϕj(x) \u0011 ϕi(x) µ(x) dx = 0, for each i = 1, . . . , K. Define the Gram matrix G ∈RK×K and the vector f ∈RK by: Gij = Z Ω ϕi(x) ϕj(x) µ(x) dx, fi = Z Ω b(x) ϕi(x) µ(x) dx. The Galerkin conditions then yield a linear system G θ = f. Once solved for θ, we obtain the Galerkin approximation bK(x) = PK j=1 θjϕj(x). Substituting this back into the SDE gives: dXt = bK(Xt) dt + σ(Xt) dWt , which is a spline-based approximation of the original dynamics. 3.1 AR-term Approximation via Galerkin Method We now return to time-series modeling. The key idea of Galerkin-ARIMA is to replace the fixed linear AR structure with a more flexible function approximated by basis functions, while still retaining the MA component. Essentially, we treat the p-dimensional lag vector as an input to an unknown 3",
      "4": "function f(·) (analogous to a drift function in an SDE), and we approximate this function using a finite basis expansion. Let Φ(xt) = [ϕ1(xt), ϕ2(xt), . . . , ϕK(xt)]T ∈RK be a vector of K basis function evaluations at the state xt = (y(d) t−1, . . . , y(d) t−p). For example, each ϕj could be a multi-variate B-spline or some polynomial basis defined on the lag space. We then posit that the AR component mt can be represented as: mt = Φ(xt)T β , β ∈RK, where β = (β1, . . . , βK)T are the coefficients of the basis expansion. In other words, mt is the Galerkin approximation of the potentially nonlinear relationship between past values and the next differenced value y(d) t . The Galerkin-ARIMA model then becomes: y(d) t = mt + q X j=1 θj ϵt−j + ϵt , similar in form to the classical ARIMA, but with mt now given by a flexible spline-based function rather than a fixed linear combination of lags. Estimating the coefficients in this Galerkin framework can be done in closed form using least squares, thanks to the linearity in the parameters β. First, we fit the basis-function coefficients β to approximate the AR part. This can be done by regressing y(d) t onto Φ(xt). In matrix terms, let Y = [y(d) 1 , y(d) 2 , . . . , y(d) N ]T be the vector of responses (differenced series), and let Φ be the N × K matrix whose t-th row is Φ(xt)T . Then the ordinary least squares (OLS) solution for β is: ˆβ = (ΦT Φ)−1 ΦT Y , provided N > K and the basis functions are linearly independent. This ˆβ minimizes the sum of squared errors P t(y(d) t −Φ(xt)T β)2. Using ˆβ, we can compute the fitted residuals (initially ignoring the MA terms) as: ˆϵ(0) t = y(d) t −Φ(xt)T ˆβ , for each t. These residuals ˆϵ(0) t represent the remaining structure after accounting for the spline-based AR approximation. Next, to estimate the moving-average coefficients θ1, . . . , θq, we include the lagged residuals in a sec- ond regression. For each t, form the vector of the last q residuals ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−q]T . Now define the combined regression design matrix Ψ of dimension N × (K + q) whose t-th row is [Φ(xt)T | ˆϵ(0) T t−1:t−q]. In other words, Ψt = [ϕ1(xt), . . . , ϕK(xt) | ˆϵ(0) t−1, . . . , ˆϵ(0) t−q] includes both the basis functions and the lagged residuals. Let γ = (βT , θ1, . . . , θq)T be the combined coefficient vector of length K + q. Then an OLS estimate for γ is given by: ˆγ = (ΨT Ψ)−1 ΨT Y . This yields refined estimates ˆβ (the first K entries of ˆγ) and ˆθ1, . . . , ˆθq (the last q entries of ˆγ) simultaneously. Once we have these estimates, we can form the final fitted model: ˆmt = Φ(xt)T ˆβ , ˆy(d) t+1 = ˆmt+1 + q X j=1 ˆθj ˆϵt+1−j , which provides one-step-ahead forecasts for the differenced series. (Here ˆϵt+1−j are the in-sample residuals at time t + 1 −j.) In essence, ˆmt is a nonparametric (spline-based) estimate of the AR part, and the ˆθj adjust for the remaining serial correlation (the MA part). Under standard conditions for nonparametric regression (in particular, as K →∞and K/N →0 as N →∞), the spline approximation mt = Φ(xt)T β can approximate any sufficiently smooth function of the lags. This suggests that as we increase the number of basis functions K, the Galerkin-ARIMA model can in principle capture the true underlying relationship among the past values arbitrarily well. 4",
      "5": "4 Error Analysis One major purpose of proposing the Galerkin-ARIMA algorithm is to achieve better accuracy or efficiency than classical ARIMA. We therefore analyze the bias, variance, and computational complexity of the new model, comparing it to the standard ARIMA approach. 4.1 Bias First, consider the data-generating process to be a linear ARMA(p, q) process: yt = p X i=1 ψ∗ i yt−i + q X j=1 θ∗ j ϵt−j + ϵt, ϵt iid ∼N(0, σ2) , and suppose we fit the model with the correct orders p and q. Based on standard time-series theory [see Hannan and Rissanen, 1982, Brockwell and Davis, 1991], under regular conditions the estimators of the true parameters are consistent. In particular, for one-step-ahead prediction we have ˆΨ →Ψ∗ (in probability) as N →∞, where Ψ∗= (ψ∗ 1, . . . , ψ∗ p, θ∗ 1, . . . , θ∗ q, σ2) are the true parameters. Consequently, the 1-step forecast from a correctly specified ARIMA model converges in probability to the true value: ˆyt+1|t = p X i=1 ˆψi,N yt+1−i + q X j=1 ˆθj,N ˆϵt+1−j −→ p yt+1 , where ˆψi,N and ˆθj,N are the estimates based on N observations. In particular, the asymptotic bias of the one-step forecast is zero: p−limN→∞ \u0010 yt+1 −ˆyt+1|t \u0011 = 0 . This confirms that a well-specified ARIMA model is asymptotically unbiased under the usual assumptions. Now consider the Galerkin-ARIMA model. Let the lag-response relationship be some smooth but unknown function f : Rp →R such that y(d) t = f(xt)+ (MA terms) + noise. We approximate f(x) by the spline expansion mt = Φ(xt)T β using K basis functions. Under standard smoothness conditions (e.g. f belongs to a Hölder class with smoothness index r) and for x in a compact domain, the Jackson-Bernstein inequality from approximation theory guarantees that the approximation error can be made arbitrarily small as K grows. In particular, one result (see, e.g., de Boor, 1978) states that: inf β∈RK sup x∈C f(x) −Φ(x)T β = O(K−r/p) , where r characterizes the smoothness of f. This means the bias due to using a finite basis of size K decreases as K increases (assuming f is sufficiently smooth). Thus, if we let K →∞as N →∞ (but slowly enough that K/N →0; see below), the bias of ˆmt as an estimator of the true f(xt) will tend to zero. In summary, the Galerkin-ARIMA model can also be made asymptotically unbiased: as the number of basis functions grows, it can recover the true AR relationship without systematic error. 4.2 Variance and Consistency For the classical ARIMA MLE, the parameter estimates are asymptotically normal. In fact, if ˆΨ is the MLE of Ψ∗, then under regular conditions: √ N (ˆΨ −Ψ∗) d−→N 0, I(Ψ∗)−1\u0001 , where I(Ψ∗) is the Fisher information matrix evaluated at the true parameters [Brockwell and Davis, 1991]. This implies Var(ˆΨ) = O(1/N) for each fixed number of parameters (since the information matrix converges to a constant). In the Galerkin-ARIMA approach, the number of parameters γ = (β1, . . . , βK, θ1, . . . , θq) grows with K. The OLS variance-covariance for ˆγ is: Var(ˆγ) = σ2 (ΨT Ψ)−1 ≈σ2 1 N \u0010 p−limN→∞ ΨT Ψ N \u0011−1 . 5",
      "6": "Roughly speaking, each coefficient in γ has variance on the order of 1/N, but there are (K + q) coefficients. If K grows with N, this introduces a trade-off between bias and variance. In fact, Var(ˆγ) will scale on the order of (K + q)/N. To keep the variance of each coefficient bounded, we require K = o(N) (so that (K + q)/N →0). Typically, in nonparametric regression one balances bias and variance by letting K grow at a suitable rate relative to N. A common choice (optimal in a minimax sense for r-smooth functions in Rp) is K ∼N p 2r+p [Stone, 1985]. With such a choice, one can show that if f has Hölder smoothness r and we let K →∞with K/N →0, then ∥ˆf −f∥2 2 = O \u0010 K−2r/p + K N \u0011 , which is the standard convergence rate for nonparametric regression [Stone, 1985]. By plugging in K ∼N p/(2r+p), the two terms in the error bound become of the same order, yielding ∥ˆf −f∥2 2 = O(N −2r/(2r+p)), which approaches 0 as N →∞. This result implies that, with an appropriate growth of K and under smoothness assumptions, the Galerkin-ARIMA estimator ˆf is consistent for the true function f(x). 4.3 Computational Complexity We briefly compare the computational complexity of fitting a standard ARIMA model versus the Galerkin-ARIMA model. In a linear ARIMA model, evaluating the objective (likelihood or sum of squared errors) for a given parameter guess and computing gradients typically requires O((p + q)N) operations per iteration (because each residual ϵt depends on p + q past terms). Let I be the number of iterations needed for convergence of the numerical optimizer. Then the overall complexity of computing the MLE is on the order of O(I · (p + q) N). In practice, I (for something like Newton’s method) might be larger than p + q, and each iteration can be costly for long series. In the Galerkin-ARIMA approach, the main cost comes from two least-squares fits. Constructing the basis function matrix Φ costs O(p N · K) if each of the K basis functions takes O(p) to evaluate for a given xt (for example, polynomial splines might be evaluated in constant time per basis). Solving the OLS for β by direct methods is O(NK2) (since computing (ΦT Φ) is O(NK2) and inverting the K × K matrix is O(K3), though if K is not too large this is manageable). Solving the second OLS for γ is O(N(K + q)2) by a similar reasoning. Combining these, the total complexity for Galerkin-ARIMA is approximately OGAL ≈O(p N K) + O(N(K + q)2) . For reasonably chosen K, this can be significantly smaller than the ARIMA MLE cost. For instance, if I is on the order of N 1/2 (a rough scenario for some difficult likelihood surfaces) or if p and q are moderately large, the direct OLS approach can be advantageous. The ratio of complexities is roughly: OARIMA OGAL ≈I (p + q) N N (K + q)2 = I (p + q) (K + q)2 . In many situations, we have I > p ≈q and we might choose K much smaller than N. As a concrete example, for one-step forecasting (M = 1), if I ∼100, p+q ∼10, and K ∼10, then the above ratio suggests ARIMA MLE could be around 100·10 (10+10)2 = 1000 400 = 2.5 times slower than Galerkin-ARIMA per forecast. For rolling forecasts over a window of M time points, the computational savings could be multiplied further by a factor on the order of M. Of course, the exact speed gain depends on implementation details and the cost of basis function evaluations, but this analysis indicates that Galerkin-ARIMA has the potential to be computationally more efficient, especially for long series or when a moderate number of basis functions suffices. 5 Numerical Experiments 5.1 Synthetic Data Generation All series have length n = 300 and are generated with a fixed random seed for reproducibility. We consider four toy processes: 1. Noisy ARMA(2,1): yt = 0.6 yt−1 −0.3 yt−2 + 0.5 ϵt−1 + ϵt, ϵt ∼N(0, 1), y1 = y2 = 0. 6",
      "7": "2. Seasonal sine + noise: yt = sin \u0010 2π t 20 \u0011 + 0.5 ηt, ηt ∼N(0, 1). 3. Linear trend + AR(1): yt = 0.01 t + 0.8 yt−1 + νt, νt ∼N(0, 0.52), y0 = 0. 4. Nonlinear recursion + noise: yt = 0.5 yt−1 −0.2 y2 t−1 + ξt, ξt ∼N(0, 0.72), y0 = 0. These four processes span a wide variety of dynamics—linear autoregression with a moving-average component, periodic fluctuations, deterministic trend combined with stochastic feedback, and a purely nonlinear recursion. By testing on data that exhibit both classical ARMA behavior and more challenging seasonal, trending, and nonlinear patterns, we can rigorously evaluate whether Galerkin-ARIMA’s flexible basis expansion captures complex dependencies that a standard ARIMA may miss, while still recovering simple linear structure when appropriate. 5.2 Forecasting Setup We perform rolling one-step-ahead forecasts on each series using: • a fixed training window of length W = 100, • a forecast horizon of H = 150, and • Monte Carlo replications R = 10 (to average out noise). At each time i = W, . . . , W + H −1, we fit two models with the same order pair (p, q): Both ARIMA(p, 0, q) and Galerkin-ARIMA(p, q) are evaluated at the same set of order pairs (p, q) ∈{(1, 0), (5, 0), (0, 1), (0, 5), (1, 5), (5, 1)}. The ARIMA(p, 0, q) model is fit by maximizing the Gaussian likelihood via the statsmodels.tsa.arima.ARIMA routine, where p denotes the number of autoregressive lags and q the number of moving-average lags. The Galerkin-ARIMA(p, q) model employs a two-stage Galerkin approximation: in the first (AR) stage, we regress yt on the polynomial basis {1, yt−1, . . . , yt−p, y2 t−1, . . . , y2 t−p} to obtain coefficients ˆβ and residuals ˆϵ(0) t , and in the second (MA) stage, we regress these residuals on the basis {1, ˆϵ(0) t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ(0) t−q)2} to obtain coefficients ˆα for the moving-average component. For each model and each (p, q) combina- tion, we record the mean absolute error (MAE), root-mean-squared error (RMSE), total CPU time, and average CPU time per fit over the H rolling forecasts, then average these metrics over the R Monte Carlo runs. 5.3 Result Table 1 collects the MAE, RMSE, total and per-fit CPU times for ARIMA and Galerkin-ARIMA across all four synthetic datasets and six choices of (p, q). A few key patterns emerge. First, when (p, q) fail to cover the true dynamics (e.g. pure MA or pure AR fits), both methods exhibit elevated MAE/RMSE, but Galerkin-ARIMA sometimes smooths slightly more aggressively, yielding marginally higher errors. Second, as soon as (p, q) match or exceed the underlying orders (for example (1, 5) or (5, 1) on the Noisy_ARMA data, or (5, 0) on the Trend_AR data), the accuracy of Galerkin-ARIMA converges to that of classical ARIMA—the two lines in the corresponding panels become virtually indistinguishable. Third, across every dataset and order, Galerkin-ARIMA runs orders of magnitude faster: total CPU time drops from seconds to hundredths of a second, and average per-fit time falls from ∼10−2s to ∼10−4s. In short, Galerkin-ARIMA attains almost identical forecasting performance to ARIMA as soon as the basis dimension is sufficient, while dramatically reducing computation time. 7",
      "8": "Dataset p q Algorithm MAE RMSE Total Time Avg Time Noisy_ARMA 0 1 ARIMA 0.8636 1.0457 1.7318 0.0115 Galerkin-ARIMA 0.9521 1.1846 0.0743 0.0005 0 5 ARIMA 0.8324 1.0045 4.1475 0.0276 Galerkin-ARIMA 0.8331 1.0148 0.0937 0.0006 1 0 ARIMA 0.9494 1.1816 1.2780 0.0085 Galerkin-ARIMA 0.9521 1.1846 0.0405 0.0003 1 5 ARIMA 0.8382 1.0110 11.1948 0.0746 Galerkin-ARIMA 0.8402 1.0249 0.1021 0.0007 5 0 ARIMA 0.8320 1.0145 3.8751 0.0258 Galerkin-ARIMA 0.8331 1.0148 0.0605 0.0004 5 1 ARIMA 0.8413 1.0272 6.4902 0.0433 Galerkin-ARIMA 0.8298 1.0133 0.0994 0.0007 Nonlinear 0 1 ARIMA 0.5441 0.6742 1.2896 0.0086 Galerkin-ARIMA 0.5180 0.6546 0.0741 0.0005 0 5 ARIMA 0.5312 0.6614 3.4696 0.0231 Galerkin-ARIMA 0.5405 0.6790 0.0945 0.0006 1 0 ARIMA 0.5296 0.6595 1.1266 0.0075 Galerkin-ARIMA 0.5180 0.6546 0.0409 0.0003 1 5 ARIMA 0.5373 0.6665 9.2855 0.0619 Galerkin-ARIMA 0.5343 0.6732 0.1013 0.0007 5 0 ARIMA 0.5321 0.6642 2.9177 0.0195 Galerkin-ARIMA 0.5405 0.6790 0.0608 0.0004 5 1 ARIMA 0.5305 0.6636 7.0720 0.0471 Galerkin-ARIMA 0.5400 0.6809 0.0998 0.0007 Seasonal 0 1 ARIMA 0.5924 0.7261 1.3999 0.0093 Galerkin-ARIMA 0.5305 0.6784 0.0740 0.0005 0 5 ARIMA 0.5280 0.6734 4.2973 0.0286 Galerkin-ARIMA 0.5331 0.6721 0.0940 0.0006 1 0 ARIMA 0.5338 0.6818 1.3825 0.0092 Galerkin-ARIMA 0.5305 0.6784 0.0404 0.0003 1 5 ARIMA 0.5401 0.6745 11.8796 0.0792 Galerkin-ARIMA 0.5349 0.6693 0.1018 0.0007 5 0 ARIMA 0.5291 0.6642 3.8499 0.0257 Galerkin-ARIMA 0.5331 0.6721 0.0600 0.0004 5 1 ARIMA 0.5017 0.6219 12.4039 0.0827 Galerkin-ARIMA 0.5331 0.6756 0.1015 0.0007 Trend_AR 0 1 ARIMA 2.4277 2.6177 1.7387 0.0116 Galerkin-ARIMA 0.3941 0.5017 0.0741 0.0005 0 5 ARIMA 0.8587 0.9952 8.4002 0.0560 Galerkin-ARIMA 0.4165 0.5186 0.0950 0.0006 1 0 ARIMA 0.3879 0.4989 2.1608 0.0144 Galerkin-ARIMA 0.3941 0.5017 0.0407 0.0003 1 5 ARIMA 0.3932 0.4977 11.6516 0.0777 Galerkin-ARIMA 0.4139 0.5252 0.1032 0.0007 5 0 ARIMA 0.3912 0.5000 7.1304 0.0475 Galerkin-ARIMA 0.4165 0.5186 0.0600 0.0004 5 1 ARIMA 0.3917 0.5054 16.7612 0.1117 Galerkin-ARIMA 0.4163 0.5214 0.1010 0.0007 Table 1: Averaged over 10 runs: MAE, RMSE, total and average CPU time for ARIMA vs. Galerkin- ARIMA. 8",
      "9": "(a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 1: One-step forecasts on the Noisy_ARMA series for various (p, q). All six panels in Figure 1 draw from the same underlying ARMA(2,1) process, and together they vividly illustrate how the autoregressive order p and moving-average order q shape one-step forecasts. When p = 0 (pure MA fits), both ARIMA and Galerkin-ARIMA must rely entirely on past innova- tions: with only q = 1 they flatten out the rapid AR(2) swings and lag behind the true curve, while increasing to q = 5 gives them enough “memory” of past shocks that they begin to partially mimic the missing AR feedback. Conversely, when q = 0 (pure AR fits), a single lag (p = 1) captures general level shifts but not the innovation noise, whereas bumping to p = 5 lets each method recover the AR(2) oscillations almost perfectly—even without any MA component—since the higher-order AR basis soaks up residual randomness. Once both p and q are positive (for example, (p, q) = (1, 5) or (5, 1)), each model’s specification fully spans the true two-lag AR and one-lag MA dynamics. In these richer settings, ARIMA’s maximum-likelihood estimates and our two-stage Galerkin projections converge on virtually identical forecasts, tracing the black “True” series with only vanishing discrepancies. In short, raising either p or q in isolation compensates partially for the missing term, but only the combination of sufficient AR and MA orders unlocks near-perfect recovery of the data-generator. 9",
      "10": "(a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 2: One-step forecasts on the Seasonal series for various (p, q). Figure 2 displays one-step forecasts on the seasonal sine series yt = sin(2πt/20) + 0.5 ηt. When p = 0 (pure MA), an MA(1) fit (q = 1) overly smooths the oscillations and fails to track the peaks of each cycle, while increasing to q = 5 spreads past shocks across a full period and yields a visibly improved—but still lagged—sinusoidal trace. When q = 0 (pure AR), AR(1) (p = 1) captures the local level but cannot reproduce the periodic up–down swings, whereas AR(5) (p = 5) begins to recover the 20-step seasonality by regressing on multiple lags, albeit with slight phase shifts. Finally, combining both AR and MA terms (p = 1, q = 5 or p = 5, q = 1) gives the best result: the AR component provides the backbone of the sine wave, and the MA component corrects residual phase and amplitude errors. Throughout, Galerkin-ARIMA (red) closely parallels ARIMA (blue), with minor additional smoothing at low orders due to the polynomial residual basis, but converges to essentially identical forecasts once (p, q) span the true seasonal dynamics. 10",
      "11": "(a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 3: One-step forecasts on the Trend_AR series for various (p, q). Figure 3 displays one-step forecasts on the Trend_AR series yt = 0.01 t+0.8 yt−1 +νt with various (p, q). When p = 0 (pure MA), an MA(1) fit (q = 1) dramatically underestimates the upward drift and remains far below the true curve, while increasing to q = 5 improves the level somewhat but still fails to capture the deterministic trend. When q = 0 (pure AR), an AR(1) fit (p = 1) follows the slope closely—since it directly models the feedback—but the residual noise causes small phase-offsets; raising to AR(5) (p = 5) further refines the fit by absorbing additional past values, producing an almost exact match to the trend. Finally, combining AR and MA (p = 1, q = 5 or p = 5, q = 1) yields similarly excellent forecasts: the AR term captures the linear drift, and the MA term smooths remaining volatility. Throughout, Galerkin-ARIMA (red) mirrors classical ARIMA (blue) with negligible differences once the basis dimension is sufficient, but it falls back to a lower level under purely MA specifications because no trend regressor is included in the residual basis. 11",
      "12": "(a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 4: One-step forecasts on the Nonlinear series for various (p, q). Figure 4 shows one-step forecasts on the Nonlinear series (data generated by yt = 0.5 yt−1 − 0.2 y2 t−1 + ξt). When p = 0 (pure MA), both ARIMA and Galerkin-ARIMA rely solely on past residuals: with q = 1 they heavily smooth over the sharp nonlinear jumps, and even q = 5 only partially recovers those excursions because no direct lag-value information is used. Conversely, when q = 0 (pure AR), an AR(1) fit (p = 1) tracks the general drift but misses the innovation-driven peaks and troughs, whereas an AR(5) fit (p = 5) captures more of the short-term nonlinearity by regressing on five past values (and their squares), albeit still underestimating extreme outliers. Finally, combining both AR and MA components (p = 1, q = 5 or p = 5, q = 1) yields the closest alignment with the true curve: the AR term models the smooth feedback, while the MA term corrects for residual spikes. In all cases, Galerkin-ARIMA (red) parallels classical ARIMA (blue) in shape, with small differences when the simple polynomial residual basis cannot fully mimic ARIMA’s optimized MA coefficients. As before, once (p, q) are large enough to span the nonlinear dynamics, the two approaches produce virtually indistinguishable forecasts. 12",
      "13": "References George E. P. Box and Gwilym M. Jenkins. Time Series Analysis: Forecasting and Control. Holden- Day, San Francisco, 1970. Peter J. Brockwell and Richard A. Davis. Time Series: Theory and Methods. Springer, New York, 2nd edition, 1991. Carl de Boor. A Practical Guide to Splines. Springer-Verlag, New York, 1978. Mikhail Dokuchaev, Guanglu Zhou, and Song Wang. A modification of galerkin’s method for option pricing. Journal of Industrial and Management Optimization, 18(4):2483–2504, 2022. doi: 10.3934/jimo.2021077. Matthew M. Graham, Alexandre H. Thiery, and Alexandros Beskos. Manifold markov chain monte carlo methods for bayesian inference in diffusion models. Journal of the Royal Statistical Society: Series B, 85(4):1229–1256, 2023. doi: 10.1111/rssb.12517. E. J. Hannan and J. Rissanen. Recursive estimation of mixed autoregressive-moving average order. Biometrika, 69(1):81–94, 1982. M. Kushnir and K. Tokarieva. A generalization of the arima model to the nonlinear and continuous cases. Cybernetics and Systems Analysis, 59(6):900–909, 2023. doi: 10.1007/s10559-023-00625-8. Charles J. Stone. Additive regression and other nonparametric models. Annals of Statistics, 13(2): 689–705, 1985. Qiang Sun, Wei Chen, and Zhi-Qiang Li. An improved arima stock price forecasting method based on b-spline and model averaging. Academic Journal of Computing & Information Science, 5(10): 14–20, 2022. doi: 10.25236/AJCIS.2022.051003. W. M. Thupeng, R. Sivasamy, and O. A. Daman. Rainfall series forecasting models by ARIMA, NN, and HOMM methods. Advances and Applications in Statistics, 91(1):83–98, 2024. doi: 10.17654/0972361724007. Pham Hoang Vuong, Lam Hung Phu, Tran Hong Van Nguyen, Le Nhat Duy, Pham The Bao, and Tan Dat Trinh. A bibliometric literature review of stock price forecasting: From statistical model to deep learning approach. Science Progress, 107(1):1–31, 2024. doi: 10.1177/00368504241236557. Xiaoqian Wang, Yanfei Kang, Rob J. Hyndman, and Feng Li. Distributed ARIMA models for ultra-long time series. International Journal of Forecasting, 39(3):1163–1184, 2023. doi: 10.1016/ j.ijforecast.2022.05.001. 13"
    },
    "statistics": {
      "page_count": 13,
      "word_count": 5544,
      "character_count": 32235,
      "avg_words_per_page": 426.46153846153845
    }
  },
  "sections": {
    "sections": {
      "body": "Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting Haojie Liu Department of Economics University of California, Riverside hliu332@ucr.edu Zihan Lin Department of Economics University of California, Riverside zlin169@ucr.edu Abstract Time-series models like ARIMA remain widely used for forecasting but limited to linear assumptions and high computational cost in large and complex datasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA and replace it with a flexible spline-based function estimated by Galerkin projection. This enables the model to capture nonlinear dependencies in lagged values and retain the MA component and Gaussian noise assumption. We derive a closed-form OLS estimator for the Galerkin coefficients and show the model is asymptotically unbiased and consistent under standard conditions. Our method bridges classical time-series modeling and nonparametric regression, which offering improved forecasting performance and computational efficiency. 1 Introduction Time-series forecasting plays a critical role in economics, finance, energy demand, weather prediction, and more. It turns historical data into proactive insight to understand temporal patterns. The Autoregressive Integrated Moving Average (ARIMA) model remains one of the most widely used time-series models today [Vuong et al., 2024]. However, as the volume and complexity of temporal data continue to grow, ARIMA becomes increasingly slow during forecasting—the more temporal data, the longer it takes [Wang et al., 2023]. Its speed is insufficient for handling the growing complexity of modern time series. Moreover, ARIMA assumes a fixed linear relationship among past values, which can be overly restrictive when the true dynamics are nonlinear or complex [Thupeng et al., 2024, Vuong et al., 2024]. Partial differential equations (PDEs) and stochastic differential equations (SDEs) are widely used in statistics to describe continuous-time systems with uncertainty [Graham et al., 2023]. They appear in many areas, such as asset pricing in finance or diffusion-based sampling in Bayesian inference [Dokuchaev et al., 2022]. Galerkin projection is usually used to solve the problems, which is a numerical method that approximates complex functions using basis expansions [Dokuchaev et al., 2022]. We apply this idea to time series forecasting in a discrete setting. To address this, we propose Galerkin-ARIMA, which uses the Galerkin method to replace the linear autoregressive component of the ARIMA model with a spline-based function learned via Galerkin projection [Sun et al., 2022]. Specifically, we approximate the AR term using a set of basis functions over the lag space, allowing the model to flexibly capture nonlinear dependencies [Kushnir and Tokarieva, 2023]. The MA term and Gaussian noise assumptions of ARIMA are retained. We show that under regular conditions, the Galerkin-ARIMA model is asymptotically consistent and unbiased, and it can be more computationally efficient than traditional maximum likelihood 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2507.07469v1 [stat.ML] 10 Jul 2025 estimation methods commonly used for ARIMA models. Our approach combines the structure of ARIMA with the flexibility of nonparametric regression and offers a new path forward for modeling complex time series dynamics—achieved through a closed form solution based on Ordinary Least Squares (OLS). 2 ARIMA The Autoregressive Integrated Moving Average (ARIMA) model is a fundamental time-series model that combines autoregressive (AR) terms, integration (differencing), and moving-average (MA) terms [Box and Jenkins, 1970, Brockwell and Davis, 1991]. In an ARIMA(p, d, q) model, the series is differenced d times to achieve stationarity, and then modeled as an ARMA(p, q). Using the backshift operator B (defined by Byt = yt−1), we can write the differenced series as: y(d) t = ∆dyt = (1 −B)d yt, where y(d) t denotes the d-th order differenced series. We then define the p-dimensional state (or lag) vector xt = y(d) t−1, y(d) t−2, . . . , y(d) t−p \u0001 ∈Rp, which collects the last p observed values of the differenced series. The AR part of the model is a linear combination of these lagged values: mt = p X i=1 ψi y(d) t−i, where ψi are the autoregressive coefficients. We assume the random shocks (innovations) {ϵt} are independent and identically distributed (i.i.d.) Gaussian with mean 0 and variance σ2, i.e. ϵt ∼N(0, σ2). The full ARIMA(p, d, q) model can then be written as: y(d) t = mt + q X j=1 θj ϵt−j + ϵt, which means the differenced series y(d) t is explained by the AR term mt and a moving-average term (a linear combination of the last q noise terms) plus the current noise. This formulation is standard for ARIMA models [see, e.g., Box and Jenkins, 1970]. We will next discuss how to estimate the coefficients Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) in this model. 2.1 Maximum Likelihood Estimation (MLE) The parameters of an ARIMA model are typically estimated by maximizing the likelihood of the observed data under the model [Brockwell and Davis, 1991]. For a Gaussian ARIMA model, maximizing the likelihood is equivalent to minimizing the sum of squared one-step-ahead forecast errors. Let Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) collect all the unknown parameters. We can write the one-step prediction error at time t (for given parameters Ψ) as: ϵt(Ψ) = y(d) t − p X i=1 ψi y(d) t−i − q X j=1 θj ϵt−j(Ψ) . This ϵt(Ψ) represents the residual at time t, i.e. the difference between the observed differenced value y(d) t and its predicted value based on the model with parameters Ψ. The log-likelihood of the data (up to an additive constant) can be written as a function of Ψ: L(Ψ) = −N 2 ln(σ2) − 1 2σ2 N X t=1 ϵt(Ψ)2 , where N is the number of observations in the time series after differencing. In this expression, we have assumed Gaussian innovations so that ϵt(Ψ) are normally distributed. Maximizing L(Ψ) is equivalent to minimizing the sum of squared residuals P t ϵt(Ψ)2. The maximum likelihood estimator (MLE) ˆΨ is defined as: ˆΨ = arg max Ψ L(Ψ) . 2 In practice, this optimization does not have a closed-form solution for ARMA models and must be carried out by iterative numerical methods (e.g., Newton-Raphson or gradient descent) [Brockwell and Davis, 1991]. The above formulation highlights that the MLE seeks the parameter values that best explain the data by producing the smallest prediction errors. In summary, classical ARIMA modeling involves finding linear coefficients (ψi and θj) that minimize the prediction error. In the next section, we introduce a Galerkin approximation approach, which generalizes the AR component to a flexible function (using basis splines) rather than a fixed linear form. This will lead to the Galerkin-ARIMA method, where we aim to improve forecasting by capturing possibly nonlinear relationships in the lagged values. 3 Galerkin Approximation Before introducing the Galerkin-ARIMA model, we briefly review the Galerkin method in the context of stochastic differential equations (SDEs). Consider a one–dimensional SDE: dXt = b(Xt) dt + σ(Xt) dWt , where b(x) is the drift function and σ(x) is the diffusion coefficient. Suppose the drift function b(x) is expensive to compute or is unknown; we wish to approximate b(x) using a finite–dimensional function space spanned by a set of basis functions. Let {ϕj(x)}K j=1 be a family of basis functions (for example, B-splines of degree p on the domain Ω). We postulate an approximating drift of the form: bK(x) = K X j=1 θj ϕj(x) , with unknown coefficients θ = (θ1, . . . , θK) to be determined by a Galerkin projection. The Galerkin method requires that the residual (error) between the true drift and the approximated drift is orthogonal to the space spanned by the basis. Define the residual function r(x) = b(x) −bK(x). We introduce an inner product on the space of functions (weighted by a probability measure µ(x) on Ω): ⟨f, g⟩= Z Ω f(x) g(x) µ(x) dx . The Galerkin orthogonality condition imposes that the residual is orthogonal to each basis function ϕi: ⟨r, ϕi⟩= 0, i = 1, . . . , K. Substituting r(x) = b(x) −PK j=1 θjϕj(x), this condition becomes: Z Ω \u0010 b(x) − K X j=1 θj ϕj(x) \u0011 ϕi(x) µ(x) dx = 0, for each i = 1, . . . , K. Define the Gram matrix G ∈RK×K and the vector f ∈RK by: Gij = Z Ω ϕi(x) ϕj(x) µ(x) dx, fi = Z Ω b(x) ϕi(x) µ(x) dx. The Galerkin conditions then yield a linear system G θ = f. Once solved for θ, we obtain the Galerkin approximation bK(x) = PK j=1 θjϕj(x). Substituting this back into the SDE gives: dXt = bK(Xt) dt + σ(Xt) dWt , which is a spline-based approximation of the original dynamics. 3.1 AR-term Approximation via Galerkin Method We now return to time-series modeling. The key idea of Galerkin-ARIMA is to replace the fixed linear AR structure with a more flexible function approximated by basis functions, while still retaining the MA component. Essentially, we treat the p-dimensional lag vector as an input to an unknown 3 function f(·) (analogous to a drift function in an SDE), and we approximate this function using a finite basis expansion. Let Φ(xt) = [ϕ1(xt), ϕ2(xt), . . . , ϕK(xt)]T ∈RK be a vector of K basis function evaluations at the state xt = (y(d) t−1, . . . , y(d) t−p). For example, each ϕj could be a multi-variate B-spline or some polynomial basis defined on the lag space. We then posit that the AR component mt can be represented as: mt = Φ(xt)T β , β ∈RK, where β = (β1, . . . , βK)T are the coefficients of the basis expansion. In other words, mt is the Galerkin approximation of the potentially nonlinear relationship between past values and the next differenced value y(d) t . The Galerkin-ARIMA model then becomes: y(d) t = mt + q X j=1 θj ϵt−j + ϵt , similar in form to the classical ARIMA, but with mt now given by a flexible spline-based function rather than a fixed linear combination of lags. Estimating the coefficients in this Galerkin framework can be done in closed form using least squares, thanks to the linearity in the parameters β. First, we fit the basis-function coefficients β to approximate the AR part. This can be done by regressing y(d) t onto Φ(xt). In matrix terms, let Y = [y(d) 1 , y(d) 2 , . . . , y(d) N ]T be the vector of responses (differenced series), and let Φ be the N × K matrix whose t-th row is Φ(xt)T . Then the ordinary least squares (OLS) solution for β is: ˆβ = (ΦT Φ)−1 ΦT Y , provided N > K and the basis functions are linearly independent. This ˆβ minimizes the sum of squared errors P t(y(d) t −Φ(xt)T β)2. Using ˆβ, we can compute the fitted residuals (initially ignoring the MA terms) as: ˆϵ(0) t = y(d) t −Φ(xt)T ˆβ , for each t. These residuals ˆϵ(0) t represent the remaining structure after accounting for the spline-based AR approximation. Next, to estimate the moving-average coefficients θ1, . . . , θq, we include the lagged residuals in a sec- ond regression. For each t, form the vector of the last q residuals ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−q]T . Now define the combined regression design matrix Ψ of dimension N × (K + q) whose t-th row is [Φ(xt)T | ˆϵ(0) T t−1:t−q]. In other words, Ψt = [ϕ1(xt), . . . , ϕK(xt) | ˆϵ(0) t−1, . . . , ˆϵ(0) t−q] includes both the basis functions and the lagged residuals. Let γ = (βT , θ1, . . . , θq)T be the combined coefficient vector of length K + q. Then an OLS estimate for γ is given by: ˆγ = (ΨT Ψ)−1 ΨT Y . This yields refined estimates ˆβ (the first K entries of ˆγ) and ˆθ1, . . . , ˆθq (the last q entries of ˆγ) simultaneously. Once we have these estimates, we can form the final fitted model: ˆmt = Φ(xt)T ˆβ , ˆy(d) t+1 = ˆmt+1 + q X j=1 ˆθj ˆϵt+1−j , which provides one-step-ahead forecasts for the differenced series. (Here ˆϵt+1−j are the in-sample residuals at time t + 1 −j.) In essence, ˆmt is a nonparametric (spline-based) estimate of the AR part, and the ˆθj adjust for the remaining serial correlation (the MA part). Under standard conditions for nonparametric regression (in particular, as K →∞and K/N →0 as N →∞), the spline approximation mt = Φ(xt)T β can approximate any sufficiently smooth function of the lags. This suggests that as we increase the number of basis functions K, the Galerkin-ARIMA model can in principle capture the true underlying relationship among the past values arbitrarily well. 4 4 Error Analysis One major purpose of proposing the Galerkin-ARIMA algorithm is to achieve better accuracy or efficiency than classical ARIMA. We therefore analyze the bias, variance, and computational complexity of the new model, comparing it to the standard ARIMA approach. 4.1 Bias First, consider the data-generating process to be a linear ARMA(p, q) process: yt = p X i=1 ψ∗ i yt−i + q X j=1 θ∗ j ϵt−j + ϵt, ϵt iid ∼N(0, σ2) , and suppose we fit the model with the correct orders p and q. Based on standard time-series theory [see Hannan and Rissanen, 1982, Brockwell and Davis, 1991], under regular conditions the estimators of the true parameters are consistent. In particular, for one-step-ahead prediction we have ˆΨ →Ψ∗ (in probability) as N →∞, where Ψ∗= (ψ∗ 1, . . . , ψ∗ p, θ∗ 1, . . . , θ∗ q, σ2) are the true parameters. Consequently, the 1-step forecast from a correctly specified ARIMA model converges in probability to the true value: ˆyt+1|t = p X i=1 ˆψi,N yt+1−i + q X j=1 ˆθj,N ˆϵt+1−j −→ p yt+1 , where ˆψi,N and ˆθj,N are the estimates based on N observations. In particular, the asymptotic bias of the one-step forecast is zero: p−limN→∞ \u0010 yt+1 −ˆyt+1|t \u0011 = 0 . This confirms that a well-specified ARIMA model is asymptotically unbiased under the usual assumptions. Now consider the Galerkin-ARIMA model. Let the lag-response relationship be some smooth but unknown function f : Rp →R such that y(d) t = f(xt)+ (MA terms) + noise. We approximate f(x) by the spline expansion mt = Φ(xt)T β using K basis functions. Under standard smoothness conditions (e.g. f belongs to a Hölder class with smoothness index r) and for x in a compact domain, the Jackson-Bernstein inequality from approximation theory guarantees that the approximation error can be made arbitrarily small as K grows. In particular, one result (see, e.g., de Boor, 1978) states that: inf β∈RK sup x∈C f(x) −Φ(x)T β = O(K−r/p) , where r characterizes the smoothness of f. This means the bias due to using a finite basis of size K decreases as K increases (assuming f is sufficiently smooth). Thus, if we let K →∞as N →∞ (but slowly enough that K/N →0; see below), the bias of ˆmt as an estimator of the true f(xt) will tend to zero. In summary, the Galerkin-ARIMA model can also be made asymptotically unbiased: as the number of basis functions grows, it can recover the true AR relationship without systematic error. 4.2 Variance and Consistency For the classical ARIMA MLE, the parameter estimates are asymptotically normal. In fact, if ˆΨ is the MLE of Ψ∗, then under regular conditions: √ N (ˆΨ −Ψ∗) d−→N 0, I(Ψ∗)−1\u0001 , where I(Ψ∗) is the Fisher information matrix evaluated at the true parameters [Brockwell and Davis, 1991]. This implies Var(ˆΨ) = O(1/N) for each fixed number of parameters (since the information matrix converges to a constant). In the Galerkin-ARIMA approach, the number of parameters γ = (β1, . . . , βK, θ1, . . . , θq) grows with K. The OLS variance-covariance for ˆγ is: Var(ˆγ) = σ2 (ΨT Ψ)−1 ≈σ2 1 N \u0010 p−limN→∞ ΨT Ψ N \u0011−1 . 5 Roughly speaking, each coefficient in γ has variance on the order of 1/N, but there are (K + q) coefficients. If K grows with N, this introduces a trade-off between bias and variance. In fact, Var(ˆγ) will scale on the order of (K + q)/N. To keep the variance of each coefficient bounded, we require K = o(N) (so that (K + q)/N →0). Typically, in nonparametric regression one balances bias and variance by letting K grow at a suitable rate relative to N. A common choice (optimal in a minimax sense for r-smooth functions in Rp) is K ∼N p 2r+p [Stone, 1985]. With such a choice, one can show that if f has Hölder smoothness r and we let K →∞with K/N →0, then ∥ˆf −f∥2 2 = O \u0010 K−2r/p + K N \u0011 , which is the standard convergence rate for nonparametric regression [Stone, 1985]. By plugging in K ∼N p/(2r+p), the two terms in the error bound become of the same order, yielding ∥ˆf −f∥2 2 = O(N −2r/(2r+p)), which approaches 0 as N →∞. This result implies that, with an appropriate growth of K and under smoothness assumptions, the Galerkin-ARIMA estimator ˆf is consistent for the true function f(x). 4.3 Computational Complexity We briefly compare the computational complexity of fitting a standard ARIMA model versus the Galerkin-ARIMA model. In a linear ARIMA model, evaluating the objective (likelihood or sum of squared errors) for a given parameter guess and computing gradients typically requires O((p + q)N) operations per iteration (because each residual ϵt depends on p + q past terms). Let I be the number of iterations needed for convergence of the numerical optimizer. Then the overall complexity of computing the MLE is on the order of O(I · (p + q) N). In practice, I (for something like Newton’s method) might be larger than p + q, and each iteration can be costly for long series. In the Galerkin-ARIMA approach, the main cost comes from two least-squares fits. Constructing the basis function matrix Φ costs O(p N · K) if each of the K basis functions takes O(p) to evaluate for a given xt (for example, polynomial splines might be evaluated in constant time per basis). Solving the OLS for β by direct methods is O(NK2) (since computing (ΦT Φ) is O(NK2) and inverting the K × K matrix is O(K3), though if K is not too large this is manageable). Solving the second OLS for γ is O(N(K + q)2) by a similar reasoning. Combining these, the total complexity for Galerkin-ARIMA is approximately OGAL ≈O(p N K) + O(N(K + q)2) . For reasonably chosen K, this can be significantly smaller than the ARIMA MLE cost. For instance, if I is on the order of N 1/2 (a rough scenario for some difficult likelihood surfaces) or if p and q are moderately large, the direct OLS approach can be advantageous. The ratio of complexities is roughly: OARIMA OGAL ≈I (p + q) N N (K + q)2 = I (p + q) (K + q)2 . In many situations, we have I > p ≈q and we might choose K much smaller than N. As a concrete example, for one-step forecasting (M = 1), if I ∼100, p+q ∼10, and K ∼10, then the above ratio suggests ARIMA MLE could be around 100·10 (10+10)2 = 1000 400 = 2.5 times slower than Galerkin-ARIMA per forecast. For rolling forecasts over a window of M time points, the computational savings could be multiplied further by a factor on the order of M. Of course, the exact speed gain depends on implementation details and the cost of basis function evaluations, but this analysis indicates that Galerkin-ARIMA has the potential to be computationally more efficient, especially for long series or when a moderate number of basis functions suffices. 5 Numerical Experiments 5.1 Synthetic Data Generation All series have length n = 300 and are generated with a fixed random seed for reproducibility. We consider four toy processes: 1. Noisy ARMA(2,1): yt = 0.6 yt−1 −0.3 yt−2 + 0.5 ϵt−1 + ϵt, ϵt ∼N(0, 1), y1 = y2 = 0. 6 2. Seasonal sine + noise: yt = sin \u0010 2π t 20 \u0011 + 0.5 ηt, ηt ∼N(0, 1). 3. Linear trend + AR(1): yt = 0.01 t + 0.8 yt−1 + νt, νt ∼N(0, 0.52), y0 = 0. 4. Nonlinear recursion + noise: yt = 0.5 yt−1 −0.2 y2 t−1 + ξt, ξt ∼N(0, 0.72), y0 = 0. These four processes span a wide variety of dynamics—linear autoregression with a moving-average component, periodic fluctuations, deterministic trend combined with stochastic feedback, and a purely nonlinear recursion. By testing on data that exhibit both classical ARMA behavior and more challenging seasonal, trending, and nonlinear patterns, we can rigorously evaluate whether Galerkin-ARIMA’s flexible basis expansion captures complex dependencies that a standard ARIMA may miss, while still recovering simple linear structure when appropriate. 5.2 Forecasting Setup We perform rolling one-step-ahead forecasts on each series using: • a fixed training window of length W = 100, • a forecast horizon of H = 150, and • Monte Carlo replications R = 10 (to average out noise). At each time i = W, . . . , W + H −1, we fit two models with the same order pair (p, q): Both ARIMA(p, 0, q) and Galerkin-ARIMA(p, q) are evaluated at the same set of order pairs (p, q) ∈{(1, 0), (5, 0), (0, 1), (0, 5), (1, 5), (5, 1)}. The ARIMA(p, 0, q) model is fit by maximizing the Gaussian likelihood via the statsmodels.tsa.arima.ARIMA routine, where p denotes the number of autoregressive lags and q the number of moving-average lags. The Galerkin-ARIMA(p, q) model employs a two-stage Galerkin approximation: in the first (AR) stage, we regress yt on the polynomial basis {1, yt−1, . . . , yt−p, y2 t−1, . . . , y2 t−p} to obtain coefficients ˆβ and residuals ˆϵ(0) t , and in the second (MA) stage, we regress these residuals on the basis {1, ˆϵ(0) t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ(0) t−q)2} to obtain coefficients ˆα for the moving-average component. For each model and each (p, q) combina- tion, we record the mean absolute error (MAE), root-mean-squared error (RMSE), total CPU time, and average CPU time per fit over the H rolling forecasts, then average these metrics over the R Monte Carlo runs. 5.3 Result Table 1 collects the MAE, RMSE, total and per-fit CPU times for ARIMA and Galerkin-ARIMA across all four synthetic datasets and six choices of (p, q). A few key patterns emerge. First, when (p, q) fail to cover the true dynamics (e.g. pure MA or pure AR fits), both methods exhibit elevated MAE/RMSE, but Galerkin-ARIMA sometimes smooths slightly more aggressively, yielding marginally higher errors. Second, as soon as (p, q) match or exceed the underlying orders (for example (1, 5) or (5, 1) on the Noisy_ARMA data, or (5, 0) on the Trend_AR data), the accuracy of Galerkin-ARIMA converges to that of classical ARIMA—the two lines in the corresponding panels become virtually indistinguishable. Third, across every dataset and order, Galerkin-ARIMA runs orders of magnitude faster: total CPU time drops from seconds to hundredths of a second, and average per-fit time falls from ∼10−2s to ∼10−4s. In short, Galerkin-ARIMA attains almost identical forecasting performance to ARIMA as soon as the basis dimension is sufficient, while dramatically reducing computation time. 7 Dataset p q Algorithm MAE RMSE Total Time Avg Time Noisy_ARMA 0 1 ARIMA 0.8636 1.0457 1.7318 0.0115 Galerkin-ARIMA 0.9521 1.1846 0.0743 0.0005 0 5 ARIMA 0.8324 1.0045 4.1475 0.0276 Galerkin-ARIMA 0.8331 1.0148 0.0937 0.0006 1 0 ARIMA 0.9494 1.1816 1.2780 0.0085 Galerkin-ARIMA 0.9521 1.1846 0.0405 0.0003 1 5 ARIMA 0.8382 1.0110 11.1948 0.0746 Galerkin-ARIMA 0.8402 1.0249 0.1021 0.0007 5 0 ARIMA 0.8320 1.0145 3.8751 0.0258 Galerkin-ARIMA 0.8331 1.0148 0.0605 0.0004 5 1 ARIMA 0.8413 1.0272 6.4902 0.0433 Galerkin-ARIMA 0.8298 1.0133 0.0994 0.0007 Nonlinear 0 1 ARIMA 0.5441 0.6742 1.2896 0.0086 Galerkin-ARIMA 0.5180 0.6546 0.0741 0.0005 0 5 ARIMA 0.5312 0.6614 3.4696 0.0231 Galerkin-ARIMA 0.5405 0.6790 0.0945 0.0006 1 0 ARIMA 0.5296 0.6595 1.1266 0.0075 Galerkin-ARIMA 0.5180 0.6546 0.0409 0.0003 1 5 ARIMA 0.5373 0.6665 9.2855 0.0619 Galerkin-ARIMA 0.5343 0.6732 0.1013 0.0007 5 0 ARIMA 0.5321 0.6642 2.9177 0.0195 Galerkin-ARIMA 0.5405 0.6790 0.0608 0.0004 5 1 ARIMA 0.5305 0.6636 7.0720 0.0471 Galerkin-ARIMA 0.5400 0.6809 0.0998 0.0007 Seasonal 0 1 ARIMA 0.5924 0.7261 1.3999 0.0093 Galerkin-ARIMA 0.5305 0.6784 0.0740 0.0005 0 5 ARIMA 0.5280 0.6734 4.2973 0.0286 Galerkin-ARIMA 0.5331 0.6721 0.0940 0.0006 1 0 ARIMA 0.5338 0.6818 1.3825 0.0092 Galerkin-ARIMA 0.5305 0.6784 0.0404 0.0003 1 5 ARIMA 0.5401 0.6745 11.8796 0.0792 Galerkin-ARIMA 0.5349 0.6693 0.1018 0.0007 5 0 ARIMA 0.5291 0.6642 3.8499 0.0257 Galerkin-ARIMA 0.5331 0.6721 0.0600 0.0004 5 1 ARIMA 0.5017 0.6219 12.4039 0.0827 Galerkin-ARIMA 0.5331 0.6756 0.1015 0.0007 Trend_AR 0 1 ARIMA 2.4277 2.6177 1.7387 0.0116 Galerkin-ARIMA 0.3941 0.5017 0.0741 0.0005 0 5 ARIMA 0.8587 0.9952 8.4002 0.0560 Galerkin-ARIMA 0.4165 0.5186 0.0950 0.0006 1 0 ARIMA 0.3879 0.4989 2.1608 0.0144 Galerkin-ARIMA 0.3941 0.5017 0.0407 0.0003 1 5 ARIMA 0.3932 0.4977 11.6516 0.0777 Galerkin-ARIMA 0.4139 0.5252 0.1032 0.0007 5 0 ARIMA 0.3912 0.5000 7.1304 0.0475 Galerkin-ARIMA 0.4165 0.5186 0.0600 0.0004 5 1 ARIMA 0.3917 0.5054 16.7612 0.1117 Galerkin-ARIMA 0.4163 0.5214 0.1010 0.0007 Table 1: Averaged over 10 runs: MAE, RMSE, total and average CPU time for ARIMA vs. Galerkin- ARIMA. 8 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 1: One-step forecasts on the Noisy_ARMA series for various (p, q). All six panels in Figure 1 draw from the same underlying ARMA(2,1) process, and together they vividly illustrate how the autoregressive order p and moving-average order q shape one-step forecasts. When p = 0 (pure MA fits), both ARIMA and Galerkin-ARIMA must rely entirely on past innova- tions: with only q = 1 they flatten out the rapid AR(2) swings and lag behind the true curve, while increasing to q = 5 gives them enough “memory” of past shocks that they begin to partially mimic the missing AR feedback. Conversely, when q = 0 (pure AR fits), a single lag (p = 1) captures general level shifts but not the innovation noise, whereas bumping to p = 5 lets each method recover the AR(2) oscillations almost perfectly—even without any MA component—since the higher-order AR basis soaks up residual randomness. Once both p and q are positive (for example, (p, q) = (1, 5) or (5, 1)), each model’s specification fully spans the true two-lag AR and one-lag MA dynamics. In these richer settings, ARIMA’s maximum-likelihood estimates and our two-stage Galerkin projections converge on virtually identical forecasts, tracing the black “True” series with only vanishing discrepancies. In short, raising either p or q in isolation compensates partially for the missing term, but only the combination of sufficient AR and MA orders unlocks near-perfect recovery of the data-generator. 9 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 2: One-step forecasts on the Seasonal series for various (p, q). Figure 2 displays one-step forecasts on the seasonal sine series yt = sin(2πt/20) + 0.5 ηt. When p = 0 (pure MA), an MA(1) fit (q = 1) overly smooths the oscillations and fails to track the peaks of each cycle, while increasing to q = 5 spreads past shocks across a full period and yields a visibly improved—but still lagged—sinusoidal trace. When q = 0 (pure AR), AR(1) (p = 1) captures the local level but cannot reproduce the periodic up–down swings, whereas AR(5) (p = 5) begins to recover the 20-step seasonality by regressing on multiple lags, albeit with slight phase shifts. Finally, combining both AR and MA terms (p = 1, q = 5 or p = 5, q = 1) gives the best result: the AR component provides the backbone of the sine wave, and the MA component corrects residual phase and amplitude errors. Throughout, Galerkin-ARIMA (red) closely parallels ARIMA (blue), with minor additional smoothing at low orders due to the polynomial residual basis, but converges to essentially identical forecasts once (p, q) span the true seasonal dynamics. 10 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 3: One-step forecasts on the Trend_AR series for various (p, q). Figure 3 displays one-step forecasts on the Trend_AR series yt = 0.01 t+0.8 yt−1 +νt with various (p, q). When p = 0 (pure MA), an MA(1) fit (q = 1) dramatically underestimates the upward drift and remains far below the true curve, while increasing to q = 5 improves the level somewhat but still fails to capture the deterministic trend. When q = 0 (pure AR), an AR(1) fit (p = 1) follows the slope closely—since it directly models the feedback—but the residual noise causes small phase-offsets; raising to AR(5) (p = 5) further refines the fit by absorbing additional past values, producing an almost exact match to the trend. Finally, combining AR and MA (p = 1, q = 5 or p = 5, q = 1) yields similarly excellent forecasts: the AR term captures the linear drift, and the MA term smooths remaining volatility. Throughout, Galerkin-ARIMA (red) mirrors classical ARIMA (blue) with negligible differences once the basis dimension is sufficient, but it falls back to a lower level under purely MA specifications because no trend regressor is included in the residual basis. 11 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 4: One-step forecasts on the Nonlinear series for various (p, q). Figure 4 shows one-step forecasts on the Nonlinear series (data generated by yt = 0.5 yt−1 − 0.2 y2 t−1 + ξt). When p = 0 (pure MA), both ARIMA and Galerkin-ARIMA rely solely on past residuals: with q = 1 they heavily smooth over the sharp nonlinear jumps, and even q = 5 only partially recovers those excursions because no direct lag-value information is used. Conversely, when q = 0 (pure AR), an AR(1) fit (p = 1) tracks the general drift but misses the innovation-driven peaks and troughs, whereas an AR(5) fit (p = 5) captures more of the short-term nonlinearity by regressing on five past values (and their squares), albeit still underestimating extreme outliers. Finally, combining both AR and MA components (p = 1, q = 5 or p = 5, q = 1) yields the closest alignment with the true curve: the AR term models the smooth feedback, while the MA term corrects for residual spikes. In all cases, Galerkin-ARIMA (red) parallels classical ARIMA (blue) in shape, with small differences when the simple polynomial residual basis cannot fully mimic ARIMA’s optimized MA coefficients. As before, once (p, q) are large enough to span the nonlinear dynamics, the two approaches produce virtually indistinguishable forecasts. 12 References George E. P. Box and Gwilym M. Jenkins. Time Series Analysis: Forecasting and Control. Holden- Day, San Francisco, 1970. Peter J. Brockwell and Richard A. Davis. Time Series: Theory and Methods. Springer, New York, 2nd edition, 1991. Carl de Boor. A Practical Guide to Splines. Springer-Verlag, New York, 1978. Mikhail Dokuchaev, Guanglu Zhou, and Song Wang. A modification of galerkin’s method for option pricing. Journal of Industrial and Management Optimization, 18(4):2483–2504, 2022. doi: 10.3934/jimo.2021077. Matthew M. Graham, Alexandre H. Thiery, and Alexandros Beskos. Manifold markov chain monte carlo methods for bayesian inference in diffusion models. Journal of the Royal Statistical Society: Series B, 85(4):1229–1256, 2023. doi: 10.1111/rssb.12517. E. J. Hannan and J. Rissanen. Recursive estimation of mixed autoregressive-moving average order. Biometrika, 69(1):81–94, 1982. M. Kushnir and K. Tokarieva. A generalization of the arima model to the nonlinear and continuous cases. Cybernetics and Systems Analysis, 59(6):900–909, 2023. doi: 10.1007/s10559-023-00625-8. Charles J. Stone. Additive regression and other nonparametric models. Annals of Statistics, 13(2): 689–705, 1985. Qiang Sun, Wei Chen, and Zhi-Qiang Li. An improved arima stock price forecasting method based on b-spline and model averaging. Academic Journal of Computing & Information Science, 5(10): 14–20, 2022. doi: 10.25236/AJCIS.2022.051003. W. M. Thupeng, R. Sivasamy, and O. A. Daman. Rainfall series forecasting models by ARIMA, NN, and HOMM methods. Advances and Applications in Statistics, 91(1):83–98, 2024. doi: 10.17654/0972361724007. Pham Hoang Vuong, Lam Hung Phu, Tran Hong Van Nguyen, Le Nhat Duy, Pham The Bao, and Tan Dat Trinh. A bibliometric literature review of stock price forecasting: From statistical model to deep learning approach. Science Progress, 107(1):1–31, 2024. doi: 10.1177/00368504241236557. Xiaoqian Wang, Yanfei Kang, Rob J. Hyndman, and Feng Li. Distributed ARIMA models for ultra-long time series. International Journal of Forecasting, 39(3):1163–1184, 2023. doi: 10.1016/ j.ijforecast.2022.05.001. 13"
    },
    "section_count": 1,
    "identified_sections": [
      "body"
    ]
  },
  "tables": {
    "tables": [
      {
        "table_id": "table_1",
        "page": 5,
        "start_line": 2,
        "end_line": 4,
        "row_count": 3,
        "detection_method": "data_pattern",
        "raw_text": "One major purpose of proposing the Galerkin-ARIMA algorithm is to achieve better accuracy\nor efficiency than classical ARIMA. We therefore analyze the bias, variance, and computational\ncomplexity of the new model, comparing it to the standard ARIMA approach.",
        "confidence": 0.6
      },
      {
        "table_id": "table_1",
        "page": 13,
        "start_line": 26,
        "end_line": 28,
        "row_count": 3,
        "detection_method": "data_pattern",
        "raw_text": "deep learning approach. Science Progress, 107(1):1–31, 2024. doi: 10.1177/00368504241236557.\nXiaoqian Wang, Yanfei Kang, Rob J. Hyndman, and Feng Li. Distributed ARIMA models for\nultra-long time series. International Journal of Forecasting, 39(3):1163–1184, 2023. doi: 10.1016/",
        "confidence": 0.6
      }
    ],
    "table_count": 2,
    "note": "Enhanced table detection with multiple heuristics. For production use, consider pdfplumber."
  },
  "citations": {
    "reference_list": [
      {
        "citation_id": "citation_1",
        "raw_text": "Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting Haojie Liu Department of Economics University of California, Riverside hliu332@ucr.edu Zihan Lin Department of Economics University of California, Riverside zlin169@ucr.edu Abstract Time-series models like ARIMA remain widely used for forecasting but limited to linear assumptions and high computational cost in large and complex datasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA and replace it with a flexible spline-based function estimated by Galerkin projection. This enables the model to capture nonlinear dependencies in lagged values and retain the MA component and Gaussian noise assumption. We derive a closed-form OLS estimator for the Galerkin coefficients and show the model is asymptotically unbiased and consistent under standard conditions. Our method bridges classical time-series modeling and nonparametric regression, which offering improved forecasting performance and computational efficiency. 1 Introduction Time-series forecasting plays a critical role in economics, finance, energy demand, weather prediction, and more. It turns historical data into proactive insight to understand temporal patterns. The Autoregressive Integrated Moving Average (ARIMA) model remains one of the most widely used time-series models today [Vuong et al., 2024]. However, as the volume and complexity of temporal data continue to grow, ARIMA becomes increasingly slow during forecasting—the more temporal data, the longer it takes [Wang et al., 2023]. Its speed is insufficient for handling the growing complexity of modern time series. Moreover, ARIMA assumes a fixed linear relationship among past values, which can be overly restrictive when the true dynamics are nonlinear or complex [Thupeng et al., 2024, Vuong et al., 2024]. Partial differential equations (PDEs) and stochastic differential equations (SDEs) are widely used in statistics to describe continuous-time systems with uncertainty [Graham et al., 2023]. They appear in many areas, such as asset pricing in finance or diffusion-based sampling in Bayesian inference [Dokuchaev et al., 2022]. Galerkin projection is usually used to solve the problems, which is a numerical method that approximates complex functions using basis expansions [Dokuchaev et al., 2022]. We apply this idea to time series forecasting in a discrete setting. To address this, we propose Galerkin-ARIMA, which uses the Galerkin method to replace the linear autoregressive component of the ARIMA model with a spline-based function learned via Galerkin projection [Sun et al., 2022]. Specifically, we approximate the AR term using a set of basis functions over the lag space, allowing the model to flexibly capture nonlinear dependencies [Kushnir and Tokarieva, 2023]. The MA term and Gaussian noise assumptions of ARIMA are retained. We show that under regular conditions, the Galerkin-ARIMA model is asymptotically consistent and unbiased, and it can be more computationally efficient than traditional maximum likelihood 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2507.07469v1 [stat.ML] 10 Jul 2025 estimation methods commonly used for ARIMA models. Our approach combines the structure of ARIMA with the flexibility of nonparametric regression and offers a new path forward for modeling complex time series dynamics—achieved through a closed form solution based on Ordinary Least Squares (OLS). 2 ARIMA The Autoregressive Integrated Moving Average (ARIMA) model is a fundamental time-series model that combines autoregressive (AR) terms, integration (differencing), and moving-average (MA) terms [Box and Jenkins, 1970, Brockwell and Davis, 1991]. In an ARIMA(p, d, q) model, the series is differenced d times to achieve stationarity, and then modeled as an ARMA(p, q). Using the backshift operator B (defined by Byt = yt−1), we can write the differenced series as: y(d) t = ∆dyt = (1 −B)d yt, where y(d) t denotes the d-th order differenced series. We then define the p-dimensional state (or lag) vector xt = y(d) t−1, y(d) t−2, . . . , y(d) t−p \u0001 ∈Rp, which collects the last p observed values of the differenced series. The AR part of the model is a linear combination of these lagged values: mt = p X i=1 ψi y(d) t−i, where ψi are the autoregressive coefficients. We assume the random shocks (innovations) {ϵt} are independent and identically distributed (i.i.d.) Gaussian with mean 0 and variance σ2, i.e. ϵt ∼N(0, σ2). The full ARIMA(p, d, q) model can then be written as: y(d) t = mt + q X j=1 θj ϵt−j + ϵt, which means the differenced series y(d) t is explained by the AR term mt and a moving-average term (a linear combination of the last q noise terms) plus the current noise. This formulation is standard for ARIMA models [see, e.g., Box and Jenkins, 1970]. We will next discuss how to estimate the coefficients Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) in this model. 2.1 Maximum Likelihood Estimation (MLE) The parameters of an ARIMA model are typically estimated by maximizing the likelihood of the observed data under the model [Brockwell and Davis, 1991]. For a Gaussian ARIMA model, maximizing the likelihood is equivalent to minimizing the sum of squared one-step-ahead forecast errors. Let Ψ = (ψ1, . . . , ψp, θ1, . . . , θq, σ2) collect all the unknown parameters. We can write the one-step prediction error at time t (for given parameters Ψ) as: ϵt(Ψ) = y(d) t − p X i=1 ψi y(d) t−i − q X j=1 θj ϵt−j(Ψ) . This ϵt(Ψ) represents the residual at time t, i.e. the difference between the observed differenced value y(d) t and its predicted value based on the model with parameters Ψ. The log-likelihood of the data (up to an additive constant) can be written as a function of Ψ: L(Ψ) = −N 2 ln(σ2) − 1 2σ2 N X t=1 ϵt(Ψ)2 , where N is the number of observations in the time series after differencing. In this expression, we have assumed Gaussian innovations so that ϵt(Ψ) are normally distributed. Maximizing L(Ψ) is equivalent to minimizing the sum of squared residuals P t ϵt(Ψ)2. The maximum likelihood estimator (MLE) ˆΨ is defined as: ˆΨ = arg max Ψ L(Ψ) . 2 In practice, this optimization does not have a closed-form solution for ARMA models and must be carried out by iterative numerical methods (e.g., Newton-Raphson or gradient descent) [Brockwell and Davis, 1991]. The above formulation highlights that the MLE seeks the parameter values that best explain the data by producing the smallest prediction errors. In summary, classical ARIMA modeling involves finding linear coefficients (ψi and θj) that minimize the prediction error. In the next section, we introduce a Galerkin approximation approach, which generalizes the AR component to a flexible function (using basis splines) rather than a fixed linear form. This will lead to the Galerkin-ARIMA method, where we aim to improve forecasting by capturing possibly nonlinear relationships in the lagged values. 3 Galerkin Approximation Before introducing the Galerkin-ARIMA model, we briefly review the Galerkin method in the context of stochastic differential equations (SDEs). Consider a one–dimensional SDE: dXt = b(Xt) dt + σ(Xt) dWt , where b(x) is the drift function and σ(x) is the diffusion coefficient. Suppose the drift function b(x) is expensive to compute or is unknown; we wish to approximate b(x) using a finite–dimensional function space spanned by a set of basis functions. Let {ϕj(x)}K j=1 be a family of basis functions (for example, B-splines of degree p on the domain Ω). We postulate an approximating drift of the form: bK(x) = K X j=1 θj ϕj(x) , with unknown coefficients θ = (θ1, . . . , θK) to be determined by a Galerkin projection. The Galerkin method requires that the residual (error) between the true drift and the approximated drift is orthogonal to the space spanned by the basis. Define the residual function r(x) = b(x) −bK(x). We introduce an inner product on the space of functions (weighted by a probability measure µ(x) on Ω): ⟨f, g⟩= Z Ω f(x) g(x) µ(x) dx . The Galerkin orthogonality condition imposes that the residual is orthogonal to each basis function ϕi: ⟨r, ϕi⟩= 0, i = 1, . . . , K. Substituting r(x) = b(x) −PK j=1 θjϕj(x), this condition becomes: Z Ω \u0010 b(x) − K X j=1 θj ϕj(x) \u0011 ϕi(x) µ(x) dx = 0, for each i = 1, . . . , K. Define the Gram matrix G ∈RK×K and the vector f ∈RK by: Gij = Z Ω ϕi(x) ϕj(x) µ(x) dx, fi = Z Ω b(x) ϕi(x) µ(x) dx. The Galerkin conditions then yield a linear system G θ = f. Once solved for θ, we obtain the Galerkin approximation bK(x) = PK j=1 θjϕj(x). Substituting this back into the SDE gives: dXt = bK(Xt) dt + σ(Xt) dWt , which is a spline-based approximation of the original dynamics. 3.1 AR-term Approximation via Galerkin Method We now return to time-series modeling. The key idea of Galerkin-ARIMA is to replace the fixed linear AR structure with a more flexible function approximated by basis functions, while still retaining the MA component. Essentially, we treat the p-dimensional lag vector as an input to an unknown 3 function f(·) (analogous to a drift function in an SDE), and we approximate this function using a finite basis expansion. Let Φ(xt) = [ϕ1(xt), ϕ2(xt), . . . , ϕK(xt)]T ∈RK be a vector of K basis function evaluations at the state xt = (y(d) t−1, . . . , y(d) t−p). For example, each ϕj could be a multi-variate B-spline or some polynomial basis defined on the lag space. We then posit that the AR component mt can be represented as: mt = Φ(xt)T β , β ∈RK, where β = (β1, . . . , βK)T are the coefficients of the basis expansion. In other words, mt is the Galerkin approximation of the potentially nonlinear relationship between past values and the next differenced value y(d) t . The Galerkin-ARIMA model then becomes: y(d) t = mt + q X j=1 θj ϵt−j + ϵt , similar in form to the classical ARIMA, but with mt now given by a flexible spline-based function rather than a fixed linear combination of lags. Estimating the coefficients in this Galerkin framework can be done in closed form using least squares, thanks to the linearity in the parameters β. First, we fit the basis-function coefficients β to approximate the AR part. This can be done by regressing y(d) t onto Φ(xt). In matrix terms, let Y = [y(d) 1 , y(d) 2 , . . . , y(d) N ]T be the vector of responses (differenced series), and let Φ be the N × K matrix whose t-th row is Φ(xt)T . Then the ordinary least squares (OLS) solution for β is: ˆβ = (ΦT Φ)−1 ΦT Y , provided N > K and the basis functions are linearly independent. This ˆβ minimizes the sum of squared errors P t(y(d) t −Φ(xt)T β)2. Using ˆβ, we can compute the fitted residuals (initially ignoring the MA terms) as: ˆϵ(0) t = y(d) t −Φ(xt)T ˆβ , for each t. These residuals ˆϵ(0) t represent the remaining structure after accounting for the spline-based AR approximation. Next, to estimate the moving-average coefficients θ1, . . . , θq, we include the lagged residuals in a sec- ond regression. For each t, form the vector of the last q residuals ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−q]T . Now define the combined regression design matrix Ψ of dimension N × (K + q) whose t-th row is [Φ(xt)T | ˆϵ(0) T t−1:t−q]. In other words, Ψt = [ϕ1(xt), . . . , ϕK(xt) | ˆϵ(0) t−1, . . . , ˆϵ(0) t−q] includes both the basis functions and the lagged residuals. Let γ = (βT , θ1, . . . , θq)T be the combined coefficient vector of length K + q. Then an OLS estimate for γ is given by: ˆγ = (ΨT Ψ)−1 ΨT Y . This yields refined estimates ˆβ (the first K entries of ˆγ) and ˆθ1, . . . , ˆθq (the last q entries of ˆγ) simultaneously. Once we have these estimates, we can form the final fitted model: ˆmt = Φ(xt)T ˆβ , ˆy(d) t+1 = ˆmt+1 + q X j=1 ˆθj ˆϵt+1−j , which provides one-step-ahead forecasts for the differenced series. (Here ˆϵt+1−j are the in-sample residuals at time t + 1 −j.) In essence, ˆmt is a nonparametric (spline-based) estimate of the AR part, and the ˆθj adjust for the remaining serial correlation (the MA part). Under standard conditions for nonparametric regression (in particular, as K →∞and K/N →0 as N →∞), the spline approximation mt = Φ(xt)T β can approximate any sufficiently smooth function of the lags. This suggests that as we increase the number of basis functions K, the Galerkin-ARIMA model can in principle capture the true underlying relationship among the past values arbitrarily well. 4 4 Error Analysis One major purpose of proposing the Galerkin-ARIMA algorithm is to achieve better accuracy or efficiency than classical ARIMA. We therefore analyze the bias, variance, and computational complexity of the new model, comparing it to the standard ARIMA approach. 4.1 Bias First, consider the data-generating process to be a linear ARMA(p, q) process: yt = p X i=1 ψ∗ i yt−i + q X j=1 θ∗ j ϵt−j + ϵt, ϵt iid ∼N(0, σ2) , and suppose we fit the model with the correct orders p and q. Based on standard time-series theory [see Hannan and Rissanen, 1982, Brockwell and Davis, 1991], under regular conditions the estimators of the true parameters are consistent. In particular, for one-step-ahead prediction we have ˆΨ →Ψ∗ (in probability) as N →∞, where Ψ∗= (ψ∗ 1, . . . , ψ∗ p, θ∗ 1, . . . , θ∗ q, σ2) are the true parameters. Consequently, the 1-step forecast from a correctly specified ARIMA model converges in probability to the true value: ˆyt+1|t = p X i=1 ˆψi,N yt+1−i + q X j=1 ˆθj,N ˆϵt+1−j −→ p yt+1 , where ˆψi,N and ˆθj,N are the estimates based on N observations. In particular, the asymptotic bias of the one-step forecast is zero: p−limN→∞ \u0010 yt+1 −ˆyt+1|t \u0011 = 0 . This confirms that a well-specified ARIMA model is asymptotically unbiased under the usual assumptions. Now consider the Galerkin-ARIMA model. Let the lag-response relationship be some smooth but unknown function f : Rp →R such that y(d) t = f(xt)+ (MA terms) + noise. We approximate f(x) by the spline expansion mt = Φ(xt)T β using K basis functions. Under standard smoothness conditions (e.g. f belongs to a Hölder class with smoothness index r) and for x in a compact domain, the Jackson-Bernstein inequality from approximation theory guarantees that the approximation error can be made arbitrarily small as K grows. In particular, one result (see, e.g., de Boor, 1978) states that: inf β∈RK sup x∈C f(x) −Φ(x)T β = O(K−r/p) , where r characterizes the smoothness of f. This means the bias due to using a finite basis of size K decreases as K increases (assuming f is sufficiently smooth). Thus, if we let K →∞as N →∞ (but slowly enough that K/N →0; see below), the bias of ˆmt as an estimator of the true f(xt) will tend to zero. In summary, the Galerkin-ARIMA model can also be made asymptotically unbiased: as the number of basis functions grows, it can recover the true AR relationship without systematic error. 4.2 Variance and Consistency For the classical ARIMA MLE, the parameter estimates are asymptotically normal. In fact, if ˆΨ is the MLE of Ψ∗, then under regular conditions: √ N (ˆΨ −Ψ∗) d−→N 0, I(Ψ∗)−1\u0001 , where I(Ψ∗) is the Fisher information matrix evaluated at the true parameters [Brockwell and Davis, 1991]. This implies Var(ˆΨ) = O(1/N) for each fixed number of parameters (since the information matrix converges to a constant). In the Galerkin-ARIMA approach, the number of parameters γ = (β1, . . . , βK, θ1, . . . , θq) grows with K. The OLS variance-covariance for ˆγ is: Var(ˆγ) = σ2 (ΨT Ψ)−1 ≈σ2 1 N \u0010 p−limN→∞ ΨT Ψ N \u0011−1 . 5 Roughly speaking, each coefficient in γ has variance on the order of 1/N, but there are (K + q) coefficients. If K grows with N, this introduces a trade-off between bias and variance. In fact, Var(ˆγ) will scale on the order of (K + q)/N. To keep the variance of each coefficient bounded, we require K = o(N) (so that (K + q)/N →0). Typically, in nonparametric regression one balances bias and variance by letting K grow at a suitable rate relative to N. A common choice (optimal in a minimax sense for r-smooth functions in Rp) is K ∼N p 2r+p [Stone, 1985]. With such a choice, one can show that if f has Hölder smoothness r and we let K →∞with K/N →0, then ∥ˆf −f∥2 2 = O \u0010 K−2r/p + K N \u0011 , which is the standard convergence rate for nonparametric regression [Stone, 1985]. By plugging in K ∼N p/(2r+p), the two terms in the error bound become of the same order, yielding ∥ˆf −f∥2 2 = O(N −2r/(2r+p)), which approaches 0 as N →∞. This result implies that, with an appropriate growth of K and under smoothness assumptions, the Galerkin-ARIMA estimator ˆf is consistent for the true function f(x). 4.3 Computational Complexity We briefly compare the computational complexity of fitting a standard ARIMA model versus the Galerkin-ARIMA model. In a linear ARIMA model, evaluating the objective (likelihood or sum of squared errors) for a given parameter guess and computing gradients typically requires O((p + q)N) operations per iteration (because each residual ϵt depends on p + q past terms). Let I be the number of iterations needed for convergence of the numerical optimizer. Then the overall complexity of computing the MLE is on the order of O(I · (p + q) N). In practice, I (for something like Newton’s method) might be larger than p + q, and each iteration can be costly for long series. In the Galerkin-ARIMA approach, the main cost comes from two least-squares fits. Constructing the basis function matrix Φ costs O(p N · K) if each of the K basis functions takes O(p) to evaluate for a given xt (for example, polynomial splines might be evaluated in constant time per basis). Solving the OLS for β by direct methods is O(NK2) (since computing (ΦT Φ) is O(NK2) and inverting the K × K matrix is O(K3), though if K is not too large this is manageable). Solving the second OLS for γ is O(N(K + q)2) by a similar reasoning. Combining these, the total complexity for Galerkin-ARIMA is approximately OGAL ≈O(p N K) + O(N(K + q)2) . For reasonably chosen K, this can be significantly smaller than the ARIMA MLE cost. For instance, if I is on the order of N 1/2 (a rough scenario for some difficult likelihood surfaces) or if p and q are moderately large, the direct OLS approach can be advantageous. The ratio of complexities is roughly: OARIMA OGAL ≈I (p + q) N N (K + q)2 = I (p + q) (K + q)2 . In many situations, we have I > p ≈q and we might choose K much smaller than N. As a concrete example, for one-step forecasting (M = 1), if I ∼100, p+q ∼10, and K ∼10, then the above ratio suggests ARIMA MLE could be around 100·10 (10+10)2 = 1000 400 = 2.5 times slower than Galerkin-ARIMA per forecast. For rolling forecasts over a window of M time points, the computational savings could be multiplied further by a factor on the order of M. Of course, the exact speed gain depends on implementation details and the cost of basis function evaluations, but this analysis indicates that Galerkin-ARIMA has the potential to be computationally more efficient, especially for long series or when a moderate number of basis functions suffices. 5 Numerical Experiments 5.1 Synthetic Data Generation All series have length n = 300 and are generated with a fixed random seed for reproducibility. We consider four toy processes: 1. Noisy ARMA(2,1): yt = 0.6 yt−1 −0.3 yt−2 + 0.5 ϵt−1 + ϵt, ϵt ∼N(0, 1), y1 = y2 = 0. 6 2. Seasonal sine + noise: yt = sin \u0010 2π t 20 \u0011 + 0.5 ηt, ηt ∼N(0, 1). 3. Linear trend + AR(1): yt = 0.01 t + 0.8 yt−1 + νt, νt ∼N(0, 0.52), y0 = 0. 4. Nonlinear recursion + noise: yt = 0.5 yt−1 −0.2 y2 t−1 + ξt, ξt ∼N(0, 0.72), y0 = 0. These four processes span a wide variety of dynamics—linear autoregression with a moving-average component, periodic fluctuations, deterministic trend combined with stochastic feedback, and a purely nonlinear recursion. By testing on data that exhibit both classical ARMA behavior and more challenging seasonal, trending, and nonlinear patterns, we can rigorously evaluate whether Galerkin-ARIMA’s flexible basis expansion captures complex dependencies that a standard ARIMA may miss, while still recovering simple linear structure when appropriate. 5.2 Forecasting Setup We perform rolling one-step-ahead forecasts on each series using: • a fixed training window of length W = 100, • a forecast horizon of H = 150, and • Monte Carlo replications R = 10 (to average out noise). At each time i = W, . . . , W + H −1, we fit two models with the same order pair (p, q): Both ARIMA(p, 0, q) and Galerkin-ARIMA(p, q) are evaluated at the same set of order pairs (p, q) ∈{(1, 0), (5, 0), (0, 1), (0, 5), (1, 5), (5, 1)}. The ARIMA(p, 0, q) model is fit by maximizing the Gaussian likelihood via the statsmodels.tsa.arima.ARIMA routine, where p denotes the number of autoregressive lags and q the number of moving-average lags. The Galerkin-ARIMA(p, q) model employs a two-stage Galerkin approximation: in the first (AR) stage, we regress yt on the polynomial basis {1, yt−1, . . . , yt−p, y2 t−1, . . . , y2 t−p} to obtain coefficients ˆβ and residuals ˆϵ(0) t , and in the second (MA) stage, we regress these residuals on the basis {1, ˆϵ(0) t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ(0) t−q)2} to obtain coefficients ˆα for the moving-average component. For each model and each (p, q) combina- tion, we record the mean absolute error (MAE), root-mean-squared error (RMSE), total CPU time, and average CPU time per fit over the H rolling forecasts, then average these metrics over the R Monte Carlo runs. 5.3 Result Table 1 collects the MAE, RMSE, total and per-fit CPU times for ARIMA and Galerkin-ARIMA across all four synthetic datasets and six choices of (p, q). A few key patterns emerge. First, when (p, q) fail to cover the true dynamics (e.g. pure MA or pure AR fits), both methods exhibit elevated MAE/RMSE, but Galerkin-ARIMA sometimes smooths slightly more aggressively, yielding marginally higher errors. Second, as soon as (p, q) match or exceed the underlying orders (for example (1, 5) or (5, 1) on the Noisy_ARMA data, or (5, 0) on the Trend_AR data), the accuracy of Galerkin-ARIMA converges to that of classical ARIMA—the two lines in the corresponding panels become virtually indistinguishable. Third, across every dataset and order, Galerkin-ARIMA runs orders of magnitude faster: total CPU time drops from seconds to hundredths of a second, and average per-fit time falls from ∼10−2s to ∼10−4s. In short, Galerkin-ARIMA attains almost identical forecasting performance to ARIMA as soon as the basis dimension is sufficient, while dramatically reducing computation time. 7 Dataset p q Algorithm MAE RMSE Total Time Avg Time Noisy_ARMA 0 1 ARIMA 0.8636 1.0457 1.7318 0.0115 Galerkin-ARIMA 0.9521 1.1846 0.0743 0.0005 0 5 ARIMA 0.8324 1.0045 4.1475 0.0276 Galerkin-ARIMA 0.8331 1.0148 0.0937 0.0006 1 0 ARIMA 0.9494 1.1816 1.2780 0.0085 Galerkin-ARIMA 0.9521 1.1846 0.0405 0.0003 1 5 ARIMA 0.8382 1.0110 11.1948 0.0746 Galerkin-ARIMA 0.8402 1.0249 0.1021 0.0007 5 0 ARIMA 0.8320 1.0145 3.8751 0.0258 Galerkin-ARIMA 0.8331 1.0148 0.0605 0.0004 5 1 ARIMA 0.8413 1.0272 6.4902 0.0433 Galerkin-ARIMA 0.8298 1.0133 0.0994 0.0007 Nonlinear 0 1 ARIMA 0.5441 0.6742 1.2896 0.0086 Galerkin-ARIMA 0.5180 0.6546 0.0741 0.0005 0 5 ARIMA 0.5312 0.6614 3.4696 0.0231 Galerkin-ARIMA 0.5405 0.6790 0.0945 0.0006 1 0 ARIMA 0.5296 0.6595 1.1266 0.0075 Galerkin-ARIMA 0.5180 0.6546 0.0409 0.0003 1 5 ARIMA 0.5373 0.6665 9.2855 0.0619 Galerkin-ARIMA 0.5343 0.6732 0.1013 0.0007 5 0 ARIMA 0.5321 0.6642 2.9177 0.0195 Galerkin-ARIMA 0.5405 0.6790 0.0608 0.0004 5 1 ARIMA 0.5305 0.6636 7.0720 0.0471 Galerkin-ARIMA 0.5400 0.6809 0.0998 0.0007 Seasonal 0 1 ARIMA 0.5924 0.7261 1.3999 0.0093 Galerkin-ARIMA 0.5305 0.6784 0.0740 0.0005 0 5 ARIMA 0.5280 0.6734 4.2973 0.0286 Galerkin-ARIMA 0.5331 0.6721 0.0940 0.0006 1 0 ARIMA 0.5338 0.6818 1.3825 0.0092 Galerkin-ARIMA 0.5305 0.6784 0.0404 0.0003 1 5 ARIMA 0.5401 0.6745 11.8796 0.0792 Galerkin-ARIMA 0.5349 0.6693 0.1018 0.0007 5 0 ARIMA 0.5291 0.6642 3.8499 0.0257 Galerkin-ARIMA 0.5331 0.6721 0.0600 0.0004 5 1 ARIMA 0.5017 0.6219 12.4039 0.0827 Galerkin-ARIMA 0.5331 0.6756 0.1015 0.0007 Trend_AR 0 1 ARIMA 2.4277 2.6177 1.7387 0.0116 Galerkin-ARIMA 0.3941 0.5017 0.0741 0.0005 0 5 ARIMA 0.8587 0.9952 8.4002 0.0560 Galerkin-ARIMA 0.4165 0.5186 0.0950 0.0006 1 0 ARIMA 0.3879 0.4989 2.1608 0.0144 Galerkin-ARIMA 0.3941 0.5017 0.0407 0.0003 1 5 ARIMA 0.3932 0.4977 11.6516 0.0777 Galerkin-ARIMA 0.4139 0.5252 0.1032 0.0007 5 0 ARIMA 0.3912 0.5000 7.1304 0.0475 Galerkin-ARIMA 0.4165 0.5186 0.0600 0.0004 5 1 ARIMA 0.3917 0.5054 16.7612 0.1117 Galerkin-ARIMA 0.4163 0.5214 0.1010 0.0007 Table 1: Averaged over 10 runs: MAE, RMSE, total and average CPU time for ARIMA vs. Galerkin- ARIMA. 8 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 1: One-step forecasts on the Noisy_ARMA series for various (p, q). All six panels in Figure 1 draw from the same underlying ARMA(2,1) process, and together they vividly illustrate how the autoregressive order p and moving-average order q shape one-step forecasts. When p = 0 (pure MA fits), both ARIMA and Galerkin-ARIMA must rely entirely on past innova- tions: with only q = 1 they flatten out the rapid AR(2) swings and lag behind the true curve, while increasing to q = 5 gives them enough “memory” of past shocks that they begin to partially mimic the missing AR feedback. Conversely, when q = 0 (pure AR fits), a single lag (p = 1) captures general level shifts but not the innovation noise, whereas bumping to p = 5 lets each method recover the AR(2) oscillations almost perfectly—even without any MA component—since the higher-order AR basis soaks up residual randomness. Once both p and q are positive (for example, (p, q) = (1, 5) or (5, 1)), each model’s specification fully spans the true two-lag AR and one-lag MA dynamics. In these richer settings, ARIMA’s maximum-likelihood estimates and our two-stage Galerkin projections converge on virtually identical forecasts, tracing the black “True” series with only vanishing discrepancies. In short, raising either p or q in isolation compensates partially for the missing term, but only the combination of sufficient AR and MA orders unlocks near-perfect recovery of the data-generator. 9 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 2: One-step forecasts on the Seasonal series for various (p, q). Figure 2 displays one-step forecasts on the seasonal sine series yt = sin(2πt/20) + 0.5 ηt. When p = 0 (pure MA), an MA(1) fit (q = 1) overly smooths the oscillations and fails to track the peaks of each cycle, while increasing to q = 5 spreads past shocks across a full period and yields a visibly improved—but still lagged—sinusoidal trace. When q = 0 (pure AR), AR(1) (p = 1) captures the local level but cannot reproduce the periodic up–down swings, whereas AR(5) (p = 5) begins to recover the 20-step seasonality by regressing on multiple lags, albeit with slight phase shifts. Finally, combining both AR and MA terms (p = 1, q = 5 or p = 5, q = 1) gives the best result: the AR component provides the backbone of the sine wave, and the MA component corrects residual phase and amplitude errors. Throughout, Galerkin-ARIMA (red) closely parallels ARIMA (blue), with minor additional smoothing at low orders due to the polynomial residual basis, but converges to essentially identical forecasts once (p, q) span the true seasonal dynamics. 10 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 3: One-step forecasts on the Trend_AR series for various (p, q). Figure 3 displays one-step forecasts on the Trend_AR series yt = 0.01 t+0.8 yt−1 +νt with various (p, q). When p = 0 (pure MA), an MA(1) fit (q = 1) dramatically underestimates the upward drift and remains far below the true curve, while increasing to q = 5 improves the level somewhat but still fails to capture the deterministic trend. When q = 0 (pure AR), an AR(1) fit (p = 1) follows the slope closely—since it directly models the feedback—but the residual noise causes small phase-offsets; raising to AR(5) (p = 5) further refines the fit by absorbing additional past values, producing an almost exact match to the trend. Finally, combining AR and MA (p = 1, q = 5 or p = 5, q = 1) yields similarly excellent forecasts: the AR term captures the linear drift, and the MA term smooths remaining volatility. Throughout, Galerkin-ARIMA (red) mirrors classical ARIMA (blue) with negligible differences once the basis dimension is sufficient, but it falls back to a lower level under purely MA specifications because no trend regressor is included in the residual basis. 11 (a) p = 0, q = 1 (b) p = 0, q = 5 (c) p = 1, q = 0 (d) p = 1, q = 5 (e) p = 5, q = 0 (f) p = 5, q = 1 Figure 4: One-step forecasts on the Nonlinear series for various (p, q). Figure 4 shows one-step forecasts on the Nonlinear series (data generated by yt = 0.5 yt−1 − 0.2 y2 t−1 + ξt). When p = 0 (pure MA), both ARIMA and Galerkin-ARIMA rely solely on past residuals: with q = 1 they heavily smooth over the sharp nonlinear jumps, and even q = 5 only partially recovers those excursions because no direct lag-value information is used. Conversely, when q = 0 (pure AR), an AR(1) fit (p = 1) tracks the general drift but misses the innovation-driven peaks and troughs, whereas an AR(5) fit (p = 5) captures more of the short-term nonlinearity by regressing on five past values (and their squares), albeit still underestimating extreme outliers. Finally, combining both AR and MA components (p = 1, q = 5 or p = 5, q = 1) yields the closest alignment with the true curve: the AR term models the smooth feedback, while the MA term corrects for residual spikes. In all cases, Galerkin-ARIMA (red) parallels classical ARIMA (blue) in shape, with small differences when the simple polynomial residual basis cannot fully mimic ARIMA’s optimized MA coefficients. As before, once (p, q) are large enough to span the nonlinear dynamics, the two approaches produce virtually indistinguishable forecasts. 12 References George E. P. Box and Gwilym M. Jenkins. Time Series Analysis: Forecasting and Control. Holden- Day, San Francisco, 1970. Peter J. Brockwell and Richard A. Davis. Time Series: Theory and Methods. Springer, New York, 2nd edition, 1991. Carl de Boor. A Practical Guide to Splines. Springer-Verlag, New York, 1978. Mikhail Dokuchaev, Guanglu Zhou, and Song Wang. A modification of galerkin’s method for option pricing. Journal of Industrial and Management Optimization, 18(4):2483–2504, 2022. doi: 10.3934/jimo.2021077. Matthew M. Graham, Alexandre H. Thiery, and Alexandros Beskos. Manifold markov chain monte carlo methods for bayesian inference in diffusion models. Journal of the Royal Statistical Society: Series B, 85(4):1229–1256, 2023. doi: 10.1111/rssb.12517. E. J. Hannan and J. Rissanen. Recursive estimation of mixed autoregressive-moving average order. Biometrika, 69(1):81–94, 1982. M. Kushnir and K. Tokarieva. A generalization of the arima model to the nonlinear and continuous cases. Cybernetics and Systems Analysis, 59(6):900–909, 2023. doi: 10.1007/s10559-023-00625-8. Charles J. Stone. Additive regression and other nonparametric models. Annals of Statistics, 13(2): 689–705, 1985. Qiang Sun, Wei Chen, and Zhi-Qiang Li. An improved arima stock price forecasting method based on b-spline and model averaging. Academic Journal of Computing & Information Science, 5(10): 14–20, 2022. doi: 10.25236/AJCIS.2022.051003. W. M. Thupeng, R. Sivasamy, and O. A. Daman. Rainfall series forecasting models by ARIMA, NN, and HOMM methods. Advances and Applications in Statistics, 91(1):83–98, 2024. doi: 10.17654/0972361724007. Pham Hoang Vuong, Lam Hung Phu, Tran Hong Van Nguyen, Le Nhat Duy, Pham The Bao, and Tan Dat Trinh. A bibliometric literature review of stock price forecasting: From statistical model to deep learning approach. Science Progress, 107(1):1–31, 2024. doi: 10.1177/00368504241236557. Xiaoqian Wang, Yanfei Kang, Rob J. Hyndman, and Feng Li. Distributed ARIMA models for ultra-long time series. International Journal of Forecasting, 39(3):1163–1184, 2023. doi: 10.1016/ j.ijforecast.2022.05.001. 13",
        "authors": [
          "California, R",
          "California, R",
          "Moreover, ARIMA",
          "Throughout, G",
          "Throughout, G"
        ],
        "title": "edu Zihan Lin Department of Economics University of California, Riverside zlin169@ucr",
        "year": "2024",
        "doi": "10.3934/jimo.2021077.",
        "url": ""
      }
    ],
    "in_text_citations": [
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 10737,
        "type": "numbered_parentheses",
        "context": "residuals (initially ignoring the MA terms) as: ˆϵ(0) t = y(d) t −Φ(xt)T ˆβ , for each t. These residua"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 10795,
        "type": "numbered_parentheses",
        "context": "y(d) t −Φ(xt)T ˆβ , for each t. These residuals ˆϵ(0) t represent the remaining structure after account"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 11069,
        "type": "numbered_parentheses",
        "context": "each t, form the vector of the last q residuals ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 11086,
        "type": "numbered_parentheses",
        "context": "vector of the last q residuals ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−q]T . Now define "
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 11097,
        "type": "numbered_parentheses",
        "context": "he last q residuals ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−q]T . Now define the combine"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 11116,
        "type": "numbered_parentheses",
        "context": " ˆϵ(0) t−1:t−q = [ˆϵ(0) t−1, ˆϵ(0) t−2, . . . , ˆϵ(0) t−q]T . Now define the combined regression design"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 11234,
        "type": "numbered_parentheses",
        "context": "mension N × (K + q) whose t-th row is [Φ(xt)T | ˆϵ(0) T t−1:t−q]. In other words, Ψt = [ϕ1(xt), . . . ,"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 11299,
        "type": "numbered_parentheses",
        "context": " In other words, Ψt = [ϕ1(xt), . . . , ϕK(xt) | ˆϵ(0) t−1, . . . , ˆϵ(0) t−q] includes both the basis f"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 11318,
        "type": "numbered_parentheses",
        "context": " = [ϕ1(xt), . . . , ϕK(xt) | ˆϵ(0) t−1, . . . , ˆϵ(0) t−q] includes both the basis functions and the la"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 19391,
        "type": "numbered_parentheses",
        "context": "t 20 \u0011 + 0.5 ηt, ηt ∼N(0, 1). 3. Linear trend + AR(1): yt = 0.01 t + 0.8 yt−1 + νt, νt ∼N(0, 0.52), y0 "
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 20986,
        "type": "numbered_parentheses",
        "context": "y2 t−p} to obtain coefficients ˆβ and residuals ˆϵ(0) t , and in the second (MA) stage, we regress thes"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 21070,
        "type": "numbered_parentheses",
        "context": "ge, we regress these residuals on the basis {1, ˆϵ(0) t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 21089,
        "type": "numbered_parentheses",
        "context": "e residuals on the basis {1, ˆϵ(0) t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ(0) t−q)2} to obtai"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 21101,
        "type": "numbered_parentheses",
        "context": "on the basis {1, ˆϵ(0) t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ(0) t−q)2} to obtain coefficien"
      },
      {
        "citation_text": "0",
        "full_match": "(0)",
        "position": 21123,
        "type": "numbered_parentheses",
        "context": " t−1, . . . , ˆϵ(0) t−q, (ˆϵ(0) t−1)2, . . . , (ˆϵ(0) t−q)2} to obtain coefficients ˆα for the moving-a"
      },
      {
        "citation_text": "2",
        "full_match": "(2)",
        "position": 25191,
        "type": "numbered_parentheses",
        "context": "ons: with only q = 1 they flatten out the rapid AR(2) swings and lag behind the true curve, while incre"
      },
      {
        "citation_text": "2",
        "full_match": "(2)",
        "position": 25537,
        "type": "numbered_parentheses",
        "context": "s bumping to p = 5 lets each method recover the AR(2) oscillations almost perfectly—even without any MA"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 26525,
        "type": "numbered_parentheses",
        "context": " sin(2πt/20) + 0.5 ηt. When p = 0 (pure MA), an MA(1) fit (q = 1) overly smooths the oscillations and f"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 26773,
        "type": "numbered_parentheses",
        "context": " lagged—sinusoidal trace. When q = 0 (pure AR), AR(1) (p = 1) captures the local level but cannot repro"
      },
      {
        "citation_text": "5",
        "full_match": "(5)",
        "position": 26870,
        "type": "numbered_parentheses",
        "context": " reproduce the periodic up–down swings, whereas AR(5) (p = 5) begins to recover the 20-step seasonality"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 27760,
        "type": "numbered_parentheses",
        "context": "t with various (p, q). When p = 0 (pure MA), an MA(1) fit (q = 1) dramatically underestimates the upwar"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 27992,
        "type": "numbered_parentheses",
        "context": "e deterministic trend. When q = 0 (pure AR), an AR(1) fit (p = 1) follows the slope closely—since it di"
      },
      {
        "citation_text": "5",
        "full_match": "(5)",
        "position": 28136,
        "type": "numbered_parentheses",
        "context": "al noise causes small phase-offsets; raising to AR(5) (p = 5) further refines the fit by absorbing addi"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 29277,
        "type": "numbered_parentheses",
        "context": "n is used. Conversely, when q = 0 (pure AR), an AR(1) fit (p = 1) tracks the general drift but misses t"
      },
      {
        "citation_text": "5",
        "full_match": "(5)",
        "position": 29383,
        "type": "numbered_parentheses",
        "context": "innovation-driven peaks and troughs, whereas an AR(5) fit (p = 5) captures more of the short-term nonli"
      },
      {
        "citation_text": "4",
        "full_match": "(4)",
        "position": 30577,
        "type": "numbered_parentheses",
        "context": "rnal of Industrial and Management Optimization, 18(4):2483–2504, 2022. doi: 10.3934/jimo.2021077. Matth"
      },
      {
        "citation_text": "4",
        "full_match": "(4)",
        "position": 30828,
        "type": "numbered_parentheses",
        "context": "nal of the Royal Statistical Society: Series B, 85(4):1229–1256, 2023. doi: 10.1111/rssb.12517. E. J. H"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 30985,
        "type": "numbered_parentheses",
        "context": "utoregressive-moving average order. Biometrika, 69(1):81–94, 1982. M. Kushnir and K. Tokarieva. A gener"
      },
      {
        "citation_text": "6",
        "full_match": "(6)",
        "position": 31142,
        "type": "numbered_parentheses",
        "context": "inuous cases. Cybernetics and Systems Analysis, 59(6):900–909, 2023. doi: 10.1007/s10559-023-00625-8. C"
      },
      {
        "citation_text": "2",
        "full_match": "(2)",
        "position": 31288,
        "type": "numbered_parentheses",
        "context": "her nonparametric models. Annals of Statistics, 13(2): 689–705, 1985. Qiang Sun, Wei Chen, and Zhi-Qian"
      },
      {
        "citation_text": "10",
        "full_match": "(10)",
        "position": 31489,
        "type": "numbered_parentheses",
        "context": "emic Journal of Computing & Information Science, 5(10): 14–20, 2022. doi: 10.25236/AJCIS.2022.051003. W."
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 31696,
        "type": "numbered_parentheses",
        "context": "thods. Advances and Applications in Statistics, 91(1):83–98, 2024. doi: 10.17654/0972361724007. Pham Ho"
      },
      {
        "citation_text": "1",
        "full_match": "(1)",
        "position": 31974,
        "type": "numbered_parentheses",
        "context": "l to deep learning approach. Science Progress, 107(1):1–31, 2024. doi: 10.1177/00368504241236557. Xiaoq"
      },
      {
        "citation_text": "3",
        "full_match": "(3)",
        "position": 32172,
        "type": "numbered_parentheses",
        "context": "e series. International Journal of Forecasting, 39(3):1163–1184, 2023. doi: 10.1016/ j.ijforecast.2022."
      }
    ],
    "reference_count": 1,
    "in_text_count": 34,
    "has_reference_section": true,
    "note": "Enhanced citation extraction with multiple detection methods"
  },
  "summary": {
    "total_pages": 13,
    "total_words": 5544,
    "sections_found": 1,
    "figures_found": 24,
    "tables_found": 2,
    "references_found": 1
  },
  "extraction_timestamp": "2025-07-12T03:42:48.860300",
  "figures": {
    "summary": {},
    "figure_count": 24
  }
}