# Body

Topological Machine Learning with Unreduced Persistence Diagrams Nicole Abreu1, Parker B. Edwards1, and Francis Motta1 1Department of Mathematics and Statistics, Florida Atlantic University, Boca Raton, FL Abstract Supervised machine learning pipelines trained on features derived from persistent homology have been experimentally observed to ignore much of the information contained in a persistence diagram. Computing persistence diagrams is often the most computationally demanding step in such a pipeline, however. To explore this, we introduce several methods to generate topological feature vectors from unreduced boundary matrices. We compared the performance of pipelines trained on vectorizations of unreduced PDs to vectorizations of fully-reduced PDs across several data and task types. Our results indicate that models trained on PDs built from unreduced diagrams can perform on par and even outperform those trained on fully-reduced diagrams on some tasks. This observation suggests that machine learning pipelines which incorporate topology-based features may benefit in terms of computational cost and performance by utilizing information contained in unreduced boundary matrices. 1 Introduction Topological feature extraction followed by subsequent machine learning comprises one of the now standard approaches to practical topological data analysis. Successful analyses following this strat- egy have been conducted on a wide range of data types in diverse scientific domains, including biology [5, 13, 23, 39], biomedicine [19, 33], neuroscience [26, 2], finance [24, 6], cosmology [28, 41], material science [32, 17], and many others [35, 25]. Supervised machine learning (ML) models trained on persistent homology (PH) based features often discard much of the information encoded in a persistence diagram, however, or perform well with vectorizations which discard information before training (e.g., [4, 12]). Significant computation is therefore wasted fully computing persistent homology from inputs, as this often demands the most computational resources of any step in a combined PH/ML pipeline. This is true even though substantial and ongoing improvements have been made to the process of boundary matrix reduction for PH (e.g., [8, 9, 11, 27, 31, 34]). Thus we are motivated to pursue approximate PH algorithms which use fewer computational resources but that still yield good performance in downstream ML tasks. In this work, we investi- gate a solution which avoids boundary matrix reduction entirely, and compare the discriminating power of information contained in unreduced boundary matrices against the topological information contained in fully reduced boundary matrices, i.e., persistence diagrams. 2 Background The theory of persistent homology can be cast in purely algebraic or categorical terms (e.g., [16]), but is most easily motivated geometrically. In the latter case, one constructs a finite abstract complex Σ, cubical or simplicial, from input data. This complex is equipped with a filtration function f : Σ →R which respects face relations from Σ in the sense that all faces of an element 1 arXiv:2507.07156v1 [stat.ML] 9 Jul 2025 σ ∈Σ map under f to values at most f(σ). Common filtrations include the Vietoris-Rips and Alpha filtrations for simplicial complexes built on point-cloud data (see [21, Chap. III.2,4]) and the lower-star filtration built for cubical complexes constructed from image data [20]. The process of computing a PD usually begins by transforming a filtered complex into a bound- ary matrix M ∈Zn×n 2 . The rows and columns of M are ordered by ascending filtration value, breaking ties in a way that respects face relations. One then proceeds to reduce this ordered matrix using a column-wise variant of typical matrix reduction algorithms. We give an overview of the “standard” algorithm here, as it motivates our unreduced persistence diagram constructions. First, define low(Mj) = max({i | Mi,j = 1}) with low(Mj) = −1 if Mj is a zero column. Reduce each column of a boundary matrix (proceeding from left to right), by replacing the j-th column Mj with the mod-2 sum Mi + Mj, whenever low(Mi) = low(Mj) and i < j, until the low 1 in column j is distinct from all low 1s in the preceding columns or column j consists of all 0 entries [21, 22]. The result is a matrix, R, of the same size as M, whose non-zero columns contain distinct low 1s. After reduction, the matrix R may have some columns consisting entirely of 0s. These are positive columns which correspond to the “birth” of a homology class. Negative columns contain 1s, and correspond to the “death” of a homology class. One can prove [21] that the row index low(Rj) for a negative column is the index of a positive column. Another important row index associated to columns of a boundary matrix is given by β(Mj) :=            z, if z is the lowest index of a 1 in column Mj such that Mz,j′ = 0 for j′ < j, −1, if there is no such row index z. Since reduction of a column is achieved through repeated mod-2 additions with preceding columns, a 1 in row z of column Mj cannot be eliminated by reduction if β(Mj) = z. Thus, if β(Mj) ̸= −1, column Rj is necessarily a negative column [29]. If β(Mj) = −1, column Rj may be either negative or positive after reduction. Moreover, since a mod-2 addition of Mj with a preceding column can only decrease low(Mj), it follows [29, Thm. 5] that low(Rj) ∈[β(Mj), low(Mj)]. In the special case where β(Mj) = low(Mj), one has that Mj = Rj without any comparisons of the low 1s in other columns, i.e., Mj is fully reduced. The corresponding apparent pair [8] is thus guaranteed to be a point in the fully reduced PD. This observation has been exploited to great effect in the pursuit of more efficient reduction algorithms [8, 29, 30, 42]. Altogether, these observations suggest four multisets of pairs we can associate to a boundary matrix M: the standard fully reduced (FR) persistence diagram {(low(Rj), j) | low(Rj) ̸= 1}, the non-negative betas (NNB) diagram {(low(Mj), j) | β(Mj) ̸= −1}, the apparent pair (AP) persistence diagram {(low(Mj), j) | β(Mj) = low(Mj) ̸= −1}, and the naive low-ones (L1) persis- tence diagram {(low(Mj), j) | low(Mj) ̸= −1}. We will call the latter three unreduced persistence diagrams. Note that each of these persistence diagrams may be translated from indexed-based PDs to filtration-value PDs using filtration values for the columns. That is to say, an indexed-based PD consisting of index pairs (i, j) is mapped via a filtration function to the multiset with pairs (f(σi), f(σj)). One has directly that the NNB diagram converges in any of the standard matching distances to the diagram defined similarly from R as M undergoes reduction. Storing a dense boundary matrix can require substantial memory in practice, but this can be avoided in some important instances, e.g., when using Rips complexes [8, 42]. This leaves storing and accessing reduced columns as a primary contributor to memory and computation requirements. 2 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Rips PI No Noise FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.6 Figure 1: Average precision scores of random forest classifiers from synthetic shape classification experiments. Input Filtered complex Persistence diagram (reduced or unreduced) Vectorized persistence diagram Classifier or regressor output Figure 2: Persistent homology and machine learning pipeline template for experiments. Unreduced PDs do not require these resources by construction. Most PD vectorization methods only require aggregation, not storage, of results from small computations on each point in a PD. Taken together, these observations suggest unreduced PDs could save resources in PH/ML pipelines, provided that they perform comparably to fully reduced PDs. Pairs, (f(σi), f(σj)), with f(σi) = f(σj) are called ephemeral, and typically do not contribute information after a PD is vectorized. These pairs arise naturally in both fully reduced and unreduced PDs. For example, in the Vietoris-Rips construction, the filtration function f : Σ →R built from point cloud data is defined on simplices of dimension higher than 1 as the largest filtration value of a simplex’s 1-dimensional faces, i.e., f(σ) := max{f(σ′) | σ′ ⊆σ, dim(σ′) = 1}. After sorting by filtration value, it follows that if Mj is a column corresponding to a simplex σj with dimension greater than 1, f(σi) = f(σj) when i = low(Mj). All points in homology degrees greater than 0 are subsequently ephemeral in unreduced diagrams for Vietoris-Rips complexes. While this limitation can be mitigated by taking into account other positive entries in each column, we are focused here on obtaining baseline PH/ML performance information for the simplest versions of unreduced PDs. 3 Methods We conducted PH/ML experiments to better understand the relative performance of these four di- agram constructions, relying in places on various libraries for topological data analysis and machine learning [20, 10, 37, 36, 38]. The tasks and data sets were chosen to capture a range of data types and relative task difficulties. Figure 2 records the general schematic of the pipeline used for each experiment and Table 1 collects specific information about component variations tested for each task. In some experiments, all unreduced PDs of a particular type or homology degree were empty, i.e., all points were ephemeral. In these cases, the affected PD types or homology degrees were excluded from model training. Additional technical information is collected in the Appendix and a code repository is available at https://parkeredw.com/files/unreduced_abstract_files.zip. Every task included vectorization with both persistence images [1] or Adcock-Carlsson coor- dinates [3] as a variation. One experiment was conducted for each combination of component variations. Random forests were used for both classification and regression [15]. Synthetic shape classification. Each entry in this data set consisted of a point cloud of 50 points randomly sampled from one of five shapes in R3: a circle, two distant clusters, uniform random points, a sphere, and a torus. These shapes were as similar in geometric scale as possible. Each of the 200 point clouds per shape was labeled by shape and the ML task assessed was classification. Persistence performs quite well on this task, so to increase difficulty we also tested 3 Data Filt. Types Shape Alpha, Rips L1, NNB, AP, FR f-MNIST “Sweep” L1, FR Brain trees See text L1, NNB, AP, FR Table 1: Experiment component variations. Figure 3: Top: Differences between average H0-sweep 3 PIs for class 5 (sandals) and 7 (sneakers). Bottom: Reshaped weight vectors corresponding to FR principal component 2 (PC2) and L1 prin- cipal component 3 (PC3) for trained H0-sweep 3 random forests. variants where the point clouds were perturbed in a uniform random direction with a uniform random magnitude drawn from (0,µ) for each value µ = 0.1, 0.3, and 0.6. Fashion MNIST image classification. The Fashion MNIST dataset [40] consists of 70,000 grayscale images with 28 × 28 pixel resolution. The images depict clothing items from 10 clothing classes. To create complexes from the images, we followed a similar cubical-complex protocol to one previously used by Adcock and Carlsson [3] and Barnes et al. [7]. Each image is filtered by “sweeping” in the four cardinal directions. From each sweep, cubical complexes were constructed using the lower-star filtration [20]. These complexes contained 0-, 1-, and 2-dimensional cubes from which H0 and H1 features arise. Thus 8 PDs were generated for each image in total. Brain artery tree regression. This task was performed on a data set prepared by Bendich et al. [12] from 3D Magnetic Resonance Angiography brain images collected by Bullit et al. [18]. Each of the 98 data entries consisted of a 3D point cloud depicting a subject’s brain artery tree, together with edges between points determined by a tube-tracking algorithm. To create a filtered complex, we largely followed the procedure of Bendich et al. [12]. In degree 0, vertices were filtered by height, and edges had the same filtration value as the higher of their two vertices. In degree 1, we used alpha complexes constructed from Euclidean distances between vertices (i.e., ignoring edge adjacency). Each brain was also labeled with the subject’s age. The supervised learning task was predicting a subject’s age using their brain artery tree as input. Hyperparameter tuning was conducted using 5-fold cross validation with hyperopt [14]. Only vectorization hyperparameters were tuned while random forest hyperparameters were not. Final 4 Persistence diagram (L1/FR) Persistence image Centered and scaled PI PCA projected PI Random forest output Figure 4: Schematic of feature importance experiment. performance assessment also used 5-fold cross validation with the best identified hyperparameters for every experiment. A hyperparameter training holdout set was used for the shape and fashion MNIST tasks, but there were too few data entries to do this without high variance across tuning folds for the brain artery regression. Figures 1, 6 and 7 and Table 2 report results. Feature importance for f-MNIST classification. We also conducted a feature importance analysis with the f-MNIST dataset to further investigate what differences captured by FR and L1 PDs contribute to differences in testing performance for this data set. First, we simplified the experiment to make the FR and L1 models easier to compare. Using initial testing performances, we identified that the 2-class pair which most improved performance using L1 PDs as opposed to FR PDs was class 5 (sandals) vs class 7 (sneakers). We then trained L1 and FR random forest classification models as in the original experiment with persistence image vectorization and an 80%/20% training/testing split of the whole f-MNIST data set. The PI vectorization hyperparameters used were the same for both the L1 and FR models and were manually chosen based on simple properties of the filtration (e.g. all persistence pairs occupied the [0, 1]2 region of the birth-persistence plane). As a further regularization, the original 800 dimensional vectorizations were mean-centered, scaled by standard deviation in the standard way, and projected onto the first 13 principal compo- nents (PCs) determined from the training data before classification. Figure 4 provides an overview of these experiment modifications. The first 13 PCs were chosen as the minimum number which accounted for over 90% of the variance in the FR vectorizations of the training data. Note that training does not change any model parameters except for centering and scaling until after the vectorization step. Figure 3 records differences between average PIs for sandals and sneakers after centering and scaling but before PCA projection. We then computed permutation-based feature importance using all testing data, not just class 5 and class 7, for both models. This metric is computed feature-by-feature. For each feature, the values of the feature can be shuffled among all data samples, keeping the other features unshuf- fled. Whole model testing performance on this shuffled data is then computed, with the difference between unpermuted classification accuracy and permuted classification accuracy subsequently re- flecting feature importance. A large positive feature importance in feature k indicates that shuffling feature k in the testing data among data samples resulted in degraded performance relative to the original unshuffled testing data. Repeating this procedure and averaging the results across many random permutations and all features yields a final measurement of feature importance for each feature. Abbreviated results are reported in Figure 5 with full results in the Appendix (Figure 8). For each model, we inspected the single most important feature according to this analysis. The corresponding principal component is a vector of weights in the original 800-dimensional vector space. That space is determined by concatenation of the PIs across 4 sweeps and 2 homology dimensions. We reshaped the weights of these principal components to depict the relative contribu- tion of PI regions to each PC’s projected feature. Figure 10 compares the most important principal components in the FR and L1 models. 5 Figure 5: Permutation feature importance of the first 4 principal components (PCs) of simplified FR (top) and L1 (bottom) f-MNIST classification models. FR PI L1 PI FR ACC L1 ACC 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Average Precision Score Fashion MNIST Figure 6: Average precision scores of random forest classifiers that were trained either on PI or AC coordinates of fully reduced (FR) or Low1 (L1) PDs derived from cubical filtrations of f-MNIST images. 6 Vectorization PD type Average R2 Persistence image Low1 0.3541 (0.19) AP 0.3196 (0.17) NNB 0.3130 (0.17) FR 0.3445 (0.12) Adcock-Carlsson Low1 0.1844 (0.08) AP 0.2303 (0.16) NNB 0.2733 (0.13) FR 0.3042 (0.11) Table 2: R2 testing performance of regression pipeline for brain artery experiments averaged over 5 folds. Standard deviations are in parentheses. 4 Results & Discussion In all experiments, at least one type of unreduced persistence diagram performed as well or better than fully reduced diagrams. Relative performance of different persistence diagrams types remained largely similar while varying other pipeline components, providing evidence that PD type was a primary contributor to performance differences. Although alpha complexes performed better than Rips in our synthetic shape experiments, PD type performance rankings remained the same when varying filtrations. Albeit, some ties arose using alpha complexes that were not present with Rips complexes. No PD type outperformed all others across all tasks. Low1 or fully reduced diagrams did, however, attain or tie for best performance across all of our experiments. The most pronounced performance difference between FR and L1 diagrams was in our f-MNIST classification experiment, where L1 diagrams clearly outperformed FR diagrams. Amongst the other two unreduced PD types, NNB diagrams tended to slightly outperform AP diagrams. Analyzing what differences in PD types contribute to downstream performance differences is more challenging. Our simplified models used on f-MNIST data for feature importance comparison replicated the qualitative results from the earlier cross-validated experiment. Testing performance of the simplified models was 66% accuracy for the FR model and 78% for the L1 model. Restricted to class 5 and class 7 testing examples, testing performance was 84% for the FR model and 91% for the L1 model. Figures 3 and 9 from our f-MNIST feature importance experiment visually indicate that the average difference between class 5 and class 7 persistence images was greater for L1 diagrams than FR. Permutation tests for whether the class 5 and 7 averages were distinct using 10000 permutations yielded p-value 1/10001 for both the FR examples and L1 examples. The L2 norm of the average difference between these classes’ scaled persistence images was 3.49 in the FR case and 14.18 in the L1 case. Feature importance results further reinforce the conclusion that differences in L1 and FR vector- izations that are useful for classification occur in distinct regions of the corresponding persistence images. The most important principal component features (Figure 8) were prominent in both the FR and L1 case, exhibiting substantially higher importance than the second most important feature. The cosine similarity between the corresponding most important FR and L1 principal components was 0.290, indicating that these PCs focused on different regions of persistence image space. Interestingly, there appear to be significant weights corresponding to features in each of the 8 component FR PDs, while in the L1 diagrams, the most important principal component has coefficients mostly concentrated in H0 diagrams. 7 In regards to computational costs, we observed significantly longer times to vectorize L1 dia- grams than to vectorize other diagram types. These were due to the larger number of persistence pairs in these diagrams. Careful implementation and proper benchmarking will be required to characterize the computational trade-offs due to vectorizing larger diagrams while avoiding dia- gram reduction. It is now natural to implement optimized software for benchmarking, both to answer this question and others related to potential computational resource savings. The range and number of experiments represents a significant limitation of this study. While the experiments we performed covered several different data types and exhibited a variance in difficulty, they still represent a relatively small number of contexts compared to the overall corpus of existing PH/ML applications. L1 diagrams performed, somewhat surprisingly, as well or better than FR diagrams in almost all cases. Both NNB and L1 PDs are less interpretable, however, than their fully reduced and apparent pairs counterparts. As discussed in Section 2, our unreduced persistence diagrams as defined here also do not capture any information in most homology degrees for popular flag-type filtrations. Given our experimental results, modifying unreduced PD constructions to avoid this limitation is also of interest. Altogether, these results suggest that unreduced PDs can serve as effective substitutes for fully reduced PDs in PH/ML pipelines, at least in terms of task performance. It also appears that in some tasks, unreduced diagrams can encode usefully different information than fully reduced diagrams. Our results indicate that unreduced PDs are a relevant class of approximations which merit further exploration. References [1] Persistence images: A stable vector representation of persistent homology. Journal of Machine Learning Research, 18(8):1–35, 2017. [2] H. Abdallah, A. Regalski, M. B. Kang, M. Berishaj, N. Nnadi, A. Chowdury, V. A. Diwadkar, and A. Salch. Statistical inference for persistent homology applied to simulated fmri time series data. Foundations of Data Science, 5(1):1–25, Mar 2023. [3] A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. Homology Homotopy Appl., 18(1):381–402, 2016. [4] D. Ali, A. Asaad, M.-J. Jimenez, V. Nanda, E. Paluzo-Hidalgo, and M. Soriano-Trigueros. A survey of vectorization methods in topological data analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [5] E. J. Am´ezquita, M. Y. Quigley, T. Ophelders, J. B. Landis, D. Koenig, E. Munch, and D. H. Chitwood. Measuring hidden phenotype: quantifying the shape of barley seeds using the Euler characteristic transform. in silico Plants, 4(1):diab033, 12 2021. [6] E. Baitinger and S. Flegel. The better turbulence index? forecasting adverse financial markets regimes with persistent homology. Financial Markets and Portfolio Management, 35(3):277– 308, 2021. [7] D. Barnes, L. Polanco, and J. A. Perea. A comparative study of machine learning methods for persistence diagrams. Frontiers in Artificial Intelligence, 4, 2021. [8] U. Bauer. Ripser: efficient computation of Vietoris-Rips persistence barcodes. J. Appl. Com- put. Topol., 5(3):391–423, 2021. 8 [9] U. Bauer, M. Kerber, J. Reininghaus, and H. Wagner. Phat – persistent homology algorithms toolbox. Journal of Symbolic Computation, 78:76–90, 2017. Algorithms and Software for Computational Topology. [10] U. Bauer, M. Kerber, J. Reininghaus, and H. Wagner. Phat – persistent homology algorithms toolbox. Journal of Symbolic Computation, 78:76–90, 2017. Algorithms and Software for Computational Topology. [11] U. Bauer, T. B. Masood, B. Giunti, G. Houry, M. Kerber, and A. Rathod. Keeping it sparse: Computing persistent homology revisited. arXiv preprint arXiv:2211.09075, 2022. [12] P. Bendich, J. S. Marron, E. Miller, A. Pieloch, and S. Skwerer. Persistent homology analysis of brain artery trees. Ann Appl Stat, 10(1):198–218, Mar. 2016. [13] K. Benjamin, A. Bhandari, J. D. Kepple, R. Qi, Z. Shang, Y. Xing, Y. An, N. Zhang, Y. Hou, T. L. Crockford, O. McCallion, F. Issa, J. Hester, U. Tillmann, H. A. Harrington, and K. R. Bull. Multiscale topology classifies cells in subcellular spatial transcriptomics. Nature, 630(8018):943–949, Jun 2024. [14] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter opti- mization in hundreds of dimensions for vision architectures. In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 115–123, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. [15] L. Breiman. Random forests. Machine Learning, 45(1):5–32, Oct 2001. [16] P. Bubenik and J. A. Scott. Categorification of persistent homology. Discrete Comput. Geom., 51(3):600–627, 2014. [17] M. Buchet, Y. Hiraoka, and I. Obayashi. Persistent Homology and Materials Informatics, pages 75–95. Springer Singapore, Singapore, 2018. [18] E. Bullitt, D. Zeng, G. Gerig, S. Aylward, S. Joshi, J. K. Smith, W. Lin, and M. G. Ewend. Vessel tortuosity and brain tumor malignancy: a blinded study. Acad Radiol, 12(10):1232– 1240, Oct. 2005. [19] S. Chuli´an, B. J. Stolz, ´A. Mart´ınez-Rubio, C. Bl´azquez Go˜ni, J. F. Rodr´ıguez Guti´errez, T. Caballero Vel´azquez, ´A. Molinos Quintana, M. Ram´ırez Orellana, A. Castillo Robleda, J. L. Fuster Soler, et al. The shape of cancer relapse: Topological data analysis predicts recurrence in paediatric acute lymphoblastic leukaemia. PLOS Computational Biology, 19(8):e1011329, 2023. [20] P. Dlotko. Cubical complex. In GUDHI User and Reference Manual. GUDHI Editorial Board, 3.10.1 edition, 2024. [21] H. Edelsbrunner and J. L. Harer. Computational Topology - an Introduction. American Math- ematical Society, 2010. [22] H. Edelsbrunner, D. Letscher, and A. Zomorodian. Topological persistence and simplification. In Proceedings of the 41st Annual Symposium on Foundations of Computer Science, FOCS ’00, page 454, USA, 2000. IEEE Computer Society. 9 [23] P. Edwards, K. Skruber, N. Mili´cevi´c, J. B. Heidings, T.-A. Read, P. Bubenik, and E. A. Vitriol. TDAExplore: Quantitative analysis of fluorescence microscopy images through topology-based machine learning. Patterns, 2(11), Nov 2021. [24] M. Gidea and Y. Katz. Topological data analysis of financial time series: Landscapes of crashes. Physica A: Statistical Mechanics and its Applications, 491:820–834, 2018. [25] B. Giunti, J. Lazovskis, and B. Rieck. Donut: Creation, development, and opportunities of a database. Notices of the American Mathematical Society, 70(10):1640–1644, 2023. [26] C. Giusti, E. Pastalkova, C. Curto, and V. Itskov. Clique topology reveals intrinsic geo- metric structure in neural correlations. Proceedings of the National Academy of Sciences, 112(44):13455–13460, 2015. [27] G. Henselman and R. Ghrist. Matroid Filtrations and Computational Persistent Homology. ArXiv e-prints, June 2016. [28] Heydenreich, Sven, Br¨uck, Benjamin, and Harnois-D´eraps, Joachim. Persistent homology in cosmic shear: Constraining parameters with topological data analysis. A&A, 648:A74, 2021. [29] R. Mendoza-Smith. Numerical algorithms for the mathematics of information. PhD thesis, University of Oxford, 2017. [30] R. Mendoza-Smith and J. Tanner. Parallel multi-scale reduction of persistent homology filtra- tions. arXiv preprint arXiv:1708.04710, 2017. [31] K. Mischaikow and V. Nanda. Morse theory for filtrations and efficient computation of per- sistent homology. Discrete Comput. Geom., 50(2):330–353, 2013. [32] F. C. Motta, R. Neville, P. D. Shipman, D. A. Pearson, and R. M. Bradley. Measures of order for nearly hexagonal lattices. Physica D: Nonlinear Phenomena, 380-381:17–30, 2018. [33] D. D. Nguyen, Z. Cang, K. Wu, M. Wang, Y. Cao, and G.-W. Wei. Mathematical deep learning for pose and binding affinity prediction and ranking in d3r grand challenges. Journal of Computer-Aided Molecular Design, 33(1):71–82, Jan 2019. [34] N. Otter, M. A. Porter, U. Tillmann, P. Grindrod, and H. A. Harrington. A roadmap for the computation of persistent homology. EPJ Data Science, 6(1):17, 2017. [35] T. Papamarkou, T. Birdal, M. M. Bronstein, G. E. Carlsson, J. Curry, Y. Gao, M. Hajij, R. Kwitt, P. Lio, P. Di Lorenzo, V. Maroulas, N. Miolane, F. Nasrin, K. Natesan Ramamurthy, B. Rieck, S. Scardapane, M. T. Schaub, P. Veliˇckovi´c, B. Wang, Y. Wang, G. Wei, and G. Zamzmi. Position: Topological deep learning is the new frontier for relational learning. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 39529–39555. PMLR, 21–27 Jul 2024. [36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 10 [37] T. G. Project. GUDHI User and Reference Manual. GUDHI Editorial Board, 3.10.1 edition, 2024. [38] N. Saul and C. Tralie. Scikit-tda: Topological data analysis for python, 2019. [39] A. Thomas, K. Bates, A. Elchesen, I. Hartsock, H. Lu, and P. Bubenik. Topological data analysis of c. elegans locomotion and behavior. Frontiers in Artificial Intelligence, 4:668395, 2021. [40] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. [41] J. H. Yip, M. Biagetti, A. Cole, K. Viswanathan, and G. Shiu. Cosmology with persistent homology: a fisher forecast. Journal of Cosmology and Astroparticle Physics, 2024(09):034, Sept. 2024. [42] S. Zhang, M. Xiao, and H. Wang. GPU-Accelerated Computation of Vietoris-Rips Persistence Barcodes. In S. Cabello and D. Z. Chen, editors, 36th International Symposium on Computa- tional Geometry (SoCG 2020), volume 164 of Leibniz International Proceedings in Informatics (LIPIcs), pages 70:1–70:17, Dagstuhl, Germany, 2020. Schloss Dagstuhl – Leibniz-Zentrum f¨ur Informatik. Appendix - Additional technical details In most experiments, multiple persistence diagrams were computed for each data entry. Persistence diagrams for homology of different degrees were counted as separate for this purpose. We used a standard method to vectorize, computing a single vector for each of a data entry’s PDs, then concatenating the resulting vectors to obtain a final output. This procedure introduces a set of vectorization hyperparameters for each persistence diagram associated to a data entry. Our experiments allow each PD’s vectorization hyperparameters to vary independently. For example, each synthetic shape point cloud had associated H0, H1, and H2 PDs. Correspondingly, vectorization hyperparameters for all 3 homology degrees were allowed to vary independently. Some hyperparameter choices for both persistence image and Adcock-Carlsson vectorization entail raising birth, death, or persistence coordinates of PD points to a positive power as part of computations. We observed that this can cause overflows of the 32-bit floats preferred by the Python library sklearn when moderately large coordinates occur in the input PDs. This arose primarily for L1 diagrams computed from alpha complexes. Other filtration types cannot have especially large filtration values relative to inputs by construction. To mitigate the issue, we preprocessed outputs after vectorizing by replacing overflows with the maximum possible non-infinite representable 32-bit float in Python for the synthetic shape and f-MNIST experiments. 11 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Rips PI No Noise FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Rips PI Noise = 0.6 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Alpha PI No Noise FR AP NNB L1 0.6 0.8 1.0 Alpha PI Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Alpha PI Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Alpha PI Noise = 0.6 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Rips ACC No Noise FR AP NNB L1 0.6 0.8 1.0 Rips ACC Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Rips ACC Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Rips ACC Noise = 0.6 FR AP NNB L1 0.6 0.8 1.0 Average Precision Score Alpha ACC No Noise FR AP NNB L1 0.6 0.8 1.0 Alpha ACC Noise = 0.1 FR AP NNB L1 0.6 0.8 1.0 Alpha ACC Noise = 0.3 FR AP NNB L1 0.6 0.8 1.0 Alpha ACC Noise = 0.6 (a) (b) (c) (d) Figure 7: Average precision scores of random forest classifiers that were trained on (a) PIs derived from Rips filtrations, (b) PIs derived from Alpha filtrations, (c) AC coordinates derived from Rips filtrations, and (d) AC coordinates derived from Alpha filtrations. Filtrations were built on point cloud samples (from 5 shape classes), that were perturbed with varying amounts of noise. Unreduced diagrams were computed from either apparent pairs (AP), low 1s in non-negative-beta (NNB) columns, or low 1s across all columns (L1). 12 Figure 8: Permutation feature importance per component for the FR (top) and L1 (bottom) mod- els. The importance score of a component increases proportionately to the average amount of performance degradation after randomly permuting only the corresponding component among all samples. 13 Figure 9: The differences between the average class 7 PIs and average class 5 PIs of corresponding sweep numbers and homology dimensions. Averages were taken over all PIs in the testing set after normalization via mean centering and standard deviation scaling. 14 Figure 10: Reconstructed feature weight vectors corresponding to most important principal compo- nents with the largest positive permutation feature importance score per model type. The second principal component (PC2) for the FR PD and the third principal component (PC3) for the L1 PD were most important to the held-out testing data prediction score. The color map scale is nor- malized by the minimum and maximum weight values across all feature weights and model types to allow comparison of the scales of the weights of FR PC2 and L1 PC3. 15